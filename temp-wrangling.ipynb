{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample code for processing netcdf4 files for kaggle Solar Energy Prediction Competition.\n",
    "https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest#description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good link to start wrapping your head around netcdf data format: \n",
    "# https://www.unidata.ucar.edu/software/netcdf/netcdf-4/newdocs/netcdf-tutorial.html#Intro\n",
    "\n",
    "# This is a good link describing the dataset for the competition.\n",
    "# https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest/discussion/5057\n",
    "\n",
    "# It is important to undertsand that the data provided is the *prediction* of a parameter\n",
    "# (eg. *prediction* of the total precipitation), rather than the *observed* data.\n",
    "# The data is a dictionary of all the helper/axis variables in it once you load the \n",
    "# netcdf4 and the actual data. The actual data is a big array of shape (5113, 11, 5, 9, 16) \n",
    "# with 5113 daily predictions from 1994 to 2007, 11 ensemble members of the GEFS \n",
    "# (different submodel predictions I think), 5 actual predictions (it's released at midnight \n",
    "# I think so it's forcast for 12, 15, 18, 21, and 24 hours out), and 9 latitudes and 16 \n",
    "# longitudes for where the predictions are spatially.\n",
    "# The GEFS is a weather model that just predicts various things at various locations, \n",
    "# and the data is those predictions.\n",
    "\n",
    "# A good python code sample if you prefer a hacker's approach:\n",
    "# http://schubert.atmos.colostate.edu/~cslocum/netcdf_example.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing netcdf4 python library may be non-trivial. The below code is confirmed to work on AWS SageMaker notebook \n",
    "# with 'conda python 3' kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.12\n",
      "  latest version: 4.7.10\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs: \n",
      "    - netcdf4\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2019.6.16          |           py36_0         154 KB  anaconda\n",
      "    libssh2-1.8.2              |       h1ba5d50_0         250 KB  anaconda\n",
      "    cftime-1.0.3.4             |   py36hdd07704_1         310 KB  anaconda\n",
      "    libnetcdf-4.6.1            |       h10edf3e_2         1.3 MB  anaconda\n",
      "    ca-certificates-2019.5.15  |                0         133 KB  anaconda\n",
      "    openssl-1.1.1              |       h7b6447c_0         5.0 MB  anaconda\n",
      "    netcdf4-1.4.2              |   py36h4b4f87f_0         526 KB  anaconda\n",
      "    curl-7.65.2                |       hbc83047_0         141 KB  anaconda\n",
      "    libcurl-7.65.2             |       h20c2e04_0         588 KB  anaconda\n",
      "    hdf4-4.2.13                |       h3ca952b_2         916 KB\n",
      "    krb5-1.16.1                |       h173b8e3_7         1.4 MB  anaconda\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        10.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    cftime:          1.0.3.4-py36hdd07704_1 anaconda\n",
      "    hdf4:            4.2.13-h3ca952b_2              \n",
      "    libnetcdf:       4.6.1-h10edf3e_2       anaconda\n",
      "    netcdf4:         1.4.2-py36h4b4f87f_0   anaconda\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates: 2019.5.15-0                     --> 2019.5.15-0       anaconda\n",
      "    curl:            7.60.0-h84994c4_0               --> 7.65.2-hbc83047_0 anaconda\n",
      "    krb5:            1.14.2-hcdc1b81_6               --> 1.16.1-h173b8e3_7 anaconda\n",
      "    libcurl:         7.60.0-h1ad7b7a_0               --> 7.65.2-h20c2e04_0 anaconda\n",
      "    libssh2:         1.8.0-h9cfc8f7_4                --> 1.8.2-h1ba5d50_0  anaconda\n",
      "    openssl:         1.1.1c-h7b6447c_1               --> 1.1.1-h7b6447c_0  anaconda\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "    certifi:         2019.6.16-py36_1                --> 2019.6.16-py36_0  anaconda\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2019.6.16    | 154 KB    | ##################################### | 100% \n",
      "libssh2-1.8.2        | 250 KB    | ##################################### | 100% \n",
      "cftime-1.0.3.4       | 310 KB    | ##################################### | 100% \n",
      "libnetcdf-4.6.1      | 1.3 MB    | ##################################### | 100% \n",
      "ca-certificates-2019 | 133 KB    | ##################################### | 100% \n",
      "openssl-1.1.1        | 5.0 MB    | ##################################### | 100% \n",
      "netcdf4-1.4.2        | 526 KB    | ##################################### | 100% \n",
      "curl-7.65.2          | 141 KB    | ##################################### | 100% \n",
      "libcurl-7.65.2       | 588 KB    | ##################################### | 100% \n",
      "hdf4-4.2.13          | 916 KB    | ##################################### | 100% \n",
      "krb5-1.16.1          | 1.4 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c anaconda netcdf4 --yes\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "#!conda install -c ioos xarray -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest (not necessarily the fastest) way to xfer the data to your SageMaker machine is to:\n",
    "1. download it to your local machine (eg. your laptop)\n",
    "2. upload the file to your AWS S3 bucket (eg: 3://sergey-ML-workshop) \n",
    "3. download the file from AWS S3 bucket to the machine used to host your SageMaker notebook.\n",
    "    3a. in SageMaker Jupyter console, open a terminal window.\n",
    "    3b. in the terminal window, issue a command to copy the file to your data directory. Example:\n",
    "         $cd SageMaker\n",
    "        $cd <YourProjectDirecotory>\n",
    "         $mkdir data\n",
    "        $mkdir data/train\n",
    "         $cd data/train\n",
    "        $aws s3 cp s3://sergey-ML-workshop/data.nc .\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/xarray/coding/times.py:211: FutureWarning: the 'box' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'box'\n",
      "  result = pd.to_timedelta(num_timedeltas, unit=units, box=False)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import netCDF4\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "ds = xr.open_dataset('/home/ec2-user/SageMaker/train/tmax_2m_latlon_subset_19940101_20071231.nc')\n",
    "df = ds.to_dataframe()\n",
    "\n",
    "# loop through columns you want to use\n",
    "\n",
    "def get_split(df, freq='D', split_type = 'train', cols_to_use = ['Maximum_temperature']):\n",
    "    rt_set = []\n",
    "    \n",
    "    # use 70% for training\n",
    "    if split_type == 'train':\n",
    "        lower_bound = 0\n",
    "        upper_bound = round(df.shape[0] * .7)\n",
    "        \n",
    "    # use 15% for validation\n",
    "    elif split_type == 'validation':\n",
    "        lower_bound = round(df.shape[0] * .7)\n",
    "        upper_bound = round(df.shape[0] * .85)\n",
    "        \n",
    "    # use 15% for test\n",
    "    elif split_type == 'test':\n",
    "        lower_bound = round(df.shape[0] * .85)\n",
    "        upper_bound = df.shape[0]\n",
    "        \n",
    "    for h in list(df):\n",
    "        #if h in cols_to_use:\n",
    "            \n",
    "            target_column = df[h].values.tolist()[lower_bound:upper_bound]\n",
    "            \n",
    "            #date_str = str(df.iloc[0]['time'])\n",
    "            #date_str = ''\n",
    "            #year = date_str[0:4]\n",
    "            #month = date_str[4:6]\n",
    "            #date = date_str[7:]\n",
    "                                                \n",
    "            start_dataset = pd.Timestamp(\"{}-{}-{} 00:00:00\".format('1994', '01', '01', freq=freq))\n",
    "                        \n",
    "            # create a new json object for each column\n",
    "            json_obj = {'start': str(start_dataset),\n",
    "                       'target':target_column}\n",
    "    \n",
    "            rt_set.append(json_obj)\n",
    "        \n",
    "    return rt_set   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_split(df, 'D')\n",
    "test_set = get_split(df, 'D', split_type = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dicts_to_file('max_temp_train.json', train_set)\n",
    "write_dicts_to_file('max_temp_test.json', test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./max_temp_train.json to s3://forecasting-do-not-delete/train/max_temp_train.json\n",
      "upload: ./max_temp_test.json to s3://forecasting-do-not-delete/test/max_temp_test.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp max_temp_train.json s3://forecasting-do-not-delete/train/max_temp_train.json\n",
    "!aws s3 cp max_temp_test.json s3://forecasting-do-not-delete/test/max_temp_test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
