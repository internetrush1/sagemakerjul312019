{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train you first forecasting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you picked the forecasting project. Good for you! Forecasting is an important skill, and it's helpful to learn about the techchnologies out there that can make your life easier. Or worse, depending on how you look at it. ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the Kaggle forecasting data\n",
    "Use the notebook example for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Access your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sagemaker\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5113, 99)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>ADAX</th>\n",
       "      <th>ALTU</th>\n",
       "      <th>APAC</th>\n",
       "      <th>ARNE</th>\n",
       "      <th>BEAV</th>\n",
       "      <th>BESS</th>\n",
       "      <th>BIXB</th>\n",
       "      <th>BLAC</th>\n",
       "      <th>...</th>\n",
       "      <th>VINI</th>\n",
       "      <th>WASH</th>\n",
       "      <th>WATO</th>\n",
       "      <th>WAUR</th>\n",
       "      <th>WEAT</th>\n",
       "      <th>WEST</th>\n",
       "      <th>WILB</th>\n",
       "      <th>WIST</th>\n",
       "      <th>WOOD</th>\n",
       "      <th>WYNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "      <td>11930700</td>\n",
       "      <td>12116700</td>\n",
       "      <td>12301200</td>\n",
       "      <td>10706100</td>\n",
       "      <td>10116900</td>\n",
       "      <td>11487900</td>\n",
       "      <td>11182800</td>\n",
       "      <td>10848300</td>\n",
       "      <td>...</td>\n",
       "      <td>10771800</td>\n",
       "      <td>12116400</td>\n",
       "      <td>11308800</td>\n",
       "      <td>12361800</td>\n",
       "      <td>11331600</td>\n",
       "      <td>10644300</td>\n",
       "      <td>11715600</td>\n",
       "      <td>11241000</td>\n",
       "      <td>10490100</td>\n",
       "      <td>10545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "      <td>9778500</td>\n",
       "      <td>10862700</td>\n",
       "      <td>11666400</td>\n",
       "      <td>8062500</td>\n",
       "      <td>9262800</td>\n",
       "      <td>9235200</td>\n",
       "      <td>3963300</td>\n",
       "      <td>3318300</td>\n",
       "      <td>...</td>\n",
       "      <td>4314300</td>\n",
       "      <td>10733400</td>\n",
       "      <td>9154800</td>\n",
       "      <td>12041400</td>\n",
       "      <td>9168300</td>\n",
       "      <td>4082700</td>\n",
       "      <td>9228000</td>\n",
       "      <td>5829900</td>\n",
       "      <td>7412100</td>\n",
       "      <td>3345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "      <td>9771900</td>\n",
       "      <td>12627300</td>\n",
       "      <td>12782700</td>\n",
       "      <td>11618400</td>\n",
       "      <td>10789800</td>\n",
       "      <td>11895900</td>\n",
       "      <td>4512600</td>\n",
       "      <td>5266500</td>\n",
       "      <td>...</td>\n",
       "      <td>2976900</td>\n",
       "      <td>11775000</td>\n",
       "      <td>10700400</td>\n",
       "      <td>12687300</td>\n",
       "      <td>11324400</td>\n",
       "      <td>2746500</td>\n",
       "      <td>3686700</td>\n",
       "      <td>4488900</td>\n",
       "      <td>9712200</td>\n",
       "      <td>4442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "      <td>6466800</td>\n",
       "      <td>13065300</td>\n",
       "      <td>12817500</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11816700</td>\n",
       "      <td>12186600</td>\n",
       "      <td>3212700</td>\n",
       "      <td>8270100</td>\n",
       "      <td>...</td>\n",
       "      <td>3476400</td>\n",
       "      <td>12159600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12953100</td>\n",
       "      <td>11903700</td>\n",
       "      <td>2741400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>4089300</td>\n",
       "      <td>11401500</td>\n",
       "      <td>4365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>8060400</td>\n",
       "      <td>10379400</td>\n",
       "      <td>6918600</td>\n",
       "      <td>9936300</td>\n",
       "      <td>6411300</td>\n",
       "      <td>9566100</td>\n",
       "      <td>8009400</td>\n",
       "      <td>...</td>\n",
       "      <td>6393300</td>\n",
       "      <td>11419500</td>\n",
       "      <td>7334400</td>\n",
       "      <td>10178700</td>\n",
       "      <td>7471500</td>\n",
       "      <td>8235300</td>\n",
       "      <td>11159100</td>\n",
       "      <td>10651500</td>\n",
       "      <td>10006200</td>\n",
       "      <td>8568300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      ACME      ADAX      ALTU      APAC      ARNE      BEAV  \\\n",
       "0  19940101  12384900  11930700  12116700  12301200  10706100  10116900   \n",
       "1  19940102  11908500   9778500  10862700  11666400   8062500   9262800   \n",
       "2  19940103  12470700   9771900  12627300  12782700  11618400  10789800   \n",
       "3  19940104  12725400   6466800  13065300  12817500  12134400  11816700   \n",
       "4  19940105  10894800  11545200   8060400  10379400   6918600   9936300   \n",
       "\n",
       "       BESS      BIXB      BLAC  ...      VINI      WASH      WATO      WAUR  \\\n",
       "0  11487900  11182800  10848300  ...  10771800  12116400  11308800  12361800   \n",
       "1   9235200   3963300   3318300  ...   4314300  10733400   9154800  12041400   \n",
       "2  11895900   4512600   5266500  ...   2976900  11775000  10700400  12687300   \n",
       "3  12186600   3212700   8270100  ...   3476400  12159600  11907000  12953100   \n",
       "4   6411300   9566100   8009400  ...   6393300  11419500   7334400  10178700   \n",
       "\n",
       "       WEAT      WEST      WILB      WIST      WOOD      WYNO  \n",
       "0  11331600  10644300  11715600  11241000  10490100  10545300  \n",
       "1   9168300   4082700   9228000   5829900   7412100   3345300  \n",
       "2  11324400   2746500   3686700   4488900   9712200   4442100  \n",
       "3  11903700   2741400   4905000   4089300  11401500   4365000  \n",
       "4   7471500   8235300  11159100  10651500  10006200   8568300  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row here is a new point in time, and each column is an energy station. That means that each COLUMN is a unique time series data set. We are going to train our first model on a single column. Then, you can extend it by adding more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACME', 'ADAX', 'ALTU', 'APAC', 'ARNE', 'BEAV', 'BESS', 'BIXB', 'BLAC', 'BOIS', 'BOWL', 'BREC', 'BRIS', 'BUFF', 'BURB', 'BURN', 'BUTL', 'BYAR', 'CAMA', 'CENT', 'CHAN', 'CHER', 'CHEY', 'CHIC', 'CLAY', 'CLOU', 'COOK', 'COPA', 'DURA', 'ELRE', 'ERIC', 'EUFA', 'FAIR', 'FORA', 'FREE', 'FTCB', 'GOOD', 'GUTH', 'HASK', 'HINT', 'HOBA', 'HOLL', 'HOOK', 'HUGO', 'IDAB', 'JAYX', 'KENT', 'KETC', 'LAHO', 'LANE', 'MADI', 'MANG', 'MARE', 'MAYR', 'MCAL', 'MEDF', 'MEDI', 'MIAM', 'MINC', 'MTHE', 'NEWK', 'NINN', 'NOWA', 'OILT', 'OKEM', 'OKMU', 'PAUL', 'PAWN', 'PERK', 'PRYO', 'PUTN', 'REDR', 'RETR', 'RING', 'SALL', 'SEIL', 'SHAW', 'SKIA', 'SLAP', 'SPEN', 'STIG', 'STIL', 'STUA', 'SULP', 'TAHL', 'TALI', 'TIPT', 'TISH', 'VINI', 'WASH', 'WATO', 'WAUR', 'WEAT', 'WEST', 'WILB', 'WIST', 'WOOD', 'WYNO']\n"
     ]
    }
   ],
   "source": [
    "# build list of locations\n",
    "columns = list(df.columns[1:])\n",
    "columns = columns\n",
    "# columns[:85]\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['ACME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5113,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have 5113 obervations. That's well over the 300-limit on DeepAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Train and Test Sets\n",
    "Now, we'll build 2 datasets. One for training, another for testing. Both need to be written to json files, then copied over to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(df, freq='D', split_type = 'train', cols_to_use = ['ACME']):\n",
    "    rt_set = []\n",
    "    \n",
    "    # use 70% for training\n",
    "    if split_type == 'train':\n",
    "        lower_bound = 0\n",
    "        upper_bound = round(df.shape[0] * .7)\n",
    "        \n",
    "    # use 15% for validation\n",
    "    elif split_type == 'validation':\n",
    "        lower_bound = round(df.shape[0] * .7)\n",
    "        upper_bound = round(df.shape[0] * .85)\n",
    "        \n",
    "    # use 15% for test\n",
    "    elif split_type == 'test':\n",
    "        lower_bound = round(df.shape[0] * .85)\n",
    "        upper_bound = df.shape[0]\n",
    "            \n",
    "    # loop through columns you want to use\n",
    "    for h in list(df):\n",
    "        if h in cols_to_use:\n",
    "            \n",
    "            target_column = df[h].values.tolist()[lower_bound:upper_bound]\n",
    "            \n",
    "            date_str = str(df.iloc[0]['Date'])\n",
    "            \n",
    "            year = date_str[0:4]\n",
    "            month = date_str[4:6]\n",
    "            date = date_str[7:]\n",
    "                                                \n",
    "            start_dataset = pd.Timestamp(\"{}-{}-{} 00:00:00\".format(year, month, date, freq=freq))\n",
    "                        \n",
    "            # create a new json object for each column\n",
    "            json_obj = {\n",
    "                        'start': str(start_dataset),\n",
    "                        'target': target_column\n",
    "                       }\n",
    "    \n",
    "            rt_set.append(json_obj)\n",
    "    \n",
    "    return rt_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_split(df, cols_to_use = columns)\n",
    "test_set = get_split(df, split_type = 'test', cols_to_use = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            # fp.write(json.dumps(data).encode(\"utf-8\"))\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dicts_to_file('train.json', train_set)\n",
    "write_dicts_to_file('test.json', test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./train.json to s3://forecasting-do-not-delete/train/train.json\n",
      "upload: ./test.json to s3://forecasting-do-not-delete/test/test.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp train.json s3://forecasting-do-not-delete/train/train.json\n",
    "!aws s3 cp test.json s3://forecasting-do-not-delete/test/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run a SageMaker Training Job\n",
    "Ok! If everything worked, we should be able to train a model in SageMaker straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "image = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "role = sagemaker.get_execution_role()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sess,\n",
    "    image_name=image,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-electricity-demo',\n",
    "    output_path='s3://forecasting-do-not-delete/output'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \n",
    "    # frequency interval is once per day\n",
    "    \"time_freq\": 'D',\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \n",
    "    # let's use the last 30 days for context\n",
    "    \"context_length\": str(30),\n",
    "    \n",
    "    # let's forecast for 30 days\n",
    "    \"prediction_length\": str(30)\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-01 19:36:37 Starting - Starting the training job.........\n",
      "2019-08-01 19:37:51 Starting - Launching requested ML instances......\n",
      "2019-08-01 19:38:55 Starting - Preparing the instances for training......\n",
      "2019-08-01 19:40:04 Downloading - Downloading input data...\n",
      "2019-08-01 19:40:25 Training - Downloading the training image.\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'30', u'epochs': u'400', u'time_freq': u'D', u'context_length': u'30', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'30', u'time_freq': u'D', u'context_length': u'30', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Training set statistics:\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Integer time series\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] number of time series: 98\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] number of observations: 350742\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] mean target length: 3579\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] min/mean/max target: 300.0/16712640.5821/39442800.0\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] mean abs(target): 16712640.5821\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] contains missing values: no\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Small number of time series. Doing 6 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Test set statistics:\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Integer time series\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] number of time series: 98\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] number of observations: 75166\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] mean target length: 767\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] min/mean/max target: 4200.0/16273434.2382/32884800.0\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] mean abs(target): 16273434.2382\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] contains missing values: no\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] nvidia-smi took: 0.0251739025116 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 175.79889297485352, \"sum\": 175.79889297485352, \"min\": 175.79889297485352}}, \"EndTime\": 1564688442.499605, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688442.322979}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:42 INFO 140719711954752] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 397.88293838500977, \"sum\": 397.88293838500977, \"min\": 397.88293838500977}}, \"EndTime\": 1564688442.720999, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688442.499678}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:43 INFO 140719711954752] Epoch[0] Batch[0] avg_epoch_loss=19.552359\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=19.5523586273\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:43 INFO 140719711954752] Epoch[0] Batch[5] avg_epoch_loss=19.244887\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=19.2448870341\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:43 INFO 140719711954752] Epoch[0] Batch [5]#011Speed: 891.72 samples/sec#011loss=19.244887\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:43 INFO 140719711954752] processed a total of 585 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 1189.4559860229492, \"sum\": 1189.4559860229492, \"min\": 1189.4559860229492}}, \"EndTime\": 1564688443.910633, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688442.721062}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:43 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=491.763123275 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:43 INFO 140719711954752] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=0, train loss <loss>=18.9344764709\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:43 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:43 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_88935022-72d9-4362-b338-c91bbc57af1c-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.154985427856445, \"sum\": 36.154985427856445, \"min\": 36.154985427856445}}, \"EndTime\": 1564688443.947471, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688443.910736}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:44 INFO 140719711954752] Epoch[1] Batch[0] avg_epoch_loss=18.289957\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:44 INFO 140719711954752] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=18.2899570465\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:44 INFO 140719711954752] Epoch[1] Batch[5] avg_epoch_loss=18.109715\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:44 INFO 140719711954752] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=18.1097145081\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:44 INFO 140719711954752] Epoch[1] Batch [5]#011Speed: 932.92 samples/sec#011loss=18.109715\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:45 INFO 140719711954752] processed a total of 596 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1091.568946838379, \"sum\": 1091.568946838379, \"min\": 1091.568946838379}}, \"EndTime\": 1564688445.039182, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688443.947548}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:45 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=545.943093882 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:45 INFO 140719711954752] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=1, train loss <loss>=17.9257623672\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:45 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:45 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_00518b96-afc9-4166-b94d-1f7b91d1469a-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.676054000854492, \"sum\": 31.676054000854492, \"min\": 31.676054000854492}}, \"EndTime\": 1564688445.071435, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688445.039264}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:45 INFO 140719711954752] Epoch[2] Batch[0] avg_epoch_loss=17.544636\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=17.5446357727\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:45 INFO 140719711954752] Epoch[2] Batch[5] avg_epoch_loss=17.472899\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=17.472899437\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:45 INFO 140719711954752] Epoch[2] Batch [5]#011Speed: 917.86 samples/sec#011loss=17.472899\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:46 INFO 140719711954752] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.9159908294678, \"sum\": 1089.9159908294678, \"min\": 1089.9159908294678}}, \"EndTime\": 1564688446.161481, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688445.071509}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:46 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=569.70699577 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:46 INFO 140719711954752] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=2, train loss <loss>=17.4225870132\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:46 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:46 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_59762202-d0d0-47ca-9452-3a8099903549-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.030914306640625, \"sum\": 34.030914306640625, \"min\": 34.030914306640625}}, \"EndTime\": 1564688446.196095, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688446.161563}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:46 INFO 140719711954752] Epoch[3] Batch[0] avg_epoch_loss=17.231724\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=17.2317237854\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:46 INFO 140719711954752] Epoch[3] Batch[5] avg_epoch_loss=17.330745\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=17.3307453791\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:46 INFO 140719711954752] Epoch[3] Batch [5]#011Speed: 954.29 samples/sec#011loss=17.330745\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:47 INFO 140719711954752] processed a total of 568 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 982.9971790313721, \"sum\": 982.9971790313721, \"min\": 982.9971790313721}}, \"EndTime\": 1564688447.179222, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688446.196165}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:47 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=577.755985177 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:47 INFO 140719711954752] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:47 INFO 140719711954752] #quality_metric: host=algo-1, epoch=3, train loss <loss>=17.2999345991\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:47 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:47 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_a45a20d0-40f3-49e5-8b07-6205497d4867-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.175872802734375, \"sum\": 34.175872802734375, \"min\": 34.175872802734375}}, \"EndTime\": 1564688447.213972, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688447.179302}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:47 INFO 140719711954752] Epoch[4] Batch[0] avg_epoch_loss=17.340658\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:47 INFO 140719711954752] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=17.3406581879\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:48 INFO 140719711954752] Epoch[4] Batch[5] avg_epoch_loss=17.235285\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:48 INFO 140719711954752] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=17.2352854411\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:48 INFO 140719711954752] Epoch[4] Batch [5]#011Speed: 736.53 samples/sec#011loss=17.235285\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:48 INFO 140719711954752] processed a total of 615 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1277.5001525878906, \"sum\": 1277.5001525878906, \"min\": 1277.5001525878906}}, \"EndTime\": 1564688448.491603, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688447.214043}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:48 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=481.364924495 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:48 INFO 140719711954752] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:48 INFO 140719711954752] #quality_metric: host=algo-1, epoch=4, train loss <loss>=17.2054235458\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:48 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:48 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_98a703d7-c257-49f3-b95d-68311597f90e-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.655094146728516, \"sum\": 34.655094146728516, \"min\": 34.655094146728516}}, \"EndTime\": 1564688448.526842, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688448.491684}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:48 INFO 140719711954752] Epoch[5] Batch[0] avg_epoch_loss=17.219604\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:48 INFO 140719711954752] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=17.2196044922\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:49 INFO 140719711954752] Epoch[5] Batch[5] avg_epoch_loss=17.199845\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:49 INFO 140719711954752] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=17.199845314\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:49 INFO 140719711954752] Epoch[5] Batch [5]#011Speed: 968.86 samples/sec#011loss=17.199845\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:49 INFO 140719711954752] processed a total of 614 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1063.8649463653564, \"sum\": 1063.8649463653564, \"min\": 1063.8649463653564}}, \"EndTime\": 1564688449.590841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688448.526914}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:49 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=577.076909149 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:49 INFO 140719711954752] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:49 INFO 140719711954752] #quality_metric: host=algo-1, epoch=5, train loss <loss>=17.2087169647\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:49 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:50 INFO 140719711954752] Epoch[6] Batch[0] avg_epoch_loss=17.082178\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=17.0821781158\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:50 INFO 140719711954752] Epoch[6] Batch[5] avg_epoch_loss=17.169517\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=17.1695171992\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:50 INFO 140719711954752] Epoch[6] Batch [5]#011Speed: 935.71 samples/sec#011loss=17.169517\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:50 INFO 140719711954752] processed a total of 550 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1006.6931247711182, \"sum\": 1006.6931247711182, \"min\": 1006.6931247711182}}, \"EndTime\": 1564688450.598061, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688449.590923}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:50 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=546.27817982 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:50 INFO 140719711954752] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=6, train loss <loss>=17.130692588\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:50 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:50 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_3c3c06c9-0f64-47e6-8216-cfe44a07e8e1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.183034896850586, \"sum\": 24.183034896850586, \"min\": 24.183034896850586}}, \"EndTime\": 1564688450.622816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688450.598142}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:51 INFO 140719711954752] Epoch[7] Batch[0] avg_epoch_loss=17.248531\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=17.2485313416\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:51 INFO 140719711954752] Epoch[7] Batch[5] avg_epoch_loss=17.145473\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=17.1454725266\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:51 INFO 140719711954752] Epoch[7] Batch [5]#011Speed: 931.73 samples/sec#011loss=17.145473\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:51 INFO 140719711954752] processed a total of 554 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 967.9970741271973, \"sum\": 967.9970741271973, \"min\": 967.9970741271973}}, \"EndTime\": 1564688451.590945, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688450.622886}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:51 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=572.245448118 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:51 INFO 140719711954752] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=7, train loss <loss>=17.1378951603\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:51 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:52 INFO 140719711954752] Epoch[8] Batch[0] avg_epoch_loss=17.142309\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=17.1423091888\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:52 INFO 140719711954752] Epoch[8] Batch[5] avg_epoch_loss=17.113607\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=17.1136067708\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:52 INFO 140719711954752] Epoch[8] Batch [5]#011Speed: 972.81 samples/sec#011loss=17.113607\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:52 INFO 140719711954752] processed a total of 575 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 972.63503074646, \"sum\": 972.63503074646, \"min\": 972.63503074646}}, \"EndTime\": 1564688452.564118, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688451.591025}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:52 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=591.107136178 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:52 INFO 140719711954752] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=8, train loss <loss>=17.1104740567\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:52 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:52 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_cd124e40-df61-4dd7-9542-b1732b3a4b08-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 25.136947631835938, \"sum\": 25.136947631835938, \"min\": 25.136947631835938}}, \"EndTime\": 1564688452.589847, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688452.564198}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:53 INFO 140719711954752] Epoch[9] Batch[0] avg_epoch_loss=16.968744\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=16.968744278\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:53 INFO 140719711954752] Epoch[9] Batch[5] avg_epoch_loss=17.059765\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=17.0597645442\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:53 INFO 140719711954752] Epoch[9] Batch [5]#011Speed: 888.95 samples/sec#011loss=17.059765\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:53 INFO 140719711954752] processed a total of 586 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1075.0911235809326, \"sum\": 1075.0911235809326, \"min\": 1075.0911235809326}}, \"EndTime\": 1564688453.665059, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688452.589911}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:53 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=545.011335231 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:53 INFO 140719711954752] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=9, train loss <loss>=17.084406662\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:53 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:53 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_041675d5-85c4-46cd-a251-f9242de86f44-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.744966506958008, \"sum\": 21.744966506958008, \"min\": 21.744966506958008}}, \"EndTime\": 1564688453.687388, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688453.66514}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-08-01 19:40:39 Training - Training image download completed. Training in progress.\u001b[31m[08/01/2019 19:40:54 INFO 140719711954752] Epoch[10] Batch[0] avg_epoch_loss=17.143635\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=17.1436347961\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:54 INFO 140719711954752] Epoch[10] Batch[5] avg_epoch_loss=17.076578\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=17.0765784582\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:54 INFO 140719711954752] Epoch[10] Batch [5]#011Speed: 977.97 samples/sec#011loss=17.076578\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:54 INFO 140719711954752] processed a total of 579 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1003.0059814453125, \"sum\": 1003.0059814453125, \"min\": 1003.0059814453125}}, \"EndTime\": 1564688454.690512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688453.687448}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:54 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=577.19971878 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:54 INFO 140719711954752] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=10, train loss <loss>=17.0608449936\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:54 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:54 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_35f251ce-1894-4adb-8fe9-7bf1e041544b-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.70395851135254, \"sum\": 21.70395851135254, \"min\": 21.70395851135254}}, \"EndTime\": 1564688454.712815, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688454.690587}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:55 INFO 140719711954752] Epoch[11] Batch[0] avg_epoch_loss=17.130022\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:55 INFO 140719711954752] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=17.130022049\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:55 INFO 140719711954752] Epoch[11] Batch[5] avg_epoch_loss=17.085577\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:55 INFO 140719711954752] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=17.085577329\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:55 INFO 140719711954752] Epoch[11] Batch [5]#011Speed: 971.21 samples/sec#011loss=17.085577\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:55 INFO 140719711954752] processed a total of 631 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1009.7119808197021, \"sum\": 1009.7119808197021, \"min\": 1009.7119808197021}}, \"EndTime\": 1564688455.722637, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688454.712871}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:55 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=624.857060425 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:55 INFO 140719711954752] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:55 INFO 140719711954752] #quality_metric: host=algo-1, epoch=11, train loss <loss>=17.0729722977\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:55 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:56 INFO 140719711954752] Epoch[12] Batch[0] avg_epoch_loss=17.024120\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=17.0241203308\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:56 INFO 140719711954752] Epoch[12] Batch[5] avg_epoch_loss=17.010306\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=17.0103057226\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:56 INFO 140719711954752] Epoch[12] Batch [5]#011Speed: 932.99 samples/sec#011loss=17.010306\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:56 INFO 140719711954752] processed a total of 570 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 949.944019317627, \"sum\": 949.944019317627, \"min\": 949.944019317627}}, \"EndTime\": 1564688456.67314, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688455.722718}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:56 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=599.970307063 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:56 INFO 140719711954752] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=12, train loss <loss>=17.0326961941\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:56 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:56 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_5f4734ad-817f-47fb-b8a5-a490fa42ad1e-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.542144775390625, \"sum\": 20.542144775390625, \"min\": 20.542144775390625}}, \"EndTime\": 1564688456.694243, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688456.673211}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:57 INFO 140719711954752] Epoch[13] Batch[0] avg_epoch_loss=17.122923\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:57 INFO 140719711954752] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=17.1229228973\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:57 INFO 140719711954752] Epoch[13] Batch[5] avg_epoch_loss=17.057764\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:57 INFO 140719711954752] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=17.0577637355\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:57 INFO 140719711954752] Epoch[13] Batch [5]#011Speed: 964.21 samples/sec#011loss=17.057764\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:57 INFO 140719711954752] processed a total of 577 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1028.6839008331299, \"sum\": 1028.6839008331299, \"min\": 1028.6839008331299}}, \"EndTime\": 1564688457.723059, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688456.694314}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:57 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=560.847063709 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:57 INFO 140719711954752] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:57 INFO 140719711954752] #quality_metric: host=algo-1, epoch=13, train loss <loss>=17.0354434967\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:57 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:58 INFO 140719711954752] Epoch[14] Batch[0] avg_epoch_loss=17.059614\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=17.0596141815\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:58 INFO 140719711954752] Epoch[14] Batch[5] avg_epoch_loss=17.033400\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=17.0334002177\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:58 INFO 140719711954752] Epoch[14] Batch [5]#011Speed: 973.05 samples/sec#011loss=17.033400\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:58 INFO 140719711954752] processed a total of 576 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 995.5201148986816, \"sum\": 995.5201148986816, \"min\": 995.5201148986816}}, \"EndTime\": 1564688458.719097, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688457.72314}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:58 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=578.523581372 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:58 INFO 140719711954752] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=14, train loss <loss>=17.0093924205\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:58 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:58 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_590bb96a-f232-44f2-a352-5494f7ad9515-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.23285484313965, \"sum\": 34.23285484313965, \"min\": 34.23285484313965}}, \"EndTime\": 1564688458.7539, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688458.719177}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:59 INFO 140719711954752] Epoch[15] Batch[0] avg_epoch_loss=16.992640\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=16.9926395416\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:59 INFO 140719711954752] Epoch[15] Batch[5] avg_epoch_loss=17.024454\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=17.0244544347\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:59 INFO 140719711954752] Epoch[15] Batch [5]#011Speed: 940.33 samples/sec#011loss=17.024454\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:59 INFO 140719711954752] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1087.4390602111816, \"sum\": 1087.4390602111816, \"min\": 1087.4390602111816}}, \"EndTime\": 1564688459.841452, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688458.753959}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:59 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=571.005014314 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:59 INFO 140719711954752] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=15, train loss <loss>=16.999492836\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:59 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:40:59 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_d5295c9f-6c00-4b62-906b-7bd08d1f4940-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.892133712768555, \"sum\": 30.892133712768555, \"min\": 30.892133712768555}}, \"EndTime\": 1564688459.872919, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688459.841534}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:00 INFO 140719711954752] Epoch[16] Batch[0] avg_epoch_loss=16.972122\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=16.9721221924\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:00 INFO 140719711954752] Epoch[16] Batch[5] avg_epoch_loss=17.015950\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=17.0159502029\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:00 INFO 140719711954752] Epoch[16] Batch [5]#011Speed: 898.68 samples/sec#011loss=17.015950\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:00 INFO 140719711954752] processed a total of 599 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1074.3169784545898, \"sum\": 1074.3169784545898, \"min\": 1074.3169784545898}}, \"EndTime\": 1564688460.947356, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688459.87298}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:00 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=557.501204034 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:00 INFO 140719711954752] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=16, train loss <loss>=17.0352064133\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:00 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:01 INFO 140719711954752] Epoch[17] Batch[0] avg_epoch_loss=17.028992\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:01 INFO 140719711954752] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=17.0289916992\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:01 INFO 140719711954752] Epoch[17] Batch[5] avg_epoch_loss=16.940447\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:01 INFO 140719711954752] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=16.9404474894\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:01 INFO 140719711954752] Epoch[17] Batch [5]#011Speed: 843.95 samples/sec#011loss=16.940447\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:01 INFO 140719711954752] processed a total of 574 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1008.9061260223389, \"sum\": 1008.9061260223389, \"min\": 1008.9061260223389}}, \"EndTime\": 1564688461.956792, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688460.94744}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:01 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=568.872785116 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:01 INFO 140719711954752] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:01 INFO 140719711954752] #quality_metric: host=algo-1, epoch=17, train loss <loss>=16.9581635793\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:01 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:01 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_6a4d6abb-1257-41d6-bc03-ae753f716d08-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.23515510559082, \"sum\": 22.23515510559082, \"min\": 22.23515510559082}}, \"EndTime\": 1564688461.979643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688461.956866}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:02 INFO 140719711954752] Epoch[18] Batch[0] avg_epoch_loss=16.910025\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=16.9100246429\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:02 INFO 140719711954752] Epoch[18] Batch[5] avg_epoch_loss=16.968530\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=16.9685300191\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:02 INFO 140719711954752] Epoch[18] Batch [5]#011Speed: 965.57 samples/sec#011loss=16.968530\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:03 INFO 140719711954752] processed a total of 578 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1023.7178802490234, \"sum\": 1023.7178802490234, \"min\": 1023.7178802490234}}, \"EndTime\": 1564688463.00348, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688461.979701}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:03 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=564.545831624 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:03 INFO 140719711954752] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=18, train loss <loss>=16.93664608\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:03 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:03 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_b6e3fd8c-7819-4950-bf0f-547b33e26432-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.346973419189453, \"sum\": 22.346973419189453, \"min\": 22.346973419189453}}, \"EndTime\": 1564688463.02642, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688463.003556}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:03 INFO 140719711954752] Epoch[19] Batch[0] avg_epoch_loss=16.923988\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=16.9239883423\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:03 INFO 140719711954752] Epoch[19] Batch[5] avg_epoch_loss=16.935591\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=16.9355910619\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:03 INFO 140719711954752] Epoch[19] Batch [5]#011Speed: 963.34 samples/sec#011loss=16.935591\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:41:04 INFO 140719711954752] processed a total of 584 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1027.496099472046, \"sum\": 1027.496099472046, \"min\": 1027.496099472046}}, \"EndTime\": 1564688464.054054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688463.026493}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:04 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=568.306184732 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:04 INFO 140719711954752] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=19, train loss <loss>=16.9281686783\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:04 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:04 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_838af5ba-6cfa-4d80-8059-774e9945e99d-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.89096260070801, \"sum\": 33.89096260070801, \"min\": 33.89096260070801}}, \"EndTime\": 1564688464.088512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688464.054134}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:04 INFO 140719711954752] Epoch[20] Batch[0] avg_epoch_loss=17.028608\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=17.0286083221\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:04 INFO 140719711954752] Epoch[20] Batch[5] avg_epoch_loss=16.967004\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=16.9670041402\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:04 INFO 140719711954752] Epoch[20] Batch [5]#011Speed: 966.77 samples/sec#011loss=16.967004\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:05 INFO 140719711954752] processed a total of 557 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 962.7881050109863, \"sum\": 962.7881050109863, \"min\": 962.7881050109863}}, \"EndTime\": 1564688465.05142, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688464.088574}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:05 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=578.460945429 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:05 INFO 140719711954752] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=20, train loss <loss>=16.9687756432\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:05 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:05 INFO 140719711954752] Epoch[21] Batch[0] avg_epoch_loss=16.946363\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=16.9463634491\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:05 INFO 140719711954752] Epoch[21] Batch[5] avg_epoch_loss=16.982588\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=16.9825881322\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:05 INFO 140719711954752] Epoch[21] Batch [5]#011Speed: 963.77 samples/sec#011loss=16.982588\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:06 INFO 140719711954752] processed a total of 572 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 949.6719837188721, \"sum\": 949.6719837188721, \"min\": 949.6719837188721}}, \"EndTime\": 1564688466.001658, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688465.051496}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:06 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=602.237784257 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:06 INFO 140719711954752] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:06 INFO 140719711954752] #quality_metric: host=algo-1, epoch=21, train loss <loss>=16.9766311646\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:06 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:06 INFO 140719711954752] Epoch[22] Batch[0] avg_epoch_loss=17.011272\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:06 INFO 140719711954752] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=17.0112724304\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:06 INFO 140719711954752] Epoch[22] Batch[5] avg_epoch_loss=16.949300\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:06 INFO 140719711954752] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=16.9493004481\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:06 INFO 140719711954752] Epoch[22] Batch [5]#011Speed: 918.78 samples/sec#011loss=16.949300\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:07 INFO 140719711954752] processed a total of 583 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1050.595998764038, \"sum\": 1050.595998764038, \"min\": 1050.595998764038}}, \"EndTime\": 1564688467.052777, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688466.001738}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:07 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=554.860781472 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:07 INFO 140719711954752] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:07 INFO 140719711954752] #quality_metric: host=algo-1, epoch=22, train loss <loss>=16.9296657562\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:07 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:07 INFO 140719711954752] Epoch[23] Batch[0] avg_epoch_loss=16.886311\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:07 INFO 140719711954752] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=16.8863105774\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:07 INFO 140719711954752] Epoch[23] Batch[5] avg_epoch_loss=16.929831\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:07 INFO 140719711954752] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=16.9298305511\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:07 INFO 140719711954752] Epoch[23] Batch [5]#011Speed: 967.44 samples/sec#011loss=16.929831\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:08 INFO 140719711954752] processed a total of 578 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1083.1799507141113, \"sum\": 1083.1799507141113, \"min\": 1083.1799507141113}}, \"EndTime\": 1564688468.136482, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688467.052859}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:08 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=533.558815192 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:08 INFO 140719711954752] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=23, train loss <loss>=16.9196598053\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:08 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:08 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_b2a62927-95dc-4941-98c6-6ffbd659282b-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.17388153076172, \"sum\": 22.17388153076172, \"min\": 22.17388153076172}}, \"EndTime\": 1564688468.159321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688468.136556}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:08 INFO 140719711954752] Epoch[24] Batch[0] avg_epoch_loss=16.882200\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=16.8822002411\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:08 INFO 140719711954752] Epoch[24] Batch[5] avg_epoch_loss=16.917445\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=16.9174448649\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:08 INFO 140719711954752] Epoch[24] Batch [5]#011Speed: 973.43 samples/sec#011loss=16.917445\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:09 INFO 140719711954752] processed a total of 612 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1024.907112121582, \"sum\": 1024.907112121582, \"min\": 1024.907112121582}}, \"EndTime\": 1564688469.184352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688468.159387}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:09 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=597.039368936 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:09 INFO 140719711954752] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:09 INFO 140719711954752] #quality_metric: host=algo-1, epoch=24, train loss <loss>=16.8998054504\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:09 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:09 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_a761895b-4005-4257-b67c-e2bd9e4bf16b-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.172927856445312, \"sum\": 22.172927856445312, \"min\": 22.172927856445312}}, \"EndTime\": 1564688469.207214, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688469.184467}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:09 INFO 140719711954752] Epoch[25] Batch[0] avg_epoch_loss=16.958420\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:09 INFO 140719711954752] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=16.9584197998\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:09 INFO 140719711954752] Epoch[25] Batch[5] avg_epoch_loss=16.938790\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:09 INFO 140719711954752] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=16.9387900035\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:09 INFO 140719711954752] Epoch[25] Batch [5]#011Speed: 968.13 samples/sec#011loss=16.938790\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:10 INFO 140719711954752] processed a total of 585 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1016.4570808410645, \"sum\": 1016.4570808410645, \"min\": 1016.4570808410645}}, \"EndTime\": 1564688470.223789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688469.207276}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:10 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=575.459372087 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:10 INFO 140719711954752] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:10 INFO 140719711954752] #quality_metric: host=algo-1, epoch=25, train loss <loss>=16.9558008194\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:10 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:10 INFO 140719711954752] Epoch[26] Batch[0] avg_epoch_loss=16.886591\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:10 INFO 140719711954752] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=16.8865909576\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:10 INFO 140719711954752] Epoch[26] Batch[5] avg_epoch_loss=16.925823\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:10 INFO 140719711954752] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=16.9258225759\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:10 INFO 140719711954752] Epoch[26] Batch [5]#011Speed: 969.46 samples/sec#011loss=16.925823\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:11 INFO 140719711954752] processed a total of 580 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1016.4320468902588, \"sum\": 1016.4320468902588, \"min\": 1016.4320468902588}}, \"EndTime\": 1564688471.240787, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688470.223873}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:11 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=570.561255626 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:11 INFO 140719711954752] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:11 INFO 140719711954752] #quality_metric: host=algo-1, epoch=26, train loss <loss>=16.9409404755\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:11 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:11 INFO 140719711954752] Epoch[27] Batch[0] avg_epoch_loss=17.003681\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:11 INFO 140719711954752] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=17.0036811829\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] Epoch[27] Batch[5] avg_epoch_loss=16.902048\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=16.9020484289\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] Epoch[27] Batch [5]#011Speed: 963.61 samples/sec#011loss=16.902048\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] processed a total of 568 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 987.0810508728027, \"sum\": 987.0810508728027, \"min\": 987.0810508728027}}, \"EndTime\": 1564688472.228473, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688471.240858}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=575.36409925 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] #quality_metric: host=algo-1, epoch=27, train loss <loss>=16.9272763994\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] Epoch[28] Batch[0] avg_epoch_loss=16.876842\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=16.8768424988\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] Epoch[28] Batch[5] avg_epoch_loss=16.874498\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=16.8744977315\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:12 INFO 140719711954752] Epoch[28] Batch [5]#011Speed: 953.24 samples/sec#011loss=16.874498\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:13 INFO 140719711954752] processed a total of 602 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1027.3380279541016, \"sum\": 1027.3380279541016, \"min\": 1027.3380279541016}}, \"EndTime\": 1564688473.256476, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688472.228556}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:13 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=585.899410706 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:13 INFO 140719711954752] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:13 INFO 140719711954752] #quality_metric: host=algo-1, epoch=28, train loss <loss>=16.8943531036\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:13 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:13 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_2812564f-f543-4abe-86c0-e3e1f506a5a1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.35206413269043, \"sum\": 34.35206413269043, \"min\": 34.35206413269043}}, \"EndTime\": 1564688473.291568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688473.256559}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:13 INFO 140719711954752] Epoch[29] Batch[0] avg_epoch_loss=16.982023\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:13 INFO 140719711954752] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=16.9820232391\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:41:14 INFO 140719711954752] Epoch[29] Batch[5] avg_epoch_loss=16.945883\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:14 INFO 140719711954752] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=16.9458827972\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:14 INFO 140719711954752] Epoch[29] Batch [5]#011Speed: 965.66 samples/sec#011loss=16.945883\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:14 INFO 140719711954752] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1033.9651107788086, \"sum\": 1033.9651107788086, \"min\": 1033.9651107788086}}, \"EndTime\": 1564688474.325652, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688473.291633}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:14 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=600.548883745 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:14 INFO 140719711954752] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:14 INFO 140719711954752] #quality_metric: host=algo-1, epoch=29, train loss <loss>=16.9557439804\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:14 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:14 INFO 140719711954752] Epoch[30] Batch[0] avg_epoch_loss=16.890200\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:14 INFO 140719711954752] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=16.8901996613\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:15 INFO 140719711954752] Epoch[30] Batch[5] avg_epoch_loss=16.932210\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:15 INFO 140719711954752] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=16.9322099686\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:15 INFO 140719711954752] Epoch[30] Batch [5]#011Speed: 967.98 samples/sec#011loss=16.932210\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:15 INFO 140719711954752] processed a total of 559 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 945.8801746368408, \"sum\": 945.8801746368408, \"min\": 945.8801746368408}}, \"EndTime\": 1564688475.272025, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688474.325711}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:15 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=590.916624262 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:15 INFO 140719711954752] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:15 INFO 140719711954752] #quality_metric: host=algo-1, epoch=30, train loss <loss>=16.9442384508\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:15 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:15 INFO 140719711954752] Epoch[31] Batch[0] avg_epoch_loss=16.973104\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:15 INFO 140719711954752] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=16.9731044769\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] Epoch[31] Batch[5] avg_epoch_loss=16.928714\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=16.9287141164\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] Epoch[31] Batch [5]#011Speed: 965.92 samples/sec#011loss=16.928714\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] processed a total of 569 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 948.6260414123535, \"sum\": 948.6260414123535, \"min\": 948.6260414123535}}, \"EndTime\": 1564688476.221228, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688475.272098}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=599.739648307 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] #quality_metric: host=algo-1, epoch=31, train loss <loss>=16.9308923086\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] Epoch[32] Batch[0] avg_epoch_loss=16.909883\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=16.9098834991\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] Epoch[32] Batch[5] avg_epoch_loss=16.892026\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=16.8920256297\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:16 INFO 140719711954752] Epoch[32] Batch [5]#011Speed: 960.82 samples/sec#011loss=16.892026\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:17 INFO 140719711954752] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1033.9069366455078, \"sum\": 1033.9069366455078, \"min\": 1033.9069366455078}}, \"EndTime\": 1564688477.255715, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688476.22131}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:17 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=577.35710305 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:17 INFO 140719711954752] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:17 INFO 140719711954752] #quality_metric: host=algo-1, epoch=32, train loss <loss>=16.9091489792\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:17 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:17 INFO 140719711954752] Epoch[33] Batch[0] avg_epoch_loss=16.929108\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:17 INFO 140719711954752] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=16.929107666\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:18 INFO 140719711954752] Epoch[33] Batch[5] avg_epoch_loss=16.931273\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:18 INFO 140719711954752] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=16.9312728246\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:18 INFO 140719711954752] Epoch[33] Batch [5]#011Speed: 954.77 samples/sec#011loss=16.931273\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:18 INFO 140719711954752] processed a total of 628 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1042.524814605713, \"sum\": 1042.524814605713, \"min\": 1042.524814605713}}, \"EndTime\": 1564688478.298733, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688477.255794}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:18 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=602.323547376 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:18 INFO 140719711954752] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:18 INFO 140719711954752] #quality_metric: host=algo-1, epoch=33, train loss <loss>=16.934557724\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:18 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:18 INFO 140719711954752] Epoch[34] Batch[0] avg_epoch_loss=16.954657\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:18 INFO 140719711954752] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=16.954656601\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:19 INFO 140719711954752] Epoch[34] Batch[5] avg_epoch_loss=16.937526\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:19 INFO 140719711954752] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=16.9375260671\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:19 INFO 140719711954752] Epoch[34] Batch [5]#011Speed: 970.48 samples/sec#011loss=16.937526\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:19 INFO 140719711954752] processed a total of 580 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1017.8217887878418, \"sum\": 1017.8217887878418, \"min\": 1017.8217887878418}}, \"EndTime\": 1564688479.317125, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688478.298807}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:19 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=569.785361082 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:19 INFO 140719711954752] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:19 INFO 140719711954752] #quality_metric: host=algo-1, epoch=34, train loss <loss>=16.8830665588\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:19 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:19 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_aa4ece27-46d7-44a4-b0e4-e1079a1a0e58-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.821809768676758, \"sum\": 20.821809768676758, \"min\": 20.821809768676758}}, \"EndTime\": 1564688479.338545, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688479.317192}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:19 INFO 140719711954752] Epoch[35] Batch[0] avg_epoch_loss=16.893600\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:19 INFO 140719711954752] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=16.8936004639\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:20 INFO 140719711954752] Epoch[35] Batch[5] avg_epoch_loss=16.860992\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:20 INFO 140719711954752] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=16.8609924316\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:20 INFO 140719711954752] Epoch[35] Batch [5]#011Speed: 938.09 samples/sec#011loss=16.860992\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:20 INFO 140719711954752] processed a total of 567 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 990.0259971618652, \"sum\": 990.0259971618652, \"min\": 990.0259971618652}}, \"EndTime\": 1564688480.328688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688479.338602}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:20 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=572.641626667 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:20 INFO 140719711954752] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:20 INFO 140719711954752] #quality_metric: host=algo-1, epoch=35, train loss <loss>=16.8767619663\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:20 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:20 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_81be40da-b872-4e11-b433-6aaae9160cd8-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.664018630981445, \"sum\": 26.664018630981445, \"min\": 26.664018630981445}}, \"EndTime\": 1564688480.35594, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688480.328771}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:20 INFO 140719711954752] Epoch[36] Batch[0] avg_epoch_loss=16.934765\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:20 INFO 140719711954752] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=16.9347648621\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:21 INFO 140719711954752] Epoch[36] Batch[5] avg_epoch_loss=16.919685\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:21 INFO 140719711954752] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=16.919684728\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:21 INFO 140719711954752] Epoch[36] Batch [5]#011Speed: 882.02 samples/sec#011loss=16.919685\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:21 INFO 140719711954752] processed a total of 550 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 982.1760654449463, \"sum\": 982.1760654449463, \"min\": 982.1760654449463}}, \"EndTime\": 1564688481.338236, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688480.356003}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:21 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=559.921261849 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:21 INFO 140719711954752] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:21 INFO 140719711954752] #quality_metric: host=algo-1, epoch=36, train loss <loss>=16.9019054837\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:21 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:21 INFO 140719711954752] Epoch[37] Batch[0] avg_epoch_loss=16.928858\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:21 INFO 140719711954752] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=16.9288578033\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:22 INFO 140719711954752] Epoch[37] Batch[5] avg_epoch_loss=16.835651\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:22 INFO 140719711954752] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=16.8356510798\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:22 INFO 140719711954752] Epoch[37] Batch [5]#011Speed: 956.44 samples/sec#011loss=16.835651\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:22 INFO 140719711954752] processed a total of 578 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1027.817964553833, \"sum\": 1027.817964553833, \"min\": 1027.817964553833}}, \"EndTime\": 1564688482.366594, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688481.338311}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:22 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=562.291304146 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:22 INFO 140719711954752] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:22 INFO 140719711954752] #quality_metric: host=algo-1, epoch=37, train loss <loss>=16.8964799881\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:22 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:22 INFO 140719711954752] Epoch[38] Batch[0] avg_epoch_loss=16.923296\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:22 INFO 140719711954752] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=16.9232959747\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:23 INFO 140719711954752] Epoch[38] Batch[5] avg_epoch_loss=16.924749\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:23 INFO 140719711954752] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=16.9247487386\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:23 INFO 140719711954752] Epoch[38] Batch [5]#011Speed: 948.22 samples/sec#011loss=16.924749\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:23 INFO 140719711954752] processed a total of 628 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1059.643030166626, \"sum\": 1059.643030166626, \"min\": 1059.643030166626}}, \"EndTime\": 1564688483.426775, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688482.366676}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:23 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=592.589881425 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:23 INFO 140719711954752] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:23 INFO 140719711954752] #quality_metric: host=algo-1, epoch=38, train loss <loss>=16.9142526627\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:23 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:23 INFO 140719711954752] Epoch[39] Batch[0] avg_epoch_loss=16.971554\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:23 INFO 140719711954752] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=16.9715538025\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:41:24 INFO 140719711954752] Epoch[39] Batch[5] avg_epoch_loss=16.931659\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:24 INFO 140719711954752] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=16.9316593806\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:24 INFO 140719711954752] Epoch[39] Batch [5]#011Speed: 960.63 samples/sec#011loss=16.931659\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:24 INFO 140719711954752] processed a total of 561 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 957.7929973602295, \"sum\": 957.7929973602295, \"min\": 957.7929973602295}}, \"EndTime\": 1564688484.385087, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688483.426853}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:24 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=585.665277384 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:24 INFO 140719711954752] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:24 INFO 140719711954752] #quality_metric: host=algo-1, epoch=39, train loss <loss>=16.935819202\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:24 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:24 INFO 140719711954752] Epoch[40] Batch[0] avg_epoch_loss=16.942770\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:24 INFO 140719711954752] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=16.9427700043\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:25 INFO 140719711954752] Epoch[40] Batch[5] avg_epoch_loss=16.895868\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:25 INFO 140719711954752] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=16.8958683014\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:25 INFO 140719711954752] Epoch[40] Batch [5]#011Speed: 936.47 samples/sec#011loss=16.895868\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:25 INFO 140719711954752] processed a total of 573 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 968.7869548797607, \"sum\": 968.7869548797607, \"min\": 968.7869548797607}}, \"EndTime\": 1564688485.354383, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688484.385148}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:25 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=591.391739637 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:25 INFO 140719711954752] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:25 INFO 140719711954752] #quality_metric: host=algo-1, epoch=40, train loss <loss>=16.8786587185\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:25 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:25 INFO 140719711954752] Epoch[41] Batch[0] avg_epoch_loss=16.903305\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:25 INFO 140719711954752] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=16.9033050537\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:26 INFO 140719711954752] Epoch[41] Batch[5] avg_epoch_loss=16.897937\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:26 INFO 140719711954752] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=16.8979365031\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:26 INFO 140719711954752] Epoch[41] Batch [5]#011Speed: 962.49 samples/sec#011loss=16.897937\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:26 INFO 140719711954752] processed a total of 583 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1019.5488929748535, \"sum\": 1019.5488929748535, \"min\": 1019.5488929748535}}, \"EndTime\": 1564688486.374519, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688485.35446}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:26 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=571.76041929 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:26 INFO 140719711954752] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:26 INFO 140719711954752] #quality_metric: host=algo-1, epoch=41, train loss <loss>=16.9306304932\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:26 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:26 INFO 140719711954752] Epoch[42] Batch[0] avg_epoch_loss=16.910858\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:26 INFO 140719711954752] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=16.9108581543\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:27 INFO 140719711954752] Epoch[42] Batch[5] avg_epoch_loss=16.889448\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:27 INFO 140719711954752] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=16.8894481659\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:27 INFO 140719711954752] Epoch[42] Batch [5]#011Speed: 964.94 samples/sec#011loss=16.889448\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:27 INFO 140719711954752] processed a total of 546 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 951.6220092773438, \"sum\": 951.6220092773438, \"min\": 951.6220092773438}}, \"EndTime\": 1564688487.326724, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688486.374588}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:27 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=573.68549993 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:27 INFO 140719711954752] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:27 INFO 140719711954752] #quality_metric: host=algo-1, epoch=42, train loss <loss>=16.9051176707\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:27 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:27 INFO 140719711954752] Epoch[43] Batch[0] avg_epoch_loss=16.892876\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:27 INFO 140719711954752] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=16.8928756714\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:28 INFO 140719711954752] Epoch[43] Batch[5] avg_epoch_loss=16.879945\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:28 INFO 140719711954752] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=16.8799448013\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:28 INFO 140719711954752] Epoch[43] Batch [5]#011Speed: 953.90 samples/sec#011loss=16.879945\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:28 INFO 140719711954752] processed a total of 624 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1076.5199661254883, \"sum\": 1076.5199661254883, \"min\": 1076.5199661254883}}, \"EndTime\": 1564688488.403774, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688487.326804}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:28 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=579.5965076 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:28 INFO 140719711954752] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:28 INFO 140719711954752] #quality_metric: host=algo-1, epoch=43, train loss <loss>=16.9050046921\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:28 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:28 INFO 140719711954752] Epoch[44] Batch[0] avg_epoch_loss=16.902567\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:28 INFO 140719711954752] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=16.9025669098\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:29 INFO 140719711954752] Epoch[44] Batch[5] avg_epoch_loss=16.905719\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:29 INFO 140719711954752] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=16.9057191213\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:29 INFO 140719711954752] Epoch[44] Batch [5]#011Speed: 966.30 samples/sec#011loss=16.905719\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:29 INFO 140719711954752] processed a total of 596 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1021.7041969299316, \"sum\": 1021.7041969299316, \"min\": 1021.7041969299316}}, \"EndTime\": 1564688489.425943, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688488.403835}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:29 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=583.271855733 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:29 INFO 140719711954752] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:29 INFO 140719711954752] #quality_metric: host=algo-1, epoch=44, train loss <loss>=16.9214172363\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:29 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:29 INFO 140719711954752] Epoch[45] Batch[0] avg_epoch_loss=16.932241\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:29 INFO 140719711954752] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=16.9322414398\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:30 INFO 140719711954752] Epoch[45] Batch[5] avg_epoch_loss=16.836655\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:30 INFO 140719711954752] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=16.8366546631\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:30 INFO 140719711954752] Epoch[45] Batch [5]#011Speed: 925.61 samples/sec#011loss=16.836655\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:30 INFO 140719711954752] processed a total of 552 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 975.1701354980469, \"sum\": 975.1701354980469, \"min\": 975.1701354980469}}, \"EndTime\": 1564688490.401628, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688489.426024}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:30 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=565.989617744 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:30 INFO 140719711954752] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:30 INFO 140719711954752] #quality_metric: host=algo-1, epoch=45, train loss <loss>=16.8194238875\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:30 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:30 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_dceb7fbd-1e79-47e9-9d5a-800e18876e33-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.39704132080078, \"sum\": 22.39704132080078, \"min\": 22.39704132080078}}, \"EndTime\": 1564688490.42466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688490.401701}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:30 INFO 140719711954752] Epoch[46] Batch[0] avg_epoch_loss=16.796453\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:30 INFO 140719711954752] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=16.796453476\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:31 INFO 140719711954752] Epoch[46] Batch[5] avg_epoch_loss=16.835922\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:31 INFO 140719711954752] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=16.8359222412\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:31 INFO 140719711954752] Epoch[46] Batch [5]#011Speed: 962.83 samples/sec#011loss=16.835922\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:31 INFO 140719711954752] processed a total of 598 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1020.0109481811523, \"sum\": 1020.0109481811523, \"min\": 1020.0109481811523}}, \"EndTime\": 1564688491.444798, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688490.424728}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:31 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=586.206694814 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:31 INFO 140719711954752] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:31 INFO 140719711954752] #quality_metric: host=algo-1, epoch=46, train loss <loss>=16.8542825699\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:31 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:31 INFO 140719711954752] Epoch[47] Batch[0] avg_epoch_loss=16.854555\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:31 INFO 140719711954752] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=16.85455513\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:32 INFO 140719711954752] Epoch[47] Batch[5] avg_epoch_loss=16.846638\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:32 INFO 140719711954752] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=16.8466377258\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:32 INFO 140719711954752] Epoch[47] Batch [5]#011Speed: 969.04 samples/sec#011loss=16.846638\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:32 INFO 140719711954752] processed a total of 552 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 959.0671062469482, \"sum\": 959.0671062469482, \"min\": 959.0671062469482}}, \"EndTime\": 1564688492.404351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688491.444869}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:32 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=575.490923535 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:32 INFO 140719711954752] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:32 INFO 140719711954752] #quality_metric: host=algo-1, epoch=47, train loss <loss>=16.8694135878\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:32 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:32 INFO 140719711954752] Epoch[48] Batch[0] avg_epoch_loss=16.857117\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:32 INFO 140719711954752] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=16.8571166992\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:33 INFO 140719711954752] Epoch[48] Batch[5] avg_epoch_loss=16.864490\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:33 INFO 140719711954752] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=16.8644901911\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:33 INFO 140719711954752] Epoch[48] Batch [5]#011Speed: 881.20 samples/sec#011loss=16.864490\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:33 INFO 140719711954752] processed a total of 581 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1059.009075164795, \"sum\": 1059.009075164795, \"min\": 1059.009075164795}}, \"EndTime\": 1564688493.463972, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688492.404427}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:33 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=548.563961669 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:33 INFO 140719711954752] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:33 INFO 140719711954752] #quality_metric: host=algo-1, epoch=48, train loss <loss>=16.8788093567\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:33 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:33 INFO 140719711954752] Epoch[49] Batch[0] avg_epoch_loss=16.804348\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:33 INFO 140719711954752] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=16.8043479919\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:41:34 INFO 140719711954752] Epoch[49] Batch[5] avg_epoch_loss=16.848460\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:34 INFO 140719711954752] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=16.8484595617\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:34 INFO 140719711954752] Epoch[49] Batch [5]#011Speed: 968.07 samples/sec#011loss=16.848460\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:34 INFO 140719711954752] processed a total of 622 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1041.6359901428223, \"sum\": 1041.6359901428223, \"min\": 1041.6359901428223}}, \"EndTime\": 1564688494.506142, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688493.464055}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:34 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=597.069390862 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:34 INFO 140719711954752] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:34 INFO 140719711954752] #quality_metric: host=algo-1, epoch=49, train loss <loss>=16.8446834564\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:34 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:34 INFO 140719711954752] Epoch[50] Batch[0] avg_epoch_loss=16.925718\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:34 INFO 140719711954752] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=16.9257183075\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:35 INFO 140719711954752] Epoch[50] Batch[5] avg_epoch_loss=16.831410\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:35 INFO 140719711954752] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=16.8314097722\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:35 INFO 140719711954752] Epoch[50] Batch [5]#011Speed: 968.29 samples/sec#011loss=16.831410\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:35 INFO 140719711954752] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1032.8600406646729, \"sum\": 1032.8600406646729, \"min\": 1032.8600406646729}}, \"EndTime\": 1564688495.539522, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688494.506225}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:35 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=577.947976824 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:35 INFO 140719711954752] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:35 INFO 140719711954752] #quality_metric: host=algo-1, epoch=50, train loss <loss>=16.8299571991\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:35 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:35 INFO 140719711954752] Epoch[51] Batch[0] avg_epoch_loss=16.878584\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:35 INFO 140719711954752] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=16.8785839081\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:36 INFO 140719711954752] Epoch[51] Batch[5] avg_epoch_loss=16.864823\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:36 INFO 140719711954752] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=16.8648233414\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:36 INFO 140719711954752] Epoch[51] Batch [5]#011Speed: 934.18 samples/sec#011loss=16.864823\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:36 INFO 140719711954752] processed a total of 606 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1056.3828945159912, \"sum\": 1056.3828945159912, \"min\": 1056.3828945159912}}, \"EndTime\": 1564688496.596441, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688495.539598}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:36 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=573.604628107 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:36 INFO 140719711954752] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:36 INFO 140719711954752] #quality_metric: host=algo-1, epoch=51, train loss <loss>=16.8794130325\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:36 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] Epoch[52] Batch[0] avg_epoch_loss=16.866236\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=16.866235733\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] Epoch[52] Batch[5] avg_epoch_loss=16.841283\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=16.8412825267\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] Epoch[52] Batch [5]#011Speed: 967.05 samples/sec#011loss=16.841283\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] processed a total of 568 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 972.9390144348145, \"sum\": 972.9390144348145, \"min\": 972.9390144348145}}, \"EndTime\": 1564688497.569947, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688496.596503}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=583.726203261 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] #quality_metric: host=algo-1, epoch=52, train loss <loss>=16.8312397003\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] Epoch[53] Batch[0] avg_epoch_loss=16.889547\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:37 INFO 140719711954752] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=16.889547348\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:38 INFO 140719711954752] Epoch[53] Batch[5] avg_epoch_loss=16.821994\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:38 INFO 140719711954752] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=16.8219938278\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:38 INFO 140719711954752] Epoch[53] Batch [5]#011Speed: 956.06 samples/sec#011loss=16.821994\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:38 INFO 140719711954752] processed a total of 566 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 966.8409824371338, \"sum\": 966.8409824371338, \"min\": 966.8409824371338}}, \"EndTime\": 1564688498.537318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688497.570029}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:38 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=585.339649345 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:38 INFO 140719711954752] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:38 INFO 140719711954752] #quality_metric: host=algo-1, epoch=53, train loss <loss>=16.8237569597\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:38 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:38 INFO 140719711954752] Epoch[54] Batch[0] avg_epoch_loss=16.778967\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:38 INFO 140719711954752] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=16.7789669037\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:39 INFO 140719711954752] Epoch[54] Batch[5] avg_epoch_loss=16.863959\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:39 INFO 140719711954752] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=16.8639586767\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:39 INFO 140719711954752] Epoch[54] Batch [5]#011Speed: 980.18 samples/sec#011loss=16.863959\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:39 INFO 140719711954752] processed a total of 569 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 957.172155380249, \"sum\": 957.172155380249, \"min\": 957.172155380249}}, \"EndTime\": 1564688499.495035, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688498.537399}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:39 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=594.395603399 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:39 INFO 140719711954752] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:39 INFO 140719711954752] #quality_metric: host=algo-1, epoch=54, train loss <loss>=16.8544063568\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:39 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:39 INFO 140719711954752] Epoch[55] Batch[0] avg_epoch_loss=16.875128\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:39 INFO 140719711954752] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=16.8751277924\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:40 INFO 140719711954752] Epoch[55] Batch[5] avg_epoch_loss=16.878812\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:40 INFO 140719711954752] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=16.8788118362\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:40 INFO 140719711954752] Epoch[55] Batch [5]#011Speed: 973.52 samples/sec#011loss=16.878812\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:40 INFO 140719711954752] processed a total of 579 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1025.374174118042, \"sum\": 1025.374174118042, \"min\": 1025.374174118042}}, \"EndTime\": 1564688500.520927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688499.495108}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:40 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=564.607063337 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:40 INFO 140719711954752] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:40 INFO 140719711954752] #quality_metric: host=algo-1, epoch=55, train loss <loss>=16.8530376434\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:40 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:40 INFO 140719711954752] Epoch[56] Batch[0] avg_epoch_loss=16.845627\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:40 INFO 140719711954752] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=16.8456268311\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:41 INFO 140719711954752] Epoch[56] Batch[5] avg_epoch_loss=16.932492\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:41 INFO 140719711954752] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=16.9324916204\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:41 INFO 140719711954752] Epoch[56] Batch [5]#011Speed: 962.41 samples/sec#011loss=16.932492\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:41 INFO 140719711954752] processed a total of 594 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1038.6731624603271, \"sum\": 1038.6731624603271, \"min\": 1038.6731624603271}}, \"EndTime\": 1564688501.560177, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688500.521009}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:41 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=571.819011329 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:41 INFO 140719711954752] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:41 INFO 140719711954752] #quality_metric: host=algo-1, epoch=56, train loss <loss>=16.9245079041\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:41 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:42 INFO 140719711954752] Epoch[57] Batch[0] avg_epoch_loss=16.880802\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:42 INFO 140719711954752] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=16.8808021545\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:42 INFO 140719711954752] Epoch[57] Batch[5] avg_epoch_loss=16.868771\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:42 INFO 140719711954752] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=16.8687705994\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:42 INFO 140719711954752] Epoch[57] Batch [5]#011Speed: 943.54 samples/sec#011loss=16.868771\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:42 INFO 140719711954752] processed a total of 599 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1060.9498023986816, \"sum\": 1060.9498023986816, \"min\": 1060.9498023986816}}, \"EndTime\": 1564688502.621672, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688501.560257}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:42 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=564.532879859 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:42 INFO 140719711954752] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:42 INFO 140719711954752] #quality_metric: host=algo-1, epoch=57, train loss <loss>=16.8415000916\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:42 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:43 INFO 140719711954752] Epoch[58] Batch[0] avg_epoch_loss=16.917828\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=16.9178276062\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:43 INFO 140719711954752] Epoch[58] Batch[5] avg_epoch_loss=16.884411\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=16.8844114939\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:43 INFO 140719711954752] Epoch[58] Batch [5]#011Speed: 871.65 samples/sec#011loss=16.884411\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:43 INFO 140719711954752] processed a total of 575 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1029.675006866455, \"sum\": 1029.675006866455, \"min\": 1029.675006866455}}, \"EndTime\": 1564688503.651886, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688502.621744}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:43 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=558.363077895 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:43 INFO 140719711954752] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=58, train loss <loss>=16.875061459\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:43 INFO 140719711954752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:41:44 INFO 140719711954752] Epoch[59] Batch[0] avg_epoch_loss=16.670790\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:44 INFO 140719711954752] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=16.6707897186\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:44 INFO 140719711954752] Epoch[59] Batch[5] avg_epoch_loss=16.806519\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:44 INFO 140719711954752] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=16.8065188726\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:44 INFO 140719711954752] Epoch[59] Batch [5]#011Speed: 968.88 samples/sec#011loss=16.806519\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:44 INFO 140719711954752] processed a total of 564 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 977.0262241363525, \"sum\": 977.0262241363525, \"min\": 977.0262241363525}}, \"EndTime\": 1564688504.629457, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688503.651969}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:44 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=577.193995648 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:44 INFO 140719711954752] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:44 INFO 140719711954752] #quality_metric: host=algo-1, epoch=59, train loss <loss>=16.8125752343\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:44 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:44 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_f67b2ce5-d2f5-4ade-b1cf-f5590ef49fee-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.73916244506836, \"sum\": 32.73916244506836, \"min\": 32.73916244506836}}, \"EndTime\": 1564688504.662748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688504.629537}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:45 INFO 140719711954752] Epoch[60] Batch[0] avg_epoch_loss=16.668720\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=16.6687202454\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:45 INFO 140719711954752] Epoch[60] Batch[5] avg_epoch_loss=16.807606\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=16.8076057434\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:45 INFO 140719711954752] Epoch[60] Batch [5]#011Speed: 909.82 samples/sec#011loss=16.807606\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:45 INFO 140719711954752] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1059.6060752868652, \"sum\": 1059.6060752868652, \"min\": 1059.6060752868652}}, \"EndTime\": 1564688505.722471, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688504.662812}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:45 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=563.355323394 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:45 INFO 140719711954752] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=60, train loss <loss>=16.8065191269\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:45 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:45 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_b41c98a6-5f3e-49e3-8cc3-eb9f7f09bfa6-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.105932235717773, \"sum\": 22.105932235717773, \"min\": 22.105932235717773}}, \"EndTime\": 1564688505.7452, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688505.722554}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:46 INFO 140719711954752] Epoch[61] Batch[0] avg_epoch_loss=16.797338\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=16.7973384857\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:46 INFO 140719711954752] Epoch[61] Batch[5] avg_epoch_loss=16.802283\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=16.8022826513\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:46 INFO 140719711954752] Epoch[61] Batch [5]#011Speed: 962.12 samples/sec#011loss=16.802283\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:46 INFO 140719711954752] processed a total of 589 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1032.0448875427246, \"sum\": 1032.0448875427246, \"min\": 1032.0448875427246}}, \"EndTime\": 1564688506.777333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688505.745238}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:46 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=570.65294636 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:46 INFO 140719711954752] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=61, train loss <loss>=16.807052803\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:46 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:47 INFO 140719711954752] Epoch[62] Batch[0] avg_epoch_loss=16.868484\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:47 INFO 140719711954752] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=16.8684844971\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:47 INFO 140719711954752] Epoch[62] Batch[5] avg_epoch_loss=16.879996\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:47 INFO 140719711954752] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=16.8799959819\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:47 INFO 140719711954752] Epoch[62] Batch [5]#011Speed: 964.49 samples/sec#011loss=16.879996\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:47 INFO 140719711954752] processed a total of 625 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1021.4128494262695, \"sum\": 1021.4128494262695, \"min\": 1021.4128494262695}}, \"EndTime\": 1564688507.799337, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688506.777407}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:47 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=611.838119076 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:47 INFO 140719711954752] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:47 INFO 140719711954752] #quality_metric: host=algo-1, epoch=62, train loss <loss>=16.8762878418\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:47 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:48 INFO 140719711954752] Epoch[63] Batch[0] avg_epoch_loss=16.856396\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:48 INFO 140719711954752] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=16.8563957214\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:48 INFO 140719711954752] Epoch[63] Batch[5] avg_epoch_loss=16.808468\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:48 INFO 140719711954752] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=16.8084681829\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:48 INFO 140719711954752] Epoch[63] Batch [5]#011Speed: 941.68 samples/sec#011loss=16.808468\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:48 INFO 140719711954752] processed a total of 625 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1027.451992034912, \"sum\": 1027.451992034912, \"min\": 1027.451992034912}}, \"EndTime\": 1564688508.827296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688507.799398}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:48 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=608.231063952 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:48 INFO 140719711954752] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:48 INFO 140719711954752] #quality_metric: host=algo-1, epoch=63, train loss <loss>=16.8018373489\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:48 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:48 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_bc376565-1b20-4bcf-9764-44723ba85d87-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.8808650970459, \"sum\": 21.8808650970459, \"min\": 21.8808650970459}}, \"EndTime\": 1564688508.849771, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688508.827377}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:49 INFO 140719711954752] Epoch[64] Batch[0] avg_epoch_loss=16.632214\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:49 INFO 140719711954752] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=16.6322135925\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:49 INFO 140719711954752] Epoch[64] Batch[5] avg_epoch_loss=16.824991\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:49 INFO 140719711954752] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=16.8249912262\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:49 INFO 140719711954752] Epoch[64] Batch [5]#011Speed: 955.71 samples/sec#011loss=16.824991\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:49 INFO 140719711954752] processed a total of 620 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1037.5909805297852, \"sum\": 1037.5909805297852, \"min\": 1037.5909805297852}}, \"EndTime\": 1564688509.887466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688508.849818}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:49 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=597.468220417 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:49 INFO 140719711954752] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:49 INFO 140719711954752] #quality_metric: host=algo-1, epoch=64, train loss <loss>=16.8391002655\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:49 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:50 INFO 140719711954752] Epoch[65] Batch[0] avg_epoch_loss=16.839325\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=16.8393249512\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:50 INFO 140719711954752] Epoch[65] Batch[5] avg_epoch_loss=16.850579\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=16.8505792618\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:50 INFO 140719711954752] Epoch[65] Batch [5]#011Speed: 959.71 samples/sec#011loss=16.850579\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:50 INFO 140719711954752] processed a total of 581 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1041.8999195098877, \"sum\": 1041.8999195098877, \"min\": 1041.8999195098877}}, \"EndTime\": 1564688510.929898, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688509.88755}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:50 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=557.574144788 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:50 INFO 140719711954752] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=65, train loss <loss>=16.7914894104\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:50 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:50 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_7d87c894-1293-42db-bb16-cf0515d5c467-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.147987365722656, \"sum\": 24.147987365722656, \"min\": 24.147987365722656}}, \"EndTime\": 1564688510.954593, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688510.929977}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:51 INFO 140719711954752] Epoch[66] Batch[0] avg_epoch_loss=16.794870\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=16.7948703766\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:51 INFO 140719711954752] Epoch[66] Batch[5] avg_epoch_loss=16.833654\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=16.8336537679\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:51 INFO 140719711954752] Epoch[66] Batch [5]#011Speed: 953.52 samples/sec#011loss=16.833654\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:51 INFO 140719711954752] processed a total of 576 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 986.7899417877197, \"sum\": 986.7899417877197, \"min\": 986.7899417877197}}, \"EndTime\": 1564688511.941505, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688510.954657}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:51 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=583.642180576 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:51 INFO 140719711954752] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=66, train loss <loss>=16.8260538313\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:51 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:52 INFO 140719711954752] Epoch[67] Batch[0] avg_epoch_loss=16.675192\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=16.6751918793\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:52 INFO 140719711954752] Epoch[67] Batch[5] avg_epoch_loss=16.825828\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=16.8258282344\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:52 INFO 140719711954752] Epoch[67] Batch [5]#011Speed: 957.95 samples/sec#011loss=16.825828\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:52 INFO 140719711954752] processed a total of 566 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 963.2999897003174, \"sum\": 963.2999897003174, \"min\": 963.2999897003174}}, \"EndTime\": 1564688512.905335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688511.941585}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:52 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=587.491614178 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:52 INFO 140719711954752] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=67, train loss <loss>=16.8112430573\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:52 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:53 INFO 140719711954752] Epoch[68] Batch[0] avg_epoch_loss=16.881067\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=16.881067276\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:53 INFO 140719711954752] Epoch[68] Batch[5] avg_epoch_loss=16.773683\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=16.7736825943\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:53 INFO 140719711954752] Epoch[68] Batch [5]#011Speed: 952.56 samples/sec#011loss=16.773683\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:53 INFO 140719711954752] processed a total of 574 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 970.9720611572266, \"sum\": 970.9720611572266, \"min\": 970.9720611572266}}, \"EndTime\": 1564688513.876834, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688512.905416}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:53 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=591.089042539 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:53 INFO 140719711954752] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=68, train loss <loss>=16.7968970405\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:53 INFO 140719711954752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:41:54 INFO 140719711954752] Epoch[69] Batch[0] avg_epoch_loss=16.813234\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=16.8132343292\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:54 INFO 140719711954752] Epoch[69] Batch[5] avg_epoch_loss=16.785878\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=16.7858784993\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:54 INFO 140719711954752] Epoch[69] Batch [5]#011Speed: 962.95 samples/sec#011loss=16.785878\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:54 INFO 140719711954752] processed a total of 560 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 959.5580101013184, \"sum\": 959.5580101013184, \"min\": 959.5580101013184}}, \"EndTime\": 1564688514.836915, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688513.876915}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:54 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=583.531562938 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:54 INFO 140719711954752] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=69, train loss <loss>=16.8117955526\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:54 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:55 INFO 140719711954752] Epoch[70] Batch[0] avg_epoch_loss=16.818489\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:55 INFO 140719711954752] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=16.8184890747\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:55 INFO 140719711954752] Epoch[70] Batch[5] avg_epoch_loss=16.792516\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:55 INFO 140719711954752] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=16.7925160726\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:55 INFO 140719711954752] Epoch[70] Batch [5]#011Speed: 907.28 samples/sec#011loss=16.792516\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:55 INFO 140719711954752] processed a total of 574 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 996.5019226074219, \"sum\": 996.5019226074219, \"min\": 996.5019226074219}}, \"EndTime\": 1564688515.83394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688514.836994}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:55 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=575.947285762 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:55 INFO 140719711954752] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:55 INFO 140719711954752] #quality_metric: host=algo-1, epoch=70, train loss <loss>=16.7898860508\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:55 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:55 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_54984723-085b-4210-b92a-99fb9dd7e740-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 28.267860412597656, \"sum\": 28.267860412597656, \"min\": 28.267860412597656}}, \"EndTime\": 1564688515.862782, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688515.83402}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:56 INFO 140719711954752] Epoch[71] Batch[0] avg_epoch_loss=16.865757\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=16.8657569885\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:56 INFO 140719711954752] Epoch[71] Batch[5] avg_epoch_loss=16.824568\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=16.8245677948\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:56 INFO 140719711954752] Epoch[71] Batch [5]#011Speed: 932.71 samples/sec#011loss=16.824568\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:56 INFO 140719711954752] processed a total of 608 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1041.9130325317383, \"sum\": 1041.9130325317383, \"min\": 1041.9130325317383}}, \"EndTime\": 1564688516.904795, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688515.862827}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:56 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=583.475895616 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:56 INFO 140719711954752] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=71, train loss <loss>=16.8582611084\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:56 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:57 INFO 140719711954752] Epoch[72] Batch[0] avg_epoch_loss=16.746983\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:57 INFO 140719711954752] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=16.7469825745\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:57 INFO 140719711954752] Epoch[72] Batch[5] avg_epoch_loss=16.789222\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:57 INFO 140719711954752] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=16.7892220815\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:57 INFO 140719711954752] Epoch[72] Batch [5]#011Speed: 963.80 samples/sec#011loss=16.789222\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] Epoch[72] Batch[10] avg_epoch_loss=16.830200\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=16.8793739319\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] Epoch[72] Batch [10]#011Speed: 949.05 samples/sec#011loss=16.879374\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] processed a total of 645 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1098.0050563812256, \"sum\": 1098.0050563812256, \"min\": 1098.0050563812256}}, \"EndTime\": 1564688518.003337, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688516.904875}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=587.36802476 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=72, train loss <loss>=16.8302001953\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] Epoch[73] Batch[0] avg_epoch_loss=16.753395\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=16.7533950806\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] Epoch[73] Batch[5] avg_epoch_loss=16.776439\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=16.7764387131\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:58 INFO 140719711954752] Epoch[73] Batch [5]#011Speed: 956.00 samples/sec#011loss=16.776439\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] processed a total of 583 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1043.5020923614502, \"sum\": 1043.5020923614502, \"min\": 1043.5020923614502}}, \"EndTime\": 1564688519.047315, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688518.003415}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=558.633542376 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=73, train loss <loss>=16.8114740372\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] Epoch[74] Batch[0] avg_epoch_loss=16.798185\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=16.7981853485\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] Epoch[74] Batch[5] avg_epoch_loss=16.776817\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=16.776816686\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] Epoch[74] Batch [5]#011Speed: 968.10 samples/sec#011loss=16.776817\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] processed a total of 569 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 943.5250759124756, \"sum\": 943.5250759124756, \"min\": 943.5250759124756}}, \"EndTime\": 1564688519.991363, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688519.047395}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=602.982212401 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=74, train loss <loss>=16.7795668708\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:41:59 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:00 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_d3e9f068-211a-43d5-bb3e-52123966b0f8-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.134065628051758, \"sum\": 22.134065628051758, \"min\": 22.134065628051758}}, \"EndTime\": 1564688520.014108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688519.991444}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:00 INFO 140719711954752] Epoch[75] Batch[0] avg_epoch_loss=16.710175\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=16.7101745605\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:00 INFO 140719711954752] Epoch[75] Batch[5] avg_epoch_loss=16.822426\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=16.8224261602\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:00 INFO 140719711954752] Epoch[75] Batch [5]#011Speed: 965.83 samples/sec#011loss=16.822426\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:00 INFO 140719711954752] processed a total of 566 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 937.5810623168945, \"sum\": 937.5810623168945, \"min\": 937.5810623168945}}, \"EndTime\": 1564688520.951815, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688520.014172}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:00 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=603.607152029 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:00 INFO 140719711954752] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=75, train loss <loss>=16.8162742191\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:00 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:01 INFO 140719711954752] Epoch[76] Batch[0] avg_epoch_loss=16.849716\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:01 INFO 140719711954752] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=16.8497161865\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:01 INFO 140719711954752] Epoch[76] Batch[5] avg_epoch_loss=16.757515\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:01 INFO 140719711954752] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=16.7575146357\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:01 INFO 140719711954752] Epoch[76] Batch [5]#011Speed: 936.79 samples/sec#011loss=16.757515\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] Epoch[76] Batch[10] avg_epoch_loss=16.795325\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=16.8406970978\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] Epoch[76] Batch [10]#011Speed: 937.17 samples/sec#011loss=16.840697\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] processed a total of 652 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1101.4149188995361, \"sum\": 1101.4149188995361, \"min\": 1101.4149188995361}}, \"EndTime\": 1564688522.053767, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688520.951894}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=591.906253801 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=76, train loss <loss>=16.7953248457\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] Epoch[77] Batch[0] avg_epoch_loss=16.897070\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=16.897069931\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] Epoch[77] Batch[5] avg_epoch_loss=16.828725\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=16.8287254969\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:02 INFO 140719711954752] Epoch[77] Batch [5]#011Speed: 940.80 samples/sec#011loss=16.828725\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:03 INFO 140719711954752] processed a total of 638 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1036.419153213501, \"sum\": 1036.419153213501, \"min\": 1036.419153213501}}, \"EndTime\": 1564688523.090685, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688522.053844}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:03 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=615.512244962 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:03 INFO 140719711954752] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=77, train loss <loss>=16.8265649796\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:03 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:03 INFO 140719711954752] Epoch[78] Batch[0] avg_epoch_loss=16.953093\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=16.9530925751\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:03 INFO 140719711954752] Epoch[78] Batch[5] avg_epoch_loss=16.873761\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=16.873761495\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:03 INFO 140719711954752] Epoch[78] Batch [5]#011Speed: 966.78 samples/sec#011loss=16.873761\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:42:04 INFO 140719711954752] processed a total of 588 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1036.5631580352783, \"sum\": 1036.5631580352783, \"min\": 1036.5631580352783}}, \"EndTime\": 1564688524.127782, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688523.090764}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:04 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=567.195808237 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:04 INFO 140719711954752] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=78, train loss <loss>=16.8706111908\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:04 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:04 INFO 140719711954752] Epoch[79] Batch[0] avg_epoch_loss=16.758881\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=16.7588806152\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:04 INFO 140719711954752] Epoch[79] Batch[5] avg_epoch_loss=16.811909\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=16.8119093577\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:04 INFO 140719711954752] Epoch[79] Batch [5]#011Speed: 918.35 samples/sec#011loss=16.811909\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:05 INFO 140719711954752] processed a total of 602 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1055.3560256958008, \"sum\": 1055.3560256958008, \"min\": 1055.3560256958008}}, \"EndTime\": 1564688525.18367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688524.127862}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:05 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=570.35931875 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:05 INFO 140719711954752] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=79, train loss <loss>=16.815955925\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:05 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:05 INFO 140719711954752] Epoch[80] Batch[0] avg_epoch_loss=16.851337\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=16.8513374329\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:05 INFO 140719711954752] Epoch[80] Batch[5] avg_epoch_loss=16.746262\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=16.7462622325\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:05 INFO 140719711954752] Epoch[80] Batch [5]#011Speed: 937.64 samples/sec#011loss=16.746262\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:06 INFO 140719711954752] processed a total of 570 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 983.0758571624756, \"sum\": 983.0758571624756, \"min\": 983.0758571624756}}, \"EndTime\": 1564688526.167279, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688525.183751}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:06 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=579.74380003 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:06 INFO 140719711954752] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:06 INFO 140719711954752] #quality_metric: host=algo-1, epoch=80, train loss <loss>=16.7441474067\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:06 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:06 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_3351d16f-7cbe-4f2e-b818-319687c5438c-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.817062377929688, \"sum\": 23.817062377929688, \"min\": 23.817062377929688}}, \"EndTime\": 1564688526.191655, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688526.16736}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:06 INFO 140719711954752] Epoch[81] Batch[0] avg_epoch_loss=16.646889\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:06 INFO 140719711954752] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=16.6468887329\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:06 INFO 140719711954752] Epoch[81] Batch[5] avg_epoch_loss=16.789195\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:06 INFO 140719711954752] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=16.7891950607\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:06 INFO 140719711954752] Epoch[81] Batch [5]#011Speed: 930.54 samples/sec#011loss=16.789195\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:07 INFO 140719711954752] processed a total of 606 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1050.4188537597656, \"sum\": 1050.4188537597656, \"min\": 1050.4188537597656}}, \"EndTime\": 1564688527.242208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688526.191728}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:07 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=576.846209641 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:07 INFO 140719711954752] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:07 INFO 140719711954752] #quality_metric: host=algo-1, epoch=81, train loss <loss>=16.7939998627\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:07 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:07 INFO 140719711954752] Epoch[82] Batch[0] avg_epoch_loss=16.736465\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:07 INFO 140719711954752] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=16.7364654541\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:08 INFO 140719711954752] Epoch[82] Batch[5] avg_epoch_loss=16.782164\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=16.7821642558\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:08 INFO 140719711954752] Epoch[82] Batch [5]#011Speed: 968.12 samples/sec#011loss=16.782164\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:08 INFO 140719711954752] processed a total of 604 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1057.2590827941895, \"sum\": 1057.2590827941895, \"min\": 1057.2590827941895}}, \"EndTime\": 1564688528.300011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688527.242291}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:08 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=571.225937958 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:08 INFO 140719711954752] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=82, train loss <loss>=16.7984554291\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:08 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:08 INFO 140719711954752] Epoch[83] Batch[0] avg_epoch_loss=16.841377\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=16.8413772583\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:09 INFO 140719711954752] Epoch[83] Batch[5] avg_epoch_loss=16.828641\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:09 INFO 140719711954752] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=16.8286412557\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:09 INFO 140719711954752] Epoch[83] Batch [5]#011Speed: 926.93 samples/sec#011loss=16.828641\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:09 INFO 140719711954752] processed a total of 598 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1098.3710289001465, \"sum\": 1098.3710289001465, \"min\": 1098.3710289001465}}, \"EndTime\": 1564688529.398918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688528.300091}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:09 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=544.383654038 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:09 INFO 140719711954752] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:09 INFO 140719711954752] #quality_metric: host=algo-1, epoch=83, train loss <loss>=16.8226568222\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:09 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:09 INFO 140719711954752] Epoch[84] Batch[0] avg_epoch_loss=16.830090\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:09 INFO 140719711954752] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=16.8300895691\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:10 INFO 140719711954752] Epoch[84] Batch[5] avg_epoch_loss=16.827452\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:10 INFO 140719711954752] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=16.8274520238\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:10 INFO 140719711954752] Epoch[84] Batch [5]#011Speed: 965.31 samples/sec#011loss=16.827452\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:10 INFO 140719711954752] processed a total of 554 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 977.9481887817383, \"sum\": 977.9481887817383, \"min\": 977.9481887817383}}, \"EndTime\": 1564688530.377394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688529.399001}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:10 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=566.424513846 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:10 INFO 140719711954752] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:10 INFO 140719711954752] #quality_metric: host=algo-1, epoch=84, train loss <loss>=16.8080690172\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:10 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:10 INFO 140719711954752] Epoch[85] Batch[0] avg_epoch_loss=16.719746\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:10 INFO 140719711954752] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=16.719745636\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:11 INFO 140719711954752] Epoch[85] Batch[5] avg_epoch_loss=16.728649\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:11 INFO 140719711954752] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=16.7286485036\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:11 INFO 140719711954752] Epoch[85] Batch [5]#011Speed: 945.53 samples/sec#011loss=16.728649\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:11 INFO 140719711954752] processed a total of 604 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1035.3858470916748, \"sum\": 1035.3858470916748, \"min\": 1035.3858470916748}}, \"EndTime\": 1564688531.413306, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688530.377475}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:11 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=583.290246897 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:11 INFO 140719711954752] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:11 INFO 140719711954752] #quality_metric: host=algo-1, epoch=85, train loss <loss>=16.7637958527\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:11 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:11 INFO 140719711954752] Epoch[86] Batch[0] avg_epoch_loss=16.689007\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:11 INFO 140719711954752] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=16.6890068054\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:12 INFO 140719711954752] Epoch[86] Batch[5] avg_epoch_loss=16.773865\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:12 INFO 140719711954752] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=16.7738647461\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:12 INFO 140719711954752] Epoch[86] Batch [5]#011Speed: 813.33 samples/sec#011loss=16.773865\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:12 INFO 140719711954752] processed a total of 610 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1092.4289226531982, \"sum\": 1092.4289226531982, \"min\": 1092.4289226531982}}, \"EndTime\": 1564688532.506256, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688531.413388}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:12 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=558.343766122 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:12 INFO 140719711954752] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:12 INFO 140719711954752] #quality_metric: host=algo-1, epoch=86, train loss <loss>=16.7821046829\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:12 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:12 INFO 140719711954752] Epoch[87] Batch[0] avg_epoch_loss=16.843086\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:12 INFO 140719711954752] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=16.8430862427\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:13 INFO 140719711954752] Epoch[87] Batch[5] avg_epoch_loss=16.839006\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:13 INFO 140719711954752] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=16.8390057882\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:13 INFO 140719711954752] Epoch[87] Batch [5]#011Speed: 963.11 samples/sec#011loss=16.839006\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:13 INFO 140719711954752] processed a total of 531 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 965.8060073852539, \"sum\": 965.8060073852539, \"min\": 965.8060073852539}}, \"EndTime\": 1564688533.472576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688532.506315}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:13 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=549.731455985 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:13 INFO 140719711954752] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:13 INFO 140719711954752] #quality_metric: host=algo-1, epoch=87, train loss <loss>=16.8114291297\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:13 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:13 INFO 140719711954752] Epoch[88] Batch[0] avg_epoch_loss=16.685158\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:13 INFO 140719711954752] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=16.6851577759\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:42:14 INFO 140719711954752] Epoch[88] Batch[5] avg_epoch_loss=16.813820\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:14 INFO 140719711954752] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=16.8138195674\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:14 INFO 140719711954752] Epoch[88] Batch [5]#011Speed: 970.71 samples/sec#011loss=16.813820\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:14 INFO 140719711954752] processed a total of 580 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1006.0639381408691, \"sum\": 1006.0639381408691, \"min\": 1006.0639381408691}}, \"EndTime\": 1564688534.479208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688533.472665}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:14 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=576.434173331 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:14 INFO 140719711954752] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:14 INFO 140719711954752] #quality_metric: host=algo-1, epoch=88, train loss <loss>=16.8082811356\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:14 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:14 INFO 140719711954752] Epoch[89] Batch[0] avg_epoch_loss=16.867130\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:14 INFO 140719711954752] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=16.8671302795\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:15 INFO 140719711954752] Epoch[89] Batch[5] avg_epoch_loss=16.807258\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:15 INFO 140719711954752] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=16.8072582881\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:15 INFO 140719711954752] Epoch[89] Batch [5]#011Speed: 962.51 samples/sec#011loss=16.807258\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:15 INFO 140719711954752] processed a total of 575 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 960.26611328125, \"sum\": 960.26611328125, \"min\": 960.26611328125}}, \"EndTime\": 1564688535.440019, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688534.479291}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:15 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=598.719507624 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:15 INFO 140719711954752] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:15 INFO 140719711954752] #quality_metric: host=algo-1, epoch=89, train loss <loss>=16.8174733056\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:15 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:15 INFO 140719711954752] Epoch[90] Batch[0] avg_epoch_loss=16.853865\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:15 INFO 140719711954752] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=16.8538646698\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:16 INFO 140719711954752] Epoch[90] Batch[5] avg_epoch_loss=16.796301\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:16 INFO 140719711954752] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=16.796301206\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:16 INFO 140719711954752] Epoch[90] Batch [5]#011Speed: 968.55 samples/sec#011loss=16.796301\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:16 INFO 140719711954752] processed a total of 616 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1051.853895187378, \"sum\": 1051.853895187378, \"min\": 1051.853895187378}}, \"EndTime\": 1564688536.492402, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688535.440099}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:16 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=585.577449492 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:16 INFO 140719711954752] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:16 INFO 140719711954752] #quality_metric: host=algo-1, epoch=90, train loss <loss>=16.7871229172\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:16 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:16 INFO 140719711954752] Epoch[91] Batch[0] avg_epoch_loss=16.534582\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:16 INFO 140719711954752] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=16.5345821381\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:17 INFO 140719711954752] Epoch[91] Batch[5] avg_epoch_loss=16.754638\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:17 INFO 140719711954752] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=16.7546380361\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:17 INFO 140719711954752] Epoch[91] Batch [5]#011Speed: 943.50 samples/sec#011loss=16.754638\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:17 INFO 140719711954752] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1031.7270755767822, \"sum\": 1031.7270755767822, \"min\": 1031.7270755767822}}, \"EndTime\": 1564688537.524707, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688536.492469}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:17 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=578.577624846 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:17 INFO 140719711954752] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:17 INFO 140719711954752] #quality_metric: host=algo-1, epoch=91, train loss <loss>=16.7374624252\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:17 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:17 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_7eb95209-a36c-4a2b-b39f-78bbed50d2f1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.1099853515625, \"sum\": 22.1099853515625, \"min\": 22.1099853515625}}, \"EndTime\": 1564688537.547445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688537.524787}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:17 INFO 140719711954752] Epoch[92] Batch[0] avg_epoch_loss=16.866892\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:17 INFO 140719711954752] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=16.866891861\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:18 INFO 140719711954752] Epoch[92] Batch[5] avg_epoch_loss=16.808421\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:18 INFO 140719711954752] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=16.8084211349\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:18 INFO 140719711954752] Epoch[92] Batch [5]#011Speed: 955.12 samples/sec#011loss=16.808421\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:18 INFO 140719711954752] processed a total of 544 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 955.5060863494873, \"sum\": 955.5060863494873, \"min\": 955.5060863494873}}, \"EndTime\": 1564688538.503077, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688537.547511}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:18 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=569.260352688 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:18 INFO 140719711954752] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:18 INFO 140719711954752] #quality_metric: host=algo-1, epoch=92, train loss <loss>=16.8249475691\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:18 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:18 INFO 140719711954752] Epoch[93] Batch[0] avg_epoch_loss=16.628757\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:18 INFO 140719711954752] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=16.6287574768\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:19 INFO 140719711954752] Epoch[93] Batch[5] avg_epoch_loss=16.691167\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:19 INFO 140719711954752] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=16.6911668777\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:19 INFO 140719711954752] Epoch[93] Batch [5]#011Speed: 901.03 samples/sec#011loss=16.691167\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:19 INFO 140719711954752] processed a total of 573 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 998.2419013977051, \"sum\": 998.2419013977051, \"min\": 998.2419013977051}}, \"EndTime\": 1564688539.501875, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688538.503159}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:19 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=573.959129223 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:19 INFO 140719711954752] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:19 INFO 140719711954752] #quality_metric: host=algo-1, epoch=93, train loss <loss>=16.6919388241\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:19 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:19 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_5efc79ae-dc29-4205-9eb4-614aae9918f8-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.21512794494629, \"sum\": 22.21512794494629, \"min\": 22.21512794494629}}, \"EndTime\": 1564688539.524669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688539.501932}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:19 INFO 140719711954752] Epoch[94] Batch[0] avg_epoch_loss=16.799791\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:19 INFO 140719711954752] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=16.7997913361\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:20 INFO 140719711954752] Epoch[94] Batch[5] avg_epoch_loss=16.805073\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:20 INFO 140719711954752] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=16.8050734202\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:20 INFO 140719711954752] Epoch[94] Batch [5]#011Speed: 937.44 samples/sec#011loss=16.805073\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:20 INFO 140719711954752] processed a total of 577 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1029.2561054229736, \"sum\": 1029.2561054229736, \"min\": 1029.2561054229736}}, \"EndTime\": 1564688540.55403, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688539.524716}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:20 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=560.532575492 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:20 INFO 140719711954752] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:20 INFO 140719711954752] #quality_metric: host=algo-1, epoch=94, train loss <loss>=16.8161645889\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:20 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:20 INFO 140719711954752] Epoch[95] Batch[0] avg_epoch_loss=16.707090\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:20 INFO 140719711954752] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=16.7070903778\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:21 INFO 140719711954752] Epoch[95] Batch[5] avg_epoch_loss=16.765557\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:21 INFO 140719711954752] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=16.7655572891\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:21 INFO 140719711954752] Epoch[95] Batch [5]#011Speed: 961.33 samples/sec#011loss=16.765557\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:21 INFO 140719711954752] processed a total of 616 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1024.8169898986816, \"sum\": 1024.8169898986816, \"min\": 1024.8169898986816}}, \"EndTime\": 1564688541.579459, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688540.554113}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:21 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=601.012600209 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:21 INFO 140719711954752] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:21 INFO 140719711954752] #quality_metric: host=algo-1, epoch=95, train loss <loss>=16.7756261826\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:21 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:22 INFO 140719711954752] Epoch[96] Batch[0] avg_epoch_loss=16.826962\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:22 INFO 140719711954752] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=16.8269615173\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:22 INFO 140719711954752] Epoch[96] Batch[5] avg_epoch_loss=16.758825\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:22 INFO 140719711954752] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=16.7588249842\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:22 INFO 140719711954752] Epoch[96] Batch [5]#011Speed: 935.95 samples/sec#011loss=16.758825\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:22 INFO 140719711954752] processed a total of 636 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1100.5511283874512, \"sum\": 1100.5511283874512, \"min\": 1100.5511283874512}}, \"EndTime\": 1564688542.680551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688541.579542}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:22 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=577.830941641 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:22 INFO 140719711954752] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:22 INFO 140719711954752] #quality_metric: host=algo-1, epoch=96, train loss <loss>=16.7826257706\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:22 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:23 INFO 140719711954752] Epoch[97] Batch[0] avg_epoch_loss=16.945129\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:23 INFO 140719711954752] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=16.9451293945\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:23 INFO 140719711954752] Epoch[97] Batch[5] avg_epoch_loss=16.820761\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:23 INFO 140719711954752] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=16.8207607269\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:23 INFO 140719711954752] Epoch[97] Batch [5]#011Speed: 866.87 samples/sec#011loss=16.820761\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:23 INFO 140719711954752] processed a total of 605 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1070.1749324798584, \"sum\": 1070.1749324798584, \"min\": 1070.1749324798584}}, \"EndTime\": 1564688543.751347, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688542.680629}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:23 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=565.265170003 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:23 INFO 140719711954752] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:23 INFO 140719711954752] #quality_metric: host=algo-1, epoch=97, train loss <loss>=16.8357074738\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:23 INFO 140719711954752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:42:24 INFO 140719711954752] Epoch[98] Batch[0] avg_epoch_loss=16.637045\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:24 INFO 140719711954752] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=16.6370449066\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:24 INFO 140719711954752] Epoch[98] Batch[5] avg_epoch_loss=16.778758\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:24 INFO 140719711954752] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=16.778758049\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:24 INFO 140719711954752] Epoch[98] Batch [5]#011Speed: 958.19 samples/sec#011loss=16.778758\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:24 INFO 140719711954752] processed a total of 583 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1069.878101348877, \"sum\": 1069.878101348877, \"min\": 1069.878101348877}}, \"EndTime\": 1564688544.821742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688543.751428}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:24 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=544.859847078 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:24 INFO 140719711954752] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:24 INFO 140719711954752] #quality_metric: host=algo-1, epoch=98, train loss <loss>=16.7791797638\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:24 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:25 INFO 140719711954752] Epoch[99] Batch[0] avg_epoch_loss=16.775047\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:25 INFO 140719711954752] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=16.7750473022\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:25 INFO 140719711954752] Epoch[99] Batch[5] avg_epoch_loss=16.805145\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:25 INFO 140719711954752] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=16.8051446279\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:25 INFO 140719711954752] Epoch[99] Batch [5]#011Speed: 962.30 samples/sec#011loss=16.805145\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:25 INFO 140719711954752] processed a total of 508 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 874.4180202484131, \"sum\": 874.4180202484131, \"min\": 874.4180202484131}}, \"EndTime\": 1564688545.696702, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688544.821825}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:25 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=580.877535453 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:25 INFO 140719711954752] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:25 INFO 140719711954752] #quality_metric: host=algo-1, epoch=99, train loss <loss>=16.8029241562\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:25 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:26 INFO 140719711954752] Epoch[100] Batch[0] avg_epoch_loss=16.841129\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:26 INFO 140719711954752] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=16.841129303\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:26 INFO 140719711954752] Epoch[100] Batch[5] avg_epoch_loss=16.786183\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:26 INFO 140719711954752] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=16.7861833572\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:26 INFO 140719711954752] Epoch[100] Batch [5]#011Speed: 878.40 samples/sec#011loss=16.786183\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:26 INFO 140719711954752] processed a total of 613 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1067.018985748291, \"sum\": 1067.018985748291, \"min\": 1067.018985748291}}, \"EndTime\": 1564688546.764263, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688545.696785}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:26 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=574.431000361 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:26 INFO 140719711954752] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:26 INFO 140719711954752] #quality_metric: host=algo-1, epoch=100, train loss <loss>=16.7959449768\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:26 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:27 INFO 140719711954752] Epoch[101] Batch[0] avg_epoch_loss=16.673634\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:27 INFO 140719711954752] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=16.6736335754\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:27 INFO 140719711954752] Epoch[101] Batch[5] avg_epoch_loss=16.721278\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:27 INFO 140719711954752] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=16.7212778727\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:27 INFO 140719711954752] Epoch[101] Batch [5]#011Speed: 955.13 samples/sec#011loss=16.721278\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:27 INFO 140719711954752] processed a total of 577 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1026.8189907073975, \"sum\": 1026.8189907073975, \"min\": 1026.8189907073975}}, \"EndTime\": 1564688547.791686, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688546.764347}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:27 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=561.863341273 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:27 INFO 140719711954752] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:27 INFO 140719711954752] #quality_metric: host=algo-1, epoch=101, train loss <loss>=16.7310609818\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:27 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:28 INFO 140719711954752] Epoch[102] Batch[0] avg_epoch_loss=16.753176\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:28 INFO 140719711954752] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=16.7531757355\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:28 INFO 140719711954752] Epoch[102] Batch[5] avg_epoch_loss=16.801114\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:28 INFO 140719711954752] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=16.8011137644\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:28 INFO 140719711954752] Epoch[102] Batch [5]#011Speed: 965.12 samples/sec#011loss=16.801114\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:28 INFO 140719711954752] processed a total of 606 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1040.374994277954, \"sum\": 1040.374994277954, \"min\": 1040.374994277954}}, \"EndTime\": 1564688548.832605, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688547.791769}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:28 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=582.40006599 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:28 INFO 140719711954752] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:28 INFO 140719711954752] #quality_metric: host=algo-1, epoch=102, train loss <loss>=16.7972841263\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:28 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:29 INFO 140719711954752] Epoch[103] Batch[0] avg_epoch_loss=16.779541\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:29 INFO 140719711954752] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=16.7795410156\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:29 INFO 140719711954752] Epoch[103] Batch[5] avg_epoch_loss=16.720754\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:29 INFO 140719711954752] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=16.7207539876\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:29 INFO 140719711954752] Epoch[103] Batch [5]#011Speed: 973.19 samples/sec#011loss=16.720754\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:29 INFO 140719711954752] processed a total of 620 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1036.7789268493652, \"sum\": 1036.7789268493652, \"min\": 1036.7789268493652}}, \"EndTime\": 1564688549.869943, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688548.832716}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:29 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=597.941905233 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:29 INFO 140719711954752] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:29 INFO 140719711954752] #quality_metric: host=algo-1, epoch=103, train loss <loss>=16.7177595139\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:29 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:30 INFO 140719711954752] Epoch[104] Batch[0] avg_epoch_loss=16.784899\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:30 INFO 140719711954752] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=16.7848987579\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:30 INFO 140719711954752] Epoch[104] Batch[5] avg_epoch_loss=16.806699\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:30 INFO 140719711954752] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=16.806699117\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:30 INFO 140719711954752] Epoch[104] Batch [5]#011Speed: 972.48 samples/sec#011loss=16.806699\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:30 INFO 140719711954752] processed a total of 583 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1008.3258152008057, \"sum\": 1008.3258152008057, \"min\": 1008.3258152008057}}, \"EndTime\": 1564688550.878835, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688549.870016}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:30 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=578.121334875 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:30 INFO 140719711954752] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:30 INFO 140719711954752] #quality_metric: host=algo-1, epoch=104, train loss <loss>=16.8018541336\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:30 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:31 INFO 140719711954752] Epoch[105] Batch[0] avg_epoch_loss=16.760321\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:31 INFO 140719711954752] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=16.7603206635\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:31 INFO 140719711954752] Epoch[105] Batch[5] avg_epoch_loss=16.781150\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:31 INFO 140719711954752] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=16.7811501821\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:31 INFO 140719711954752] Epoch[105] Batch [5]#011Speed: 975.70 samples/sec#011loss=16.781150\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:31 INFO 140719711954752] processed a total of 636 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1028.8548469543457, \"sum\": 1028.8548469543457, \"min\": 1028.8548469543457}}, \"EndTime\": 1564688551.908195, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688550.878913}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:31 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=618.113154874 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:31 INFO 140719711954752] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:31 INFO 140719711954752] #quality_metric: host=algo-1, epoch=105, train loss <loss>=16.7720350266\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:31 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:32 INFO 140719711954752] Epoch[106] Batch[0] avg_epoch_loss=16.854769\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:32 INFO 140719711954752] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=16.8547687531\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:32 INFO 140719711954752] Epoch[106] Batch[5] avg_epoch_loss=16.783021\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:32 INFO 140719711954752] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=16.7830212911\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:32 INFO 140719711954752] Epoch[106] Batch [5]#011Speed: 967.58 samples/sec#011loss=16.783021\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:32 INFO 140719711954752] processed a total of 567 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 944.6020126342773, \"sum\": 944.6020126342773, \"min\": 944.6020126342773}}, \"EndTime\": 1564688552.853389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688551.908249}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:32 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=600.18038646 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:32 INFO 140719711954752] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:32 INFO 140719711954752] #quality_metric: host=algo-1, epoch=106, train loss <loss>=16.7586343553\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:32 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:33 INFO 140719711954752] Epoch[107] Batch[0] avg_epoch_loss=16.789194\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:33 INFO 140719711954752] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=16.7891941071\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:33 INFO 140719711954752] Epoch[107] Batch[5] avg_epoch_loss=16.786980\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:33 INFO 140719711954752] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=16.7869796753\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:33 INFO 140719711954752] Epoch[107] Batch [5]#011Speed: 956.23 samples/sec#011loss=16.786980\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:33 INFO 140719711954752] processed a total of 593 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1041.6851043701172, \"sum\": 1041.6851043701172, \"min\": 1041.6851043701172}}, \"EndTime\": 1564688553.895564, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688552.853465}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:33 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=569.20491491 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:33 INFO 140719711954752] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:33 INFO 140719711954752] #quality_metric: host=algo-1, epoch=107, train loss <loss>=16.8181053162\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:33 INFO 140719711954752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:42:34 INFO 140719711954752] Epoch[108] Batch[0] avg_epoch_loss=16.778360\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:34 INFO 140719711954752] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=16.7783603668\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:34 INFO 140719711954752] Epoch[108] Batch[5] avg_epoch_loss=16.772079\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:34 INFO 140719711954752] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=16.772078832\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:34 INFO 140719711954752] Epoch[108] Batch [5]#011Speed: 971.66 samples/sec#011loss=16.772079\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:34 INFO 140719711954752] processed a total of 601 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1018.0301666259766, \"sum\": 1018.0301666259766, \"min\": 1018.0301666259766}}, \"EndTime\": 1564688554.914116, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688553.895646}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:34 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=590.300211927 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:34 INFO 140719711954752] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:34 INFO 140719711954752] #quality_metric: host=algo-1, epoch=108, train loss <loss>=16.7968767166\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:34 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:35 INFO 140719711954752] Epoch[109] Batch[0] avg_epoch_loss=16.544016\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:35 INFO 140719711954752] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=16.5440158844\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:35 INFO 140719711954752] Epoch[109] Batch[5] avg_epoch_loss=16.705802\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:35 INFO 140719711954752] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=16.7058022817\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:35 INFO 140719711954752] Epoch[109] Batch [5]#011Speed: 973.85 samples/sec#011loss=16.705802\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:35 INFO 140719711954752] processed a total of 577 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1007.5762271881104, \"sum\": 1007.5762271881104, \"min\": 1007.5762271881104}}, \"EndTime\": 1564688555.922233, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688554.914177}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:35 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=572.592151786 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:35 INFO 140719711954752] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:35 INFO 140719711954752] #quality_metric: host=algo-1, epoch=109, train loss <loss>=16.786448288\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:35 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:36 INFO 140719711954752] Epoch[110] Batch[0] avg_epoch_loss=16.826286\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:36 INFO 140719711954752] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=16.8262863159\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:36 INFO 140719711954752] Epoch[110] Batch[5] avg_epoch_loss=16.789817\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:36 INFO 140719711954752] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=16.7898174922\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:36 INFO 140719711954752] Epoch[110] Batch [5]#011Speed: 961.99 samples/sec#011loss=16.789817\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:36 INFO 140719711954752] processed a total of 569 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 959.3970775604248, \"sum\": 959.3970775604248, \"min\": 959.3970775604248}}, \"EndTime\": 1564688556.882162, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688555.922316}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:36 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=593.008456715 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:36 INFO 140719711954752] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:36 INFO 140719711954752] #quality_metric: host=algo-1, epoch=110, train loss <loss>=16.7830613454\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:36 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:37 INFO 140719711954752] Epoch[111] Batch[0] avg_epoch_loss=16.802261\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:37 INFO 140719711954752] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=16.8022613525\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:37 INFO 140719711954752] Epoch[111] Batch[5] avg_epoch_loss=16.789035\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:37 INFO 140719711954752] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=16.7890354792\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:37 INFO 140719711954752] Epoch[111] Batch [5]#011Speed: 950.15 samples/sec#011loss=16.789035\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:37 INFO 140719711954752] processed a total of 601 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1085.010051727295, \"sum\": 1085.010051727295, \"min\": 1085.010051727295}}, \"EndTime\": 1564688557.9677, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688556.882243}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:37 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=553.851189567 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:37 INFO 140719711954752] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:37 INFO 140719711954752] #quality_metric: host=algo-1, epoch=111, train loss <loss>=16.8241529465\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:37 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:38 INFO 140719711954752] Epoch[112] Batch[0] avg_epoch_loss=16.801884\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:38 INFO 140719711954752] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=16.8018836975\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:38 INFO 140719711954752] Epoch[112] Batch[5] avg_epoch_loss=16.756689\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:38 INFO 140719711954752] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=16.7566893895\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:38 INFO 140719711954752] Epoch[112] Batch [5]#011Speed: 896.78 samples/sec#011loss=16.756689\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:39 INFO 140719711954752] processed a total of 614 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1098.2279777526855, \"sum\": 1098.2279777526855, \"min\": 1098.2279777526855}}, \"EndTime\": 1564688559.066458, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688557.967781}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:39 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=559.021416361 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:39 INFO 140719711954752] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:39 INFO 140719711954752] #quality_metric: host=algo-1, epoch=112, train loss <loss>=16.8086437225\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:39 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:39 INFO 140719711954752] Epoch[113] Batch[0] avg_epoch_loss=16.718729\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:39 INFO 140719711954752] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=16.7187290192\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:39 INFO 140719711954752] Epoch[113] Batch[5] avg_epoch_loss=16.758217\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:39 INFO 140719711954752] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=16.75821654\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:39 INFO 140719711954752] Epoch[113] Batch [5]#011Speed: 931.49 samples/sec#011loss=16.758217\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:40 INFO 140719711954752] processed a total of 559 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 978.1169891357422, \"sum\": 978.1169891357422, \"min\": 978.1169891357422}}, \"EndTime\": 1564688560.045101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688559.066541}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:40 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=571.444989958 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:40 INFO 140719711954752] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:40 INFO 140719711954752] #quality_metric: host=algo-1, epoch=113, train loss <loss>=16.7560212877\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:40 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:40 INFO 140719711954752] Epoch[114] Batch[0] avg_epoch_loss=16.732117\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:40 INFO 140719711954752] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=16.7321166992\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:40 INFO 140719711954752] Epoch[114] Batch[5] avg_epoch_loss=16.776858\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:40 INFO 140719711954752] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=16.776857694\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:40 INFO 140719711954752] Epoch[114] Batch [5]#011Speed: 946.41 samples/sec#011loss=16.776858\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:41 INFO 140719711954752] processed a total of 608 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1046.5359687805176, \"sum\": 1046.5359687805176, \"min\": 1046.5359687805176}}, \"EndTime\": 1564688561.092213, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688560.045167}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:41 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=580.897037311 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:41 INFO 140719711954752] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:41 INFO 140719711954752] #quality_metric: host=algo-1, epoch=114, train loss <loss>=16.7737016678\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:41 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:41 INFO 140719711954752] Epoch[115] Batch[0] avg_epoch_loss=16.641312\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:41 INFO 140719711954752] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=16.6413116455\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:41 INFO 140719711954752] Epoch[115] Batch[5] avg_epoch_loss=16.764808\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:41 INFO 140719711954752] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=16.7648083369\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:41 INFO 140719711954752] Epoch[115] Batch [5]#011Speed: 961.12 samples/sec#011loss=16.764808\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:42 INFO 140719711954752] processed a total of 562 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 957.2839736938477, \"sum\": 957.2839736938477, \"min\": 957.2839736938477}}, \"EndTime\": 1564688562.050064, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688561.092296}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:42 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=587.011979563 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:42 INFO 140719711954752] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:42 INFO 140719711954752] #quality_metric: host=algo-1, epoch=115, train loss <loss>=16.769103792\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:42 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:42 INFO 140719711954752] Epoch[116] Batch[0] avg_epoch_loss=16.782930\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:42 INFO 140719711954752] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=16.7829303741\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:42 INFO 140719711954752] Epoch[116] Batch[5] avg_epoch_loss=16.782159\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:42 INFO 140719711954752] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=16.7821585337\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:42 INFO 140719711954752] Epoch[116] Batch [5]#011Speed: 951.23 samples/sec#011loss=16.782159\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:43 INFO 140719711954752] processed a total of 569 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 977.8330326080322, \"sum\": 977.8330326080322, \"min\": 977.8330326080322}}, \"EndTime\": 1564688563.028456, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688562.05014}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:43 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=581.828144915 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:43 INFO 140719711954752] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=116, train loss <loss>=16.7723651462\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:43 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:43 INFO 140719711954752] Epoch[117] Batch[0] avg_epoch_loss=16.698162\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=16.6981620789\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:43 INFO 140719711954752] Epoch[117] Batch[5] avg_epoch_loss=16.775838\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=16.7758382161\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:43 INFO 140719711954752] Epoch[117] Batch [5]#011Speed: 957.69 samples/sec#011loss=16.775838\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:42:44 INFO 140719711954752] processed a total of 618 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1080.3589820861816, \"sum\": 1080.3589820861816, \"min\": 1080.3589820861816}}, \"EndTime\": 1564688564.109389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688563.028537}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:44 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=571.968593007 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:44 INFO 140719711954752] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:44 INFO 140719711954752] #quality_metric: host=algo-1, epoch=117, train loss <loss>=16.7832790375\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:44 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:44 INFO 140719711954752] Epoch[118] Batch[0] avg_epoch_loss=16.760740\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:44 INFO 140719711954752] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=16.7607402802\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:44 INFO 140719711954752] Epoch[118] Batch[5] avg_epoch_loss=16.794734\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:44 INFO 140719711954752] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=16.7947343191\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:44 INFO 140719711954752] Epoch[118] Batch [5]#011Speed: 931.97 samples/sec#011loss=16.794734\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:45 INFO 140719711954752] processed a total of 612 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.6790027618408, \"sum\": 1089.6790027618408, \"min\": 1089.6790027618408}}, \"EndTime\": 1564688565.199609, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688564.109472}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:45 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=561.573940346 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:45 INFO 140719711954752] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=118, train loss <loss>=16.7552419662\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:45 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:45 INFO 140719711954752] Epoch[119] Batch[0] avg_epoch_loss=16.806816\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=16.8068161011\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:45 INFO 140719711954752] Epoch[119] Batch[5] avg_epoch_loss=16.811219\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=16.8112192154\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:45 INFO 140719711954752] Epoch[119] Batch [5]#011Speed: 960.72 samples/sec#011loss=16.811219\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:46 INFO 140719711954752] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1028.9499759674072, \"sum\": 1028.9499759674072, \"min\": 1028.9499759674072}}, \"EndTime\": 1564688566.229108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688565.199685}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:46 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=603.457409273 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:46 INFO 140719711954752] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=119, train loss <loss>=16.8141263962\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:46 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:46 INFO 140719711954752] Epoch[120] Batch[0] avg_epoch_loss=16.731642\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=16.7316417694\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:46 INFO 140719711954752] Epoch[120] Batch[5] avg_epoch_loss=16.758502\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=16.7585023244\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:46 INFO 140719711954752] Epoch[120] Batch [5]#011Speed: 936.82 samples/sec#011loss=16.758502\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:47 INFO 140719711954752] processed a total of 551 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 977.6229858398438, \"sum\": 977.6229858398438, \"min\": 977.6229858398438}}, \"EndTime\": 1564688567.20726, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688566.229191}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:47 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=563.543922459 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:47 INFO 140719711954752] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:47 INFO 140719711954752] #quality_metric: host=algo-1, epoch=120, train loss <loss>=16.7575416565\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:47 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:47 INFO 140719711954752] Epoch[121] Batch[0] avg_epoch_loss=16.760876\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:47 INFO 140719711954752] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=16.7608757019\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:47 INFO 140719711954752] Epoch[121] Batch[5] avg_epoch_loss=16.658039\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:47 INFO 140719711954752] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=16.6580387751\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:47 INFO 140719711954752] Epoch[121] Batch [5]#011Speed: 960.33 samples/sec#011loss=16.658039\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:48 INFO 140719711954752] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1045.4421043395996, \"sum\": 1045.4421043395996, \"min\": 1045.4421043395996}}, \"EndTime\": 1564688568.25324, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688567.207342}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:48 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=570.98477518 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:48 INFO 140719711954752] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:48 INFO 140719711954752] #quality_metric: host=algo-1, epoch=121, train loss <loss>=16.6868495941\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:48 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:48 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_583c2073-461a-4a9a-82d0-6a169eb7f58f-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 25.513172149658203, \"sum\": 25.513172149658203, \"min\": 25.513172149658203}}, \"EndTime\": 1564688568.279337, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688568.253324}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:48 INFO 140719711954752] Epoch[122] Batch[0] avg_epoch_loss=16.842661\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:48 INFO 140719711954752] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=16.8426609039\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:49 INFO 140719711954752] Epoch[122] Batch[5] avg_epoch_loss=16.781020\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:49 INFO 140719711954752] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=16.7810198466\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:49 INFO 140719711954752] Epoch[122] Batch [5]#011Speed: 875.54 samples/sec#011loss=16.781020\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:49 INFO 140719711954752] processed a total of 593 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1082.7159881591797, \"sum\": 1082.7159881591797, \"min\": 1082.7159881591797}}, \"EndTime\": 1564688569.362193, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688568.279411}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:49 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=547.636428329 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:49 INFO 140719711954752] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:49 INFO 140719711954752] #quality_metric: host=algo-1, epoch=122, train loss <loss>=16.8272748947\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:49 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:49 INFO 140719711954752] Epoch[123] Batch[0] avg_epoch_loss=16.829119\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:49 INFO 140719711954752] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=16.8291187286\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:50 INFO 140719711954752] Epoch[123] Batch[5] avg_epoch_loss=16.818739\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=16.8187389374\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:50 INFO 140719711954752] Epoch[123] Batch [5]#011Speed: 944.46 samples/sec#011loss=16.818739\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:50 INFO 140719711954752] processed a total of 634 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1099.1930961608887, \"sum\": 1099.1930961608887, \"min\": 1099.1930961608887}}, \"EndTime\": 1564688570.461928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688569.362276}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:50 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=576.724941464 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:50 INFO 140719711954752] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=123, train loss <loss>=16.8104467392\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:50 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:50 INFO 140719711954752] Epoch[124] Batch[0] avg_epoch_loss=16.850889\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=16.8508892059\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:51 INFO 140719711954752] Epoch[124] Batch[5] avg_epoch_loss=16.794641\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=16.7946405411\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:51 INFO 140719711954752] Epoch[124] Batch [5]#011Speed: 924.84 samples/sec#011loss=16.794641\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:51 INFO 140719711954752] processed a total of 536 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 990.0820255279541, \"sum\": 990.0820255279541, \"min\": 990.0820255279541}}, \"EndTime\": 1564688571.452536, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688570.46201}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:51 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=541.305936568 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:51 INFO 140719711954752] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=124, train loss <loss>=16.8025292291\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:51 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:51 INFO 140719711954752] Epoch[125] Batch[0] avg_epoch_loss=16.630787\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=16.6307868958\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:52 INFO 140719711954752] Epoch[125] Batch[5] avg_epoch_loss=16.727423\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=16.72742335\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:52 INFO 140719711954752] Epoch[125] Batch [5]#011Speed: 913.40 samples/sec#011loss=16.727423\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:52 INFO 140719711954752] processed a total of 592 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1075.5648612976074, \"sum\": 1075.5648612976074, \"min\": 1075.5648612976074}}, \"EndTime\": 1564688572.528671, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688571.452616}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:52 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=550.321848883 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:52 INFO 140719711954752] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=125, train loss <loss>=16.7585220337\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:52 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:52 INFO 140719711954752] Epoch[126] Batch[0] avg_epoch_loss=16.858536\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=16.8585357666\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:53 INFO 140719711954752] Epoch[126] Batch[5] avg_epoch_loss=16.802692\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=16.8026920954\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:53 INFO 140719711954752] Epoch[126] Batch [5]#011Speed: 937.54 samples/sec#011loss=16.802692\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:53 INFO 140719711954752] processed a total of 559 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 981.9779396057129, \"sum\": 981.9779396057129, \"min\": 981.9779396057129}}, \"EndTime\": 1564688573.511296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688572.528781}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:53 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=569.190264257 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:53 INFO 140719711954752] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=126, train loss <loss>=16.8086507585\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:53 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:53 INFO 140719711954752] Epoch[127] Batch[0] avg_epoch_loss=16.789776\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=16.7897758484\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:42:54 INFO 140719711954752] Epoch[127] Batch[5] avg_epoch_loss=16.809465\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=16.8094650904\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:54 INFO 140719711954752] Epoch[127] Batch [5]#011Speed: 942.25 samples/sec#011loss=16.809465\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:54 INFO 140719711954752] processed a total of 568 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 993.3559894561768, \"sum\": 993.3559894561768, \"min\": 993.3559894561768}}, \"EndTime\": 1564688574.50518, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688573.511378}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:54 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=571.73042741 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:54 INFO 140719711954752] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=127, train loss <loss>=16.7747124566\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:54 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:54 INFO 140719711954752] Epoch[128] Batch[0] avg_epoch_loss=16.659710\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=16.6597099304\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:55 INFO 140719711954752] Epoch[128] Batch[5] avg_epoch_loss=16.772673\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:55 INFO 140719711954752] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=16.7726726532\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:55 INFO 140719711954752] Epoch[128] Batch [5]#011Speed: 877.06 samples/sec#011loss=16.772673\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:55 INFO 140719711954752] processed a total of 586 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.9269580841064, \"sum\": 1089.9269580841064, \"min\": 1089.9269580841064}}, \"EndTime\": 1564688575.595628, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688574.505262}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:55 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=537.591556233 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:55 INFO 140719711954752] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:55 INFO 140719711954752] #quality_metric: host=algo-1, epoch=128, train loss <loss>=16.733820343\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:55 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:56 INFO 140719711954752] Epoch[129] Batch[0] avg_epoch_loss=16.799896\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=16.7998962402\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:56 INFO 140719711954752] Epoch[129] Batch[5] avg_epoch_loss=16.819447\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=16.8194465637\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:56 INFO 140719711954752] Epoch[129] Batch [5]#011Speed: 964.20 samples/sec#011loss=16.819447\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:56 INFO 140719711954752] processed a total of 565 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 981.8809032440186, \"sum\": 981.8809032440186, \"min\": 981.8809032440186}}, \"EndTime\": 1564688576.578044, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688575.59571}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:56 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=575.357047996 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:56 INFO 140719711954752] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=129, train loss <loss>=16.7839465671\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:56 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:57 INFO 140719711954752] Epoch[130] Batch[0] avg_epoch_loss=16.842402\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:57 INFO 140719711954752] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=16.8424015045\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:57 INFO 140719711954752] Epoch[130] Batch[5] avg_epoch_loss=16.794644\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:57 INFO 140719711954752] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=16.79464372\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:57 INFO 140719711954752] Epoch[130] Batch [5]#011Speed: 892.86 samples/sec#011loss=16.794644\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:57 INFO 140719711954752] processed a total of 589 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1075.5038261413574, \"sum\": 1075.5038261413574, \"min\": 1075.5038261413574}}, \"EndTime\": 1564688577.654087, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688576.578125}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:57 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=547.592767226 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:57 INFO 140719711954752] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:57 INFO 140719711954752] #quality_metric: host=algo-1, epoch=130, train loss <loss>=16.8352806091\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:57 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:58 INFO 140719711954752] Epoch[131] Batch[0] avg_epoch_loss=16.695934\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=16.6959342957\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:58 INFO 140719711954752] Epoch[131] Batch[5] avg_epoch_loss=16.789560\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=16.7895596822\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:58 INFO 140719711954752] Epoch[131] Batch [5]#011Speed: 949.10 samples/sec#011loss=16.789560\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:58 INFO 140719711954752] processed a total of 586 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1018.1550979614258, \"sum\": 1018.1550979614258, \"min\": 1018.1550979614258}}, \"EndTime\": 1564688578.67285, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688577.65416}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:58 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=575.487076921 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:58 INFO 140719711954752] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=131, train loss <loss>=16.8039079666\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:58 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:59 INFO 140719711954752] Epoch[132] Batch[0] avg_epoch_loss=16.825741\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=16.8257408142\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:59 INFO 140719711954752] Epoch[132] Batch[5] avg_epoch_loss=16.816514\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=16.8165136973\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:59 INFO 140719711954752] Epoch[132] Batch [5]#011Speed: 969.55 samples/sec#011loss=16.816514\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:59 INFO 140719711954752] processed a total of 580 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1005.7640075683594, \"sum\": 1005.7640075683594, \"min\": 1005.7640075683594}}, \"EndTime\": 1564688579.679201, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688578.672924}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:59 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=576.607828577 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:59 INFO 140719711954752] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=132, train loss <loss>=16.8018447876\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:42:59 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:00 INFO 140719711954752] Epoch[133] Batch[0] avg_epoch_loss=16.833647\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=16.8336467743\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:00 INFO 140719711954752] Epoch[133] Batch[5] avg_epoch_loss=16.751804\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=16.7518040339\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:00 INFO 140719711954752] Epoch[133] Batch [5]#011Speed: 962.31 samples/sec#011loss=16.751804\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:00 INFO 140719711954752] processed a total of 581 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1047.2400188446045, \"sum\": 1047.2400188446045, \"min\": 1047.2400188446045}}, \"EndTime\": 1564688580.726983, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688579.679282}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:00 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=554.728108199 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:00 INFO 140719711954752] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=133, train loss <loss>=16.7928033829\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:00 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:01 INFO 140719711954752] Epoch[134] Batch[0] avg_epoch_loss=16.809322\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:01 INFO 140719711954752] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=16.8093223572\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:01 INFO 140719711954752] Epoch[134] Batch[5] avg_epoch_loss=16.793276\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:01 INFO 140719711954752] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=16.7932755152\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:01 INFO 140719711954752] Epoch[134] Batch [5]#011Speed: 967.31 samples/sec#011loss=16.793276\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:01 INFO 140719711954752] processed a total of 602 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1083.1849575042725, \"sum\": 1083.1849575042725, \"min\": 1083.1849575042725}}, \"EndTime\": 1564688581.810716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688580.727065}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:01 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=555.715463373 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:01 INFO 140719711954752] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:01 INFO 140719711954752] #quality_metric: host=algo-1, epoch=134, train loss <loss>=16.7795511246\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:01 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:02 INFO 140719711954752] Epoch[135] Batch[0] avg_epoch_loss=16.838037\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=16.8380374908\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:02 INFO 140719711954752] Epoch[135] Batch[5] avg_epoch_loss=16.760295\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=16.7602945964\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:02 INFO 140719711954752] Epoch[135] Batch [5]#011Speed: 934.02 samples/sec#011loss=16.760295\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:02 INFO 140719711954752] processed a total of 593 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1041.863203048706, \"sum\": 1041.863203048706, \"min\": 1041.863203048706}}, \"EndTime\": 1564688582.853118, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688581.810787}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:02 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=569.10762472 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:02 INFO 140719711954752] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=135, train loss <loss>=16.7719413757\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:02 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:03 INFO 140719711954752] Epoch[136] Batch[0] avg_epoch_loss=16.829041\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=16.8290405273\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:03 INFO 140719711954752] Epoch[136] Batch[5] avg_epoch_loss=16.754087\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=16.7540874481\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:03 INFO 140719711954752] Epoch[136] Batch [5]#011Speed: 937.75 samples/sec#011loss=16.754087\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:03 INFO 140719711954752] processed a total of 631 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1056.5481185913086, \"sum\": 1056.5481185913086, \"min\": 1056.5481185913086}}, \"EndTime\": 1564688583.910211, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688582.8532}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:03 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=597.159566434 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:03 INFO 140719711954752] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=136, train loss <loss>=16.7619585037\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:03 INFO 140719711954752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:43:04 INFO 140719711954752] Epoch[137] Batch[0] avg_epoch_loss=16.866482\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=16.866481781\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:04 INFO 140719711954752] Epoch[137] Batch[5] avg_epoch_loss=16.747454\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=16.7474536896\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:04 INFO 140719711954752] Epoch[137] Batch [5]#011Speed: 903.20 samples/sec#011loss=16.747454\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:04 INFO 140719711954752] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1046.0948944091797, \"sum\": 1046.0948944091797, \"min\": 1046.0948944091797}}, \"EndTime\": 1564688584.956872, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688583.910294}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:04 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=570.627856672 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:04 INFO 140719711954752] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=137, train loss <loss>=16.7192979813\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:04 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:05 INFO 140719711954752] Epoch[138] Batch[0] avg_epoch_loss=16.765711\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=16.7657108307\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:05 INFO 140719711954752] Epoch[138] Batch[5] avg_epoch_loss=16.745996\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=16.7459961573\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:05 INFO 140719711954752] Epoch[138] Batch [5]#011Speed: 940.83 samples/sec#011loss=16.745996\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:05 INFO 140719711954752] processed a total of 587 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1035.0677967071533, \"sum\": 1035.0677967071533, \"min\": 1035.0677967071533}}, \"EndTime\": 1564688585.992497, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688584.956956}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:05 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=567.04834794 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:05 INFO 140719711954752] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=138, train loss <loss>=16.71907444\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:05 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:06 INFO 140719711954752] Epoch[139] Batch[0] avg_epoch_loss=16.679733\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:06 INFO 140719711954752] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=16.6797332764\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:06 INFO 140719711954752] Epoch[139] Batch[5] avg_epoch_loss=16.731144\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:06 INFO 140719711954752] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=16.7311442693\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:06 INFO 140719711954752] Epoch[139] Batch [5]#011Speed: 942.68 samples/sec#011loss=16.731144\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:07 INFO 140719711954752] processed a total of 592 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1061.4118576049805, \"sum\": 1061.4118576049805, \"min\": 1061.4118576049805}}, \"EndTime\": 1564688587.054478, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688585.992578}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:07 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=557.685670515 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:07 INFO 140719711954752] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:07 INFO 140719711954752] #quality_metric: host=algo-1, epoch=139, train loss <loss>=16.7190504074\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:07 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:07 INFO 140719711954752] Epoch[140] Batch[0] avg_epoch_loss=16.676336\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:07 INFO 140719711954752] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=16.6763362885\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:07 INFO 140719711954752] Epoch[140] Batch[5] avg_epoch_loss=16.745897\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:07 INFO 140719711954752] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=16.7458969752\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:07 INFO 140719711954752] Epoch[140] Batch [5]#011Speed: 948.16 samples/sec#011loss=16.745897\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:08 INFO 140719711954752] processed a total of 578 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1043.407917022705, \"sum\": 1043.407917022705, \"min\": 1043.407917022705}}, \"EndTime\": 1564688588.098431, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688587.05456}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:08 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=553.888197327 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:08 INFO 140719711954752] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=140, train loss <loss>=16.7750293732\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:08 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:08 INFO 140719711954752] Epoch[141] Batch[0] avg_epoch_loss=16.804089\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=16.8040885925\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:08 INFO 140719711954752] Epoch[141] Batch[5] avg_epoch_loss=16.745099\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=16.7450987498\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:08 INFO 140719711954752] Epoch[141] Batch [5]#011Speed: 949.79 samples/sec#011loss=16.745099\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:09 INFO 140719711954752] processed a total of 574 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 961.7660045623779, \"sum\": 961.7660045623779, \"min\": 961.7660045623779}}, \"EndTime\": 1564688589.060801, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688588.098514}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:09 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=596.743617155 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:09 INFO 140719711954752] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:09 INFO 140719711954752] #quality_metric: host=algo-1, epoch=141, train loss <loss>=16.7389761607\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:09 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:09 INFO 140719711954752] Epoch[142] Batch[0] avg_epoch_loss=16.806326\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:09 INFO 140719711954752] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=16.8063259125\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:09 INFO 140719711954752] Epoch[142] Batch[5] avg_epoch_loss=16.765020\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:09 INFO 140719711954752] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=16.7650203705\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:09 INFO 140719711954752] Epoch[142] Batch [5]#011Speed: 964.08 samples/sec#011loss=16.765020\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:10 INFO 140719711954752] processed a total of 533 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 960.5560302734375, \"sum\": 960.5560302734375, \"min\": 960.5560302734375}}, \"EndTime\": 1564688590.021907, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688589.060883}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:10 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=554.816436093 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:10 INFO 140719711954752] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:10 INFO 140719711954752] #quality_metric: host=algo-1, epoch=142, train loss <loss>=16.76566018\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:10 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:10 INFO 140719711954752] Epoch[143] Batch[0] avg_epoch_loss=16.856678\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:10 INFO 140719711954752] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=16.856678009\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:10 INFO 140719711954752] Epoch[143] Batch[5] avg_epoch_loss=16.772942\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:10 INFO 140719711954752] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=16.7729422251\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:10 INFO 140719711954752] Epoch[143] Batch [5]#011Speed: 954.04 samples/sec#011loss=16.772942\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:11 INFO 140719711954752] processed a total of 577 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1024.1100788116455, \"sum\": 1024.1100788116455, \"min\": 1024.1100788116455}}, \"EndTime\": 1564688591.046601, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688590.021989}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:11 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=563.351731725 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:11 INFO 140719711954752] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:11 INFO 140719711954752] #quality_metric: host=algo-1, epoch=143, train loss <loss>=16.8002729416\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:11 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:11 INFO 140719711954752] Epoch[144] Batch[0] avg_epoch_loss=16.883249\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:11 INFO 140719711954752] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=16.8832492828\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:11 INFO 140719711954752] Epoch[144] Batch[5] avg_epoch_loss=16.798456\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:11 INFO 140719711954752] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=16.798456192\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:11 INFO 140719711954752] Epoch[144] Batch [5]#011Speed: 958.26 samples/sec#011loss=16.798456\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:12 INFO 140719711954752] processed a total of 553 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 964.0259742736816, \"sum\": 964.0259742736816, \"min\": 964.0259742736816}}, \"EndTime\": 1564688592.011124, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688591.04668}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:12 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=573.567623334 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:12 INFO 140719711954752] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:12 INFO 140719711954752] #quality_metric: host=algo-1, epoch=144, train loss <loss>=16.7675836351\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:12 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:12 INFO 140719711954752] Epoch[145] Batch[0] avg_epoch_loss=16.848442\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:12 INFO 140719711954752] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=16.8484420776\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:12 INFO 140719711954752] Epoch[145] Batch[5] avg_epoch_loss=16.741545\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:12 INFO 140719711954752] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=16.7415453593\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:12 INFO 140719711954752] Epoch[145] Batch [5]#011Speed: 956.16 samples/sec#011loss=16.741545\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:13 INFO 140719711954752] processed a total of 585 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1029.0350914001465, \"sum\": 1029.0350914001465, \"min\": 1029.0350914001465}}, \"EndTime\": 1564688593.040738, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688592.0112}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:13 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=568.441183046 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:13 INFO 140719711954752] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:13 INFO 140719711954752] #quality_metric: host=algo-1, epoch=145, train loss <loss>=16.7714099884\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:13 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:13 INFO 140719711954752] Epoch[146] Batch[0] avg_epoch_loss=16.830788\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:13 INFO 140719711954752] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=16.8307876587\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:13 INFO 140719711954752] Epoch[146] Batch[5] avg_epoch_loss=16.800496\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:13 INFO 140719711954752] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=16.8004957835\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:13 INFO 140719711954752] Epoch[146] Batch [5]#011Speed: 906.84 samples/sec#011loss=16.800496\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:14 INFO 140719711954752] processed a total of 602 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1051.4650344848633, \"sum\": 1051.4650344848633, \"min\": 1051.4650344848633}}, \"EndTime\": 1564688594.092755, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688593.0408}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:14 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=572.469718952 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:14 INFO 140719711954752] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:14 INFO 140719711954752] #quality_metric: host=algo-1, epoch=146, train loss <loss>=16.8386156082\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:14 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:14 INFO 140719711954752] Epoch[147] Batch[0] avg_epoch_loss=16.785404\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:14 INFO 140719711954752] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=16.7854042053\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:14 INFO 140719711954752] Epoch[147] Batch[5] avg_epoch_loss=16.801888\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:14 INFO 140719711954752] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=16.8018878301\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:14 INFO 140719711954752] Epoch[147] Batch [5]#011Speed: 966.85 samples/sec#011loss=16.801888\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:15 INFO 140719711954752] processed a total of 635 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1037.1770858764648, \"sum\": 1037.1770858764648, \"min\": 1037.1770858764648}}, \"EndTime\": 1564688595.130502, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688594.092836}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:15 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=612.166135877 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:15 INFO 140719711954752] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:15 INFO 140719711954752] #quality_metric: host=algo-1, epoch=147, train loss <loss>=16.7999422073\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:15 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:15 INFO 140719711954752] Epoch[148] Batch[0] avg_epoch_loss=16.758911\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:15 INFO 140719711954752] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=16.7589111328\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:15 INFO 140719711954752] Epoch[148] Batch[5] avg_epoch_loss=16.740568\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:15 INFO 140719711954752] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=16.740568161\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:15 INFO 140719711954752] Epoch[148] Batch [5]#011Speed: 960.68 samples/sec#011loss=16.740568\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:16 INFO 140719711954752] processed a total of 581 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1035.4259014129639, \"sum\": 1035.4259014129639, \"min\": 1035.4259014129639}}, \"EndTime\": 1564688596.166539, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688595.130586}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:16 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=561.055094307 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:16 INFO 140719711954752] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:16 INFO 140719711954752] #quality_metric: host=algo-1, epoch=148, train loss <loss>=16.7532455444\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:16 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:16 INFO 140719711954752] Epoch[149] Batch[0] avg_epoch_loss=16.767504\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:16 INFO 140719711954752] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=16.7675037384\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:16 INFO 140719711954752] Epoch[149] Batch[5] avg_epoch_loss=16.810351\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:16 INFO 140719711954752] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=16.810350736\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:16 INFO 140719711954752] Epoch[149] Batch [5]#011Speed: 956.20 samples/sec#011loss=16.810351\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:17 INFO 140719711954752] Epoch[149] Batch[10] avg_epoch_loss=16.810063\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:17 INFO 140719711954752] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=16.8097171783\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:17 INFO 140719711954752] Epoch[149] Batch [10]#011Speed: 918.15 samples/sec#011loss=16.809717\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:17 INFO 140719711954752] processed a total of 643 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1113.3489608764648, \"sum\": 1113.3489608764648, \"min\": 1113.3489608764648}}, \"EndTime\": 1564688597.280484, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688596.166623}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:17 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=577.485482124 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:17 INFO 140719711954752] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:17 INFO 140719711954752] #quality_metric: host=algo-1, epoch=149, train loss <loss>=16.8100627552\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:17 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:17 INFO 140719711954752] Epoch[150] Batch[0] avg_epoch_loss=16.780680\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:17 INFO 140719711954752] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=16.7806797028\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:18 INFO 140719711954752] Epoch[150] Batch[5] avg_epoch_loss=16.718470\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:18 INFO 140719711954752] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=16.7184702555\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:18 INFO 140719711954752] Epoch[150] Batch [5]#011Speed: 954.98 samples/sec#011loss=16.718470\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:18 INFO 140719711954752] processed a total of 598 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1037.0380878448486, \"sum\": 1037.0380878448486, \"min\": 1037.0380878448486}}, \"EndTime\": 1564688598.31805, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688597.280544}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:18 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=576.575066664 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:18 INFO 140719711954752] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:18 INFO 140719711954752] #quality_metric: host=algo-1, epoch=150, train loss <loss>=16.708407402\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:18 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:18 INFO 140719711954752] Epoch[151] Batch[0] avg_epoch_loss=16.801991\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:18 INFO 140719711954752] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=16.801990509\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:19 INFO 140719711954752] Epoch[151] Batch[5] avg_epoch_loss=16.684383\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:19 INFO 140719711954752] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=16.6843833923\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:19 INFO 140719711954752] Epoch[151] Batch [5]#011Speed: 956.75 samples/sec#011loss=16.684383\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:43:19 INFO 140719711954752] processed a total of 564 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 972.4469184875488, \"sum\": 972.4469184875488, \"min\": 972.4469184875488}}, \"EndTime\": 1564688599.291025, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688598.318133}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:19 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=579.91697259 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:19 INFO 140719711954752] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:19 INFO 140719711954752] #quality_metric: host=algo-1, epoch=151, train loss <loss>=16.7025080787\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:19 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:19 INFO 140719711954752] Epoch[152] Batch[0] avg_epoch_loss=16.804035\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:19 INFO 140719711954752] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=16.8040351868\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:20 INFO 140719711954752] Epoch[152] Batch[5] avg_epoch_loss=16.727542\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:20 INFO 140719711954752] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=16.7275419235\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:20 INFO 140719711954752] Epoch[152] Batch [5]#011Speed: 963.66 samples/sec#011loss=16.727542\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:20 INFO 140719711954752] processed a total of 585 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1038.114070892334, \"sum\": 1038.114070892334, \"min\": 1038.114070892334}}, \"EndTime\": 1564688600.329656, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688599.2911}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:20 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=563.463394493 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:20 INFO 140719711954752] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:20 INFO 140719711954752] #quality_metric: host=algo-1, epoch=152, train loss <loss>=16.770847702\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:20 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:20 INFO 140719711954752] Epoch[153] Batch[0] avg_epoch_loss=16.817413\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:20 INFO 140719711954752] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=16.8174133301\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:21 INFO 140719711954752] Epoch[153] Batch[5] avg_epoch_loss=16.776560\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:21 INFO 140719711954752] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=16.7765601476\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:21 INFO 140719711954752] Epoch[153] Batch [5]#011Speed: 967.95 samples/sec#011loss=16.776560\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:21 INFO 140719711954752] processed a total of 568 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 946.0241794586182, \"sum\": 946.0241794586182, \"min\": 946.0241794586182}}, \"EndTime\": 1564688601.276223, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688600.329727}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:21 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=600.339099856 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:21 INFO 140719711954752] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:21 INFO 140719711954752] #quality_metric: host=algo-1, epoch=153, train loss <loss>=16.7695026398\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:21 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:21 INFO 140719711954752] Epoch[154] Batch[0] avg_epoch_loss=16.846449\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:21 INFO 140719711954752] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=16.8464488983\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:22 INFO 140719711954752] Epoch[154] Batch[5] avg_epoch_loss=16.746329\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:22 INFO 140719711954752] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=16.7463293076\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:22 INFO 140719711954752] Epoch[154] Batch [5]#011Speed: 962.96 samples/sec#011loss=16.746329\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:22 INFO 140719711954752] processed a total of 617 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1029.7119617462158, \"sum\": 1029.7119617462158, \"min\": 1029.7119617462158}}, \"EndTime\": 1564688602.306511, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688601.276295}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:22 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=599.125110865 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:22 INFO 140719711954752] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:22 INFO 140719711954752] #quality_metric: host=algo-1, epoch=154, train loss <loss>=16.7334030151\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:22 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:22 INFO 140719711954752] Epoch[155] Batch[0] avg_epoch_loss=16.778917\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:22 INFO 140719711954752] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=16.7789173126\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:23 INFO 140719711954752] Epoch[155] Batch[5] avg_epoch_loss=16.761152\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:23 INFO 140719711954752] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=16.7611522675\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:23 INFO 140719711954752] Epoch[155] Batch [5]#011Speed: 961.24 samples/sec#011loss=16.761152\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:23 INFO 140719711954752] processed a total of 546 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 970.1149463653564, \"sum\": 970.1149463653564, \"min\": 970.1149463653564}}, \"EndTime\": 1564688603.27717, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688602.306597}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:23 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=562.750198122 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:23 INFO 140719711954752] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:23 INFO 140719711954752] #quality_metric: host=algo-1, epoch=155, train loss <loss>=16.7743856642\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:23 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:23 INFO 140719711954752] Epoch[156] Batch[0] avg_epoch_loss=16.731308\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:23 INFO 140719711954752] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=16.7313079834\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] Epoch[156] Batch[5] avg_epoch_loss=16.734984\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=16.73498408\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] Epoch[156] Batch [5]#011Speed: 964.79 samples/sec#011loss=16.734984\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] processed a total of 567 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 967.6799774169922, \"sum\": 967.6799774169922, \"min\": 967.6799774169922}}, \"EndTime\": 1564688604.245395, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688603.277254}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=585.865484771 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] #quality_metric: host=algo-1, epoch=156, train loss <loss>=16.7493447198\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] Epoch[157] Batch[0] avg_epoch_loss=16.666271\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=16.6662712097\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] Epoch[157] Batch[5] avg_epoch_loss=16.705975\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=16.7059752146\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:24 INFO 140719711954752] Epoch[157] Batch [5]#011Speed: 971.34 samples/sec#011loss=16.705975\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:25 INFO 140719711954752] processed a total of 575 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 949.9969482421875, \"sum\": 949.9969482421875, \"min\": 949.9969482421875}}, \"EndTime\": 1564688605.195918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688604.245477}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:25 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=605.200702635 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:25 INFO 140719711954752] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:25 INFO 140719711954752] #quality_metric: host=algo-1, epoch=157, train loss <loss>=16.7413438161\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:25 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:25 INFO 140719711954752] Epoch[158] Batch[0] avg_epoch_loss=16.842745\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:25 INFO 140719711954752] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=16.8427448273\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:25 INFO 140719711954752] Epoch[158] Batch[5] avg_epoch_loss=16.753508\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:25 INFO 140719711954752] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=16.7535076141\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:25 INFO 140719711954752] Epoch[158] Batch [5]#011Speed: 978.47 samples/sec#011loss=16.753508\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:26 INFO 140719711954752] processed a total of 589 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1004.6689510345459, \"sum\": 1004.6689510345459, \"min\": 1004.6689510345459}}, \"EndTime\": 1564688606.201141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688605.19599}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:26 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=586.193908383 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:26 INFO 140719711954752] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:26 INFO 140719711954752] #quality_metric: host=algo-1, epoch=158, train loss <loss>=16.7522670746\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:26 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:26 INFO 140719711954752] Epoch[159] Batch[0] avg_epoch_loss=16.789021\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:26 INFO 140719711954752] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=16.7890205383\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:26 INFO 140719711954752] Epoch[159] Batch[5] avg_epoch_loss=16.790127\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:26 INFO 140719711954752] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=16.7901271184\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:26 INFO 140719711954752] Epoch[159] Batch [5]#011Speed: 977.05 samples/sec#011loss=16.790127\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:27 INFO 140719711954752] processed a total of 571 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 935.0399971008301, \"sum\": 935.0399971008301, \"min\": 935.0399971008301}}, \"EndTime\": 1564688607.136766, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688606.201221}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:27 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=610.595889363 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:27 INFO 140719711954752] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:27 INFO 140719711954752] #quality_metric: host=algo-1, epoch=159, train loss <loss>=16.762766944\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:27 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:27 INFO 140719711954752] Epoch[160] Batch[0] avg_epoch_loss=16.606394\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:27 INFO 140719711954752] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=16.6063938141\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:27 INFO 140719711954752] Epoch[160] Batch[5] avg_epoch_loss=16.703637\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:27 INFO 140719711954752] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=16.703637441\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:27 INFO 140719711954752] Epoch[160] Batch [5]#011Speed: 968.64 samples/sec#011loss=16.703637\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:28 INFO 140719711954752] processed a total of 588 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1034.6150398254395, \"sum\": 1034.6150398254395, \"min\": 1034.6150398254395}}, \"EndTime\": 1564688608.171921, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688607.136839}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:28 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=568.272457326 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:28 INFO 140719711954752] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:28 INFO 140719711954752] #quality_metric: host=algo-1, epoch=160, train loss <loss>=16.6534341812\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:28 INFO 140719711954752] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:28 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/state_8af94075-f222-4925-af17-3bcb55696a54-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.33099937438965, \"sum\": 22.33099937438965, \"min\": 22.33099937438965}}, \"EndTime\": 1564688608.194842, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688608.171982}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:28 INFO 140719711954752] Epoch[161] Batch[0] avg_epoch_loss=16.804979\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:28 INFO 140719711954752] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=16.8049793243\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:28 INFO 140719711954752] Epoch[161] Batch[5] avg_epoch_loss=16.768602\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:28 INFO 140719711954752] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=16.7686023712\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:28 INFO 140719711954752] Epoch[161] Batch [5]#011Speed: 969.44 samples/sec#011loss=16.768602\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:43:29 INFO 140719711954752] processed a total of 618 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1016.5529251098633, \"sum\": 1016.5529251098633, \"min\": 1016.5529251098633}}, \"EndTime\": 1564688609.211519, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688608.19491}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:29 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=607.865725848 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:29 INFO 140719711954752] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:29 INFO 140719711954752] #quality_metric: host=algo-1, epoch=161, train loss <loss>=16.7578994751\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:29 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:29 INFO 140719711954752] Epoch[162] Batch[0] avg_epoch_loss=16.791227\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:29 INFO 140719711954752] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=16.7912273407\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:29 INFO 140719711954752] Epoch[162] Batch[5] avg_epoch_loss=16.788654\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:29 INFO 140719711954752] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=16.7886543274\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:29 INFO 140719711954752] Epoch[162] Batch [5]#011Speed: 966.43 samples/sec#011loss=16.788654\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:30 INFO 140719711954752] processed a total of 577 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1024.5239734649658, \"sum\": 1024.5239734649658, \"min\": 1024.5239734649658}}, \"EndTime\": 1564688610.236595, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688609.211601}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:30 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=563.111461062 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:30 INFO 140719711954752] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:30 INFO 140719711954752] #quality_metric: host=algo-1, epoch=162, train loss <loss>=16.7917612076\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:30 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:30 INFO 140719711954752] Epoch[163] Batch[0] avg_epoch_loss=16.710663\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:30 INFO 140719711954752] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=16.7106628418\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:31 INFO 140719711954752] Epoch[163] Batch[5] avg_epoch_loss=16.775229\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:31 INFO 140719711954752] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=16.7752285004\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:31 INFO 140719711954752] Epoch[163] Batch [5]#011Speed: 967.28 samples/sec#011loss=16.775229\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:31 INFO 140719711954752] processed a total of 581 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1081.8309783935547, \"sum\": 1081.8309783935547, \"min\": 1081.8309783935547}}, \"EndTime\": 1564688611.318991, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688610.236697}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:31 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=536.993890071 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:31 INFO 140719711954752] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:31 INFO 140719711954752] #quality_metric: host=algo-1, epoch=163, train loss <loss>=16.7230573654\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:31 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:31 INFO 140719711954752] Epoch[164] Batch[0] avg_epoch_loss=16.702961\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:31 INFO 140719711954752] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=16.702960968\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:32 INFO 140719711954752] Epoch[164] Batch[5] avg_epoch_loss=16.778864\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:32 INFO 140719711954752] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=16.7788639069\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:32 INFO 140719711954752] Epoch[164] Batch [5]#011Speed: 939.41 samples/sec#011loss=16.778864\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:32 INFO 140719711954752] processed a total of 625 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1057.7991008758545, \"sum\": 1057.7991008758545, \"min\": 1057.7991008758545}}, \"EndTime\": 1564688612.377313, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688611.319072}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:32 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=590.782988486 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:32 INFO 140719711954752] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:32 INFO 140719711954752] #quality_metric: host=algo-1, epoch=164, train loss <loss>=16.7654811859\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:32 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:32 INFO 140719711954752] Epoch[165] Batch[0] avg_epoch_loss=16.769529\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:32 INFO 140719711954752] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=16.7695293427\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:33 INFO 140719711954752] Epoch[165] Batch[5] avg_epoch_loss=16.716808\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:33 INFO 140719711954752] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=16.7168080012\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:33 INFO 140719711954752] Epoch[165] Batch [5]#011Speed: 964.30 samples/sec#011loss=16.716808\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:33 INFO 140719711954752] processed a total of 593 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1081.3360214233398, \"sum\": 1081.3360214233398, \"min\": 1081.3360214233398}}, \"EndTime\": 1564688613.459206, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688612.377394}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:33 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=548.338491487 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:33 INFO 140719711954752] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:33 INFO 140719711954752] #quality_metric: host=algo-1, epoch=165, train loss <loss>=16.7383060455\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:33 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:33 INFO 140719711954752] Epoch[166] Batch[0] avg_epoch_loss=16.692331\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:33 INFO 140719711954752] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=16.6923313141\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:34 INFO 140719711954752] Epoch[166] Batch[5] avg_epoch_loss=16.748071\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:34 INFO 140719711954752] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=16.7480713526\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:34 INFO 140719711954752] Epoch[166] Batch [5]#011Speed: 971.34 samples/sec#011loss=16.748071\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:34 INFO 140719711954752] processed a total of 564 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 959.644079208374, \"sum\": 959.644079208374, \"min\": 959.644079208374}}, \"EndTime\": 1564688614.419385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688613.459282}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:34 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=587.646941952 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:34 INFO 140719711954752] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:34 INFO 140719711954752] #quality_metric: host=algo-1, epoch=166, train loss <loss>=16.7667982313\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:34 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:34 INFO 140719711954752] Epoch[167] Batch[0] avg_epoch_loss=16.805471\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:34 INFO 140719711954752] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=16.8054714203\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:35 INFO 140719711954752] Epoch[167] Batch[5] avg_epoch_loss=16.764038\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:35 INFO 140719711954752] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=16.764037768\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:35 INFO 140719711954752] Epoch[167] Batch [5]#011Speed: 971.01 samples/sec#011loss=16.764038\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:35 INFO 140719711954752] processed a total of 534 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1011.3720893859863, \"sum\": 1011.3720893859863, \"min\": 1011.3720893859863}}, \"EndTime\": 1564688615.43129, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688614.419466}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:35 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=527.932986761 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:35 INFO 140719711954752] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:35 INFO 140719711954752] #quality_metric: host=algo-1, epoch=167, train loss <loss>=16.740008884\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:35 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:35 INFO 140719711954752] Epoch[168] Batch[0] avg_epoch_loss=16.852850\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:35 INFO 140719711954752] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=16.8528499603\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:36 INFO 140719711954752] Epoch[168] Batch[5] avg_epoch_loss=16.770736\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:36 INFO 140719711954752] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=16.7707360586\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:36 INFO 140719711954752] Epoch[168] Batch [5]#011Speed: 974.33 samples/sec#011loss=16.770736\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:36 INFO 140719711954752] processed a total of 558 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 987.6480102539062, \"sum\": 987.6480102539062, \"min\": 987.6480102539062}}, \"EndTime\": 1564688616.41948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688615.431373}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:36 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=564.909334386 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:36 INFO 140719711954752] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:36 INFO 140719711954752] #quality_metric: host=algo-1, epoch=168, train loss <loss>=16.7545350393\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:36 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:36 INFO 140719711954752] Epoch[169] Batch[0] avg_epoch_loss=16.824154\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:36 INFO 140719711954752] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=16.8241539001\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:37 INFO 140719711954752] Epoch[169] Batch[5] avg_epoch_loss=16.794744\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:37 INFO 140719711954752] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=16.7947438558\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:37 INFO 140719711954752] Epoch[169] Batch [5]#011Speed: 972.38 samples/sec#011loss=16.794744\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:37 INFO 140719711954752] processed a total of 551 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 978.0440330505371, \"sum\": 978.0440330505371, \"min\": 978.0440330505371}}, \"EndTime\": 1564688617.398103, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688616.419563}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:37 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=563.301895701 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:37 INFO 140719711954752] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:37 INFO 140719711954752] #quality_metric: host=algo-1, epoch=169, train loss <loss>=16.786049313\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:37 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:37 INFO 140719711954752] Epoch[170] Batch[0] avg_epoch_loss=16.777824\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:37 INFO 140719711954752] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=16.7778244019\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:38 INFO 140719711954752] Epoch[170] Batch[5] avg_epoch_loss=16.742584\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:38 INFO 140719711954752] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=16.7425839106\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:38 INFO 140719711954752] Epoch[170] Batch [5]#011Speed: 961.83 samples/sec#011loss=16.742584\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:38 INFO 140719711954752] processed a total of 589 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1049.4320392608643, \"sum\": 1049.4320392608643, \"min\": 1049.4320392608643}}, \"EndTime\": 1564688618.448062, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688617.398183}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:38 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=561.195029208 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:38 INFO 140719711954752] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:38 INFO 140719711954752] #quality_metric: host=algo-1, epoch=170, train loss <loss>=16.7353866577\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:38 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:38 INFO 140719711954752] Epoch[171] Batch[0] avg_epoch_loss=16.710613\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:38 INFO 140719711954752] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=16.7106132507\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:39 INFO 140719711954752] Epoch[171] Batch[5] avg_epoch_loss=16.738198\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:39 INFO 140719711954752] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=16.7381982803\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:39 INFO 140719711954752] Epoch[171] Batch [5]#011Speed: 967.48 samples/sec#011loss=16.738198\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:39 INFO 140719711954752] processed a total of 627 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1041.3000583648682, \"sum\": 1041.3000583648682, \"min\": 1041.3000583648682}}, \"EndTime\": 1564688619.489918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688618.44814}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:39 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=602.063129632 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:39 INFO 140719711954752] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:39 INFO 140719711954752] #quality_metric: host=algo-1, epoch=171, train loss <loss>=16.7677995682\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:39 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:39 INFO 140719711954752] Epoch[172] Batch[0] avg_epoch_loss=16.791569\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:39 INFO 140719711954752] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=16.7915687561\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:40 INFO 140719711954752] Epoch[172] Batch[5] avg_epoch_loss=16.722722\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:40 INFO 140719711954752] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=16.7227217356\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:40 INFO 140719711954752] Epoch[172] Batch [5]#011Speed: 974.12 samples/sec#011loss=16.722722\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:40 INFO 140719711954752] processed a total of 593 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1052.7970790863037, \"sum\": 1052.7970790863037, \"min\": 1052.7970790863037}}, \"EndTime\": 1564688620.543249, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688619.490001}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:40 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=563.198307341 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:40 INFO 140719711954752] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:40 INFO 140719711954752] #quality_metric: host=algo-1, epoch=172, train loss <loss>=16.7095714569\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:40 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:40 INFO 140719711954752] Epoch[173] Batch[0] avg_epoch_loss=16.746553\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:40 INFO 140719711954752] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=16.746553421\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:41 INFO 140719711954752] Epoch[173] Batch[5] avg_epoch_loss=16.794802\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:41 INFO 140719711954752] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=16.7948023478\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:41 INFO 140719711954752] Epoch[173] Batch [5]#011Speed: 968.58 samples/sec#011loss=16.794802\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:41 INFO 140719711954752] processed a total of 593 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1075.6261348724365, \"sum\": 1075.6261348724365, \"min\": 1075.6261348724365}}, \"EndTime\": 1564688621.619413, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688620.543331}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:41 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=551.246926137 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:41 INFO 140719711954752] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:41 INFO 140719711954752] #quality_metric: host=algo-1, epoch=173, train loss <loss>=16.7728450775\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:41 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:42 INFO 140719711954752] Epoch[174] Batch[0] avg_epoch_loss=16.605301\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:42 INFO 140719711954752] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=16.6053009033\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:42 INFO 140719711954752] Epoch[174] Batch[5] avg_epoch_loss=16.751012\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:42 INFO 140719711954752] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=16.7510124842\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:42 INFO 140719711954752] Epoch[174] Batch [5]#011Speed: 973.05 samples/sec#011loss=16.751012\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:42 INFO 140719711954752] processed a total of 586 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1058.8369369506836, \"sum\": 1058.8369369506836, \"min\": 1058.8369369506836}}, \"EndTime\": 1564688622.678782, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688621.619494}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:42 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=553.377875621 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:42 INFO 140719711954752] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:42 INFO 140719711954752] #quality_metric: host=algo-1, epoch=174, train loss <loss>=16.7775499344\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:42 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:43 INFO 140719711954752] Epoch[175] Batch[0] avg_epoch_loss=16.816938\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=16.8169384003\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:43 INFO 140719711954752] Epoch[175] Batch[5] avg_epoch_loss=16.773653\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=16.7736530304\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:43 INFO 140719711954752] Epoch[175] Batch [5]#011Speed: 958.27 samples/sec#011loss=16.773653\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:43 INFO 140719711954752] processed a total of 571 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 994.9679374694824, \"sum\": 994.9679374694824, \"min\": 994.9679374694824}}, \"EndTime\": 1564688623.67425, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688622.67886}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:43 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=573.82775063 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:43 INFO 140719711954752] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:43 INFO 140719711954752] #quality_metric: host=algo-1, epoch=175, train loss <loss>=16.789621777\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:43 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:44 INFO 140719711954752] Epoch[176] Batch[0] avg_epoch_loss=16.812147\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:44 INFO 140719711954752] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=16.8121471405\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:43:44 INFO 140719711954752] Epoch[176] Batch[5] avg_epoch_loss=16.751077\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:44 INFO 140719711954752] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=16.7510773341\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:44 INFO 140719711954752] Epoch[176] Batch [5]#011Speed: 959.61 samples/sec#011loss=16.751077\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:44 INFO 140719711954752] processed a total of 606 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1040.6842231750488, \"sum\": 1040.6842231750488, \"min\": 1040.6842231750488}}, \"EndTime\": 1564688624.715447, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688623.674323}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:44 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=582.246108335 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:44 INFO 140719711954752] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:44 INFO 140719711954752] #quality_metric: host=algo-1, epoch=176, train loss <loss>=16.758175087\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:44 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:45 INFO 140719711954752] Epoch[177] Batch[0] avg_epoch_loss=16.851162\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=16.8511619568\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:45 INFO 140719711954752] Epoch[177] Batch[5] avg_epoch_loss=16.725200\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=16.7251996994\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:45 INFO 140719711954752] Epoch[177] Batch [5]#011Speed: 907.19 samples/sec#011loss=16.725200\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:45 INFO 140719711954752] processed a total of 624 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1068.7708854675293, \"sum\": 1068.7708854675293, \"min\": 1068.7708854675293}}, \"EndTime\": 1564688625.784724, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688624.715526}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:45 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=583.783255402 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:45 INFO 140719711954752] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:45 INFO 140719711954752] #quality_metric: host=algo-1, epoch=177, train loss <loss>=16.7535686493\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:45 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:46 INFO 140719711954752] Epoch[178] Batch[0] avg_epoch_loss=16.711010\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=16.7110099792\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:46 INFO 140719711954752] Epoch[178] Batch[5] avg_epoch_loss=16.790602\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=16.7906023661\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:46 INFO 140719711954752] Epoch[178] Batch [5]#011Speed: 923.12 samples/sec#011loss=16.790602\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:46 INFO 140719711954752] processed a total of 583 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1053.091049194336, \"sum\": 1053.091049194336, \"min\": 1053.091049194336}}, \"EndTime\": 1564688626.83837, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688625.784807}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:46 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=553.548948739 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:46 INFO 140719711954752] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:46 INFO 140719711954752] #quality_metric: host=algo-1, epoch=178, train loss <loss>=16.7869621277\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:46 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:47 INFO 140719711954752] Epoch[179] Batch[0] avg_epoch_loss=16.813034\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:47 INFO 140719711954752] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=16.8130340576\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:47 INFO 140719711954752] Epoch[179] Batch[5] avg_epoch_loss=16.788223\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:47 INFO 140719711954752] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=16.7882229487\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:47 INFO 140719711954752] Epoch[179] Batch [5]#011Speed: 896.46 samples/sec#011loss=16.788223\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:47 INFO 140719711954752] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1053.818941116333, \"sum\": 1053.818941116333, \"min\": 1053.818941116333}}, \"EndTime\": 1564688627.892694, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688626.838446}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:47 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=589.218769851 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:47 INFO 140719711954752] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:47 INFO 140719711954752] #quality_metric: host=algo-1, epoch=179, train loss <loss>=16.799474144\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:47 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:48 INFO 140719711954752] Epoch[180] Batch[0] avg_epoch_loss=16.656027\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:48 INFO 140719711954752] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=16.6560268402\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:48 INFO 140719711954752] Epoch[180] Batch[5] avg_epoch_loss=16.757096\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:48 INFO 140719711954752] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=16.7570962906\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:48 INFO 140719711954752] Epoch[180] Batch [5]#011Speed: 948.52 samples/sec#011loss=16.757096\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:48 INFO 140719711954752] processed a total of 594 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1046.7820167541504, \"sum\": 1046.7820167541504, \"min\": 1046.7820167541504}}, \"EndTime\": 1564688628.940017, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688627.892776}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:48 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=567.392647493 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:48 INFO 140719711954752] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:48 INFO 140719711954752] #quality_metric: host=algo-1, epoch=180, train loss <loss>=16.7326454163\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:48 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:49 INFO 140719711954752] Epoch[181] Batch[0] avg_epoch_loss=16.703005\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:49 INFO 140719711954752] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=16.703004837\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:49 INFO 140719711954752] Epoch[181] Batch[5] avg_epoch_loss=16.747650\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:49 INFO 140719711954752] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=16.7476504644\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:49 INFO 140719711954752] Epoch[181] Batch [5]#011Speed: 959.68 samples/sec#011loss=16.747650\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:49 INFO 140719711954752] processed a total of 601 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1020.2889442443848, \"sum\": 1020.2889442443848, \"min\": 1020.2889442443848}}, \"EndTime\": 1564688629.960862, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688628.94009}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:49 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=588.978354238 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:49 INFO 140719711954752] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:49 INFO 140719711954752] #quality_metric: host=algo-1, epoch=181, train loss <loss>=16.7466238022\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:49 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:50 INFO 140719711954752] Epoch[182] Batch[0] avg_epoch_loss=16.799513\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=16.7995128632\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:50 INFO 140719711954752] Epoch[182] Batch[5] avg_epoch_loss=16.767916\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=16.7679163615\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:50 INFO 140719711954752] Epoch[182] Batch [5]#011Speed: 956.59 samples/sec#011loss=16.767916\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:50 INFO 140719711954752] processed a total of 545 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 936.8410110473633, \"sum\": 936.8410110473633, \"min\": 936.8410110473633}}, \"EndTime\": 1564688630.898214, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688629.960944}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:50 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=581.67830246 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:50 INFO 140719711954752] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:50 INFO 140719711954752] #quality_metric: host=algo-1, epoch=182, train loss <loss>=16.7512842814\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:50 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:51 INFO 140719711954752] Epoch[183] Batch[0] avg_epoch_loss=16.719099\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=16.7190990448\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:51 INFO 140719711954752] Epoch[183] Batch[5] avg_epoch_loss=16.767228\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=16.7672281265\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:51 INFO 140719711954752] Epoch[183] Batch [5]#011Speed: 967.52 samples/sec#011loss=16.767228\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:51 INFO 140719711954752] processed a total of 594 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1006.5178871154785, \"sum\": 1006.5178871154785, \"min\": 1006.5178871154785}}, \"EndTime\": 1564688631.905274, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688630.898287}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:51 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=590.082440463 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:51 INFO 140719711954752] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:51 INFO 140719711954752] #quality_metric: host=algo-1, epoch=183, train loss <loss>=16.7530477524\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:51 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:52 INFO 140719711954752] Epoch[184] Batch[0] avg_epoch_loss=16.703682\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=16.7036819458\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:52 INFO 140719711954752] Epoch[184] Batch[5] avg_epoch_loss=16.754642\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=16.7546421687\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:52 INFO 140719711954752] Epoch[184] Batch [5]#011Speed: 940.85 samples/sec#011loss=16.754642\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:52 INFO 140719711954752] processed a total of 596 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1017.3139572143555, \"sum\": 1017.3139572143555, \"min\": 1017.3139572143555}}, \"EndTime\": 1564688632.923149, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688631.905355}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:52 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=585.800629011 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:52 INFO 140719711954752] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:52 INFO 140719711954752] #quality_metric: host=algo-1, epoch=184, train loss <loss>=16.7744697571\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:52 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:53 INFO 140719711954752] Epoch[185] Batch[0] avg_epoch_loss=16.801449\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=16.801448822\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:53 INFO 140719711954752] Epoch[185] Batch[5] avg_epoch_loss=16.757941\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=16.757941246\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:53 INFO 140719711954752] Epoch[185] Batch [5]#011Speed: 879.84 samples/sec#011loss=16.757941\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:53 INFO 140719711954752] processed a total of 580 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1041.9120788574219, \"sum\": 1041.9120788574219, \"min\": 1041.9120788574219}}, \"EndTime\": 1564688633.965548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688632.923214}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:53 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=556.620832573 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:53 INFO 140719711954752] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:53 INFO 140719711954752] #quality_metric: host=algo-1, epoch=185, train loss <loss>=16.7393165588\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:53 INFO 140719711954752] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:43:54 INFO 140719711954752] Epoch[186] Batch[0] avg_epoch_loss=16.666842\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=16.666841507\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:54 INFO 140719711954752] Epoch[186] Batch[5] avg_epoch_loss=16.711749\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=16.7117490768\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:54 INFO 140719711954752] Epoch[186] Batch [5]#011Speed: 956.84 samples/sec#011loss=16.711749\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:54 INFO 140719711954752] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1017.7750587463379, \"sum\": 1017.7750587463379, \"min\": 1017.7750587463379}}, \"EndTime\": 1564688634.983874, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688633.965608}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:54 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=610.09158508 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:54 INFO 140719711954752] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:54 INFO 140719711954752] #quality_metric: host=algo-1, epoch=186, train loss <loss>=16.7130638123\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:54 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:55 INFO 140719711954752] Epoch[187] Batch[0] avg_epoch_loss=16.732178\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:55 INFO 140719711954752] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=16.7321777344\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:55 INFO 140719711954752] Epoch[187] Batch[5] avg_epoch_loss=16.757228\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:55 INFO 140719711954752] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=16.7572275798\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:55 INFO 140719711954752] Epoch[187] Batch [5]#011Speed: 971.96 samples/sec#011loss=16.757228\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:55 INFO 140719711954752] processed a total of 588 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1007.1911811828613, \"sum\": 1007.1911811828613, \"min\": 1007.1911811828613}}, \"EndTime\": 1564688635.991614, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688634.98394}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:55 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=583.747884425 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:55 INFO 140719711954752] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:55 INFO 140719711954752] #quality_metric: host=algo-1, epoch=187, train loss <loss>=16.751691246\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:55 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:56 INFO 140719711954752] Epoch[188] Batch[0] avg_epoch_loss=16.538586\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=16.5385856628\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:56 INFO 140719711954752] Epoch[188] Batch[5] avg_epoch_loss=16.748185\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=16.7481854757\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:56 INFO 140719711954752] Epoch[188] Batch [5]#011Speed: 967.68 samples/sec#011loss=16.748185\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:56 INFO 140719711954752] processed a total of 574 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 935.8100891113281, \"sum\": 935.8100891113281, \"min\": 935.8100891113281}}, \"EndTime\": 1564688636.927977, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688635.991674}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:56 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=613.312780971 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:56 INFO 140719711954752] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:56 INFO 140719711954752] #quality_metric: host=algo-1, epoch=188, train loss <loss>=16.7384474013\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:56 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:57 INFO 140719711954752] Epoch[189] Batch[0] avg_epoch_loss=16.690214\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:57 INFO 140719711954752] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=16.6902141571\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:57 INFO 140719711954752] Epoch[189] Batch[5] avg_epoch_loss=16.755661\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:57 INFO 140719711954752] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=16.7556606929\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:57 INFO 140719711954752] Epoch[189] Batch [5]#011Speed: 944.51 samples/sec#011loss=16.755661\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] Epoch[189] Batch[10] avg_epoch_loss=16.725079\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=16.6883815765\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] Epoch[189] Batch [10]#011Speed: 951.56 samples/sec#011loss=16.688382\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] processed a total of 641 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1091.4809703826904, \"sum\": 1091.4809703826904, \"min\": 1091.4809703826904}}, \"EndTime\": 1564688638.019984, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688636.928036}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=587.214157709 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=189, train loss <loss>=16.7250792763\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] Epoch[190] Batch[0] avg_epoch_loss=16.681652\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=16.6816520691\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] Epoch[190] Batch[5] avg_epoch_loss=16.721535\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=16.7215353648\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:58 INFO 140719711954752] Epoch[190] Batch [5]#011Speed: 960.31 samples/sec#011loss=16.721535\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:59 INFO 140719711954752] processed a total of 591 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1016.8960094451904, \"sum\": 1016.8960094451904, \"min\": 1016.8960094451904}}, \"EndTime\": 1564688639.037385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688638.020063}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:59 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=581.113474063 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:59 INFO 140719711954752] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=190, train loss <loss>=16.7192565918\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:59 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:59 INFO 140719711954752] Epoch[191] Batch[0] avg_epoch_loss=16.802235\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=16.8022346497\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:59 INFO 140719711954752] Epoch[191] Batch[5] avg_epoch_loss=16.757646\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:59 INFO 140719711954752] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=16.7576459249\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:43:59 INFO 140719711954752] Epoch[191] Batch [5]#011Speed: 901.82 samples/sec#011loss=16.757646\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:00 INFO 140719711954752] processed a total of 571 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 998.2039928436279, \"sum\": 998.2039928436279, \"min\": 998.2039928436279}}, \"EndTime\": 1564688640.036124, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688639.037465}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:00 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=571.959742706 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:00 INFO 140719711954752] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=191, train loss <loss>=16.7426738739\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:00 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:00 INFO 140719711954752] Epoch[192] Batch[0] avg_epoch_loss=16.779774\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=16.7797737122\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:00 INFO 140719711954752] Epoch[192] Batch[5] avg_epoch_loss=16.765423\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:00 INFO 140719711954752] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=16.765422821\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:00 INFO 140719711954752] Epoch[192] Batch [5]#011Speed: 962.29 samples/sec#011loss=16.765423\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:01 INFO 140719711954752] processed a total of 583 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1026.7879962921143, \"sum\": 1026.7879962921143, \"min\": 1026.7879962921143}}, \"EndTime\": 1564688641.063446, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688640.036205}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:01 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=567.738102253 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:01 INFO 140719711954752] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:01 INFO 140719711954752] #quality_metric: host=algo-1, epoch=192, train loss <loss>=16.7544589996\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:01 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:01 INFO 140719711954752] Epoch[193] Batch[0] avg_epoch_loss=16.592867\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:01 INFO 140719711954752] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=16.5928668976\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:01 INFO 140719711954752] Epoch[193] Batch[5] avg_epoch_loss=16.691419\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:01 INFO 140719711954752] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=16.6914192835\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:01 INFO 140719711954752] Epoch[193] Batch [5]#011Speed: 869.60 samples/sec#011loss=16.691419\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:02 INFO 140719711954752] processed a total of 608 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1059.4110488891602, \"sum\": 1059.4110488891602, \"min\": 1059.4110488891602}}, \"EndTime\": 1564688642.123369, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688641.063509}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:02 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=573.83881665 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:02 INFO 140719711954752] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=193, train loss <loss>=16.7305965424\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:02 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:02 INFO 140719711954752] Epoch[194] Batch[0] avg_epoch_loss=16.739401\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=16.7394008636\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:02 INFO 140719711954752] Epoch[194] Batch[5] avg_epoch_loss=16.691387\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:02 INFO 140719711954752] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=16.6913868586\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:02 INFO 140719711954752] Epoch[194] Batch [5]#011Speed: 932.99 samples/sec#011loss=16.691387\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:03 INFO 140719711954752] processed a total of 551 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 972.8310108184814, \"sum\": 972.8310108184814, \"min\": 972.8310108184814}}, \"EndTime\": 1564688643.096753, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688642.123452}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:03 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=566.327125465 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:03 INFO 140719711954752] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=194, train loss <loss>=16.731534746\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:03 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:03 INFO 140719711954752] Epoch[195] Batch[0] avg_epoch_loss=16.815214\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=16.8152141571\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:03 INFO 140719711954752] Epoch[195] Batch[5] avg_epoch_loss=16.781118\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:03 INFO 140719711954752] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=16.7811183929\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:03 INFO 140719711954752] Epoch[195] Batch [5]#011Speed: 952.93 samples/sec#011loss=16.781118\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:04 INFO 140719711954752] processed a total of 611 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1020.4238891601562, \"sum\": 1020.4238891601562, \"min\": 1020.4238891601562}}, \"EndTime\": 1564688644.117719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688643.09682}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:04 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=598.70152925 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:04 INFO 140719711954752] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=195, train loss <loss>=16.7717105865\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:04 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:04 INFO 140719711954752] Epoch[196] Batch[0] avg_epoch_loss=16.676973\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=16.6769733429\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:04 INFO 140719711954752] Epoch[196] Batch[5] avg_epoch_loss=16.747781\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:04 INFO 140719711954752] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=16.7477814356\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:04 INFO 140719711954752] Epoch[196] Batch [5]#011Speed: 927.13 samples/sec#011loss=16.747781\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:05 INFO 140719711954752] processed a total of 586 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1047.2080707550049, \"sum\": 1047.2080707550049, \"min\": 1047.2080707550049}}, \"EndTime\": 1564688645.165467, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688644.1178}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:05 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=559.519592935 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:05 INFO 140719711954752] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=196, train loss <loss>=16.7338039398\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:05 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:05 INFO 140719711954752] Epoch[197] Batch[0] avg_epoch_loss=16.664062\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=16.6640625\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:05 INFO 140719711954752] Epoch[197] Batch[5] avg_epoch_loss=16.756338\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:05 INFO 140719711954752] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=16.7563381195\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:05 INFO 140719711954752] Epoch[197] Batch [5]#011Speed: 957.55 samples/sec#011loss=16.756338\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:06 INFO 140719711954752] processed a total of 587 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1033.888816833496, \"sum\": 1033.888816833496, \"min\": 1033.888816833496}}, \"EndTime\": 1564688646.199888, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688645.165548}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:06 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=567.696077891 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:06 INFO 140719711954752] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:06 INFO 140719711954752] #quality_metric: host=algo-1, epoch=197, train loss <loss>=16.7652687073\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:06 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:06 INFO 140719711954752] Epoch[198] Batch[0] avg_epoch_loss=16.806389\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:06 INFO 140719711954752] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=16.806388855\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:06 INFO 140719711954752] Epoch[198] Batch[5] avg_epoch_loss=16.768364\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:06 INFO 140719711954752] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=16.7683636347\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:06 INFO 140719711954752] Epoch[198] Batch [5]#011Speed: 958.41 samples/sec#011loss=16.768364\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:07 INFO 140719711954752] processed a total of 586 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1031.074047088623, \"sum\": 1031.074047088623, \"min\": 1031.074047088623}}, \"EndTime\": 1564688647.231599, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688646.199967}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:07 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=568.282101994 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:07 INFO 140719711954752] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:07 INFO 140719711954752] #quality_metric: host=algo-1, epoch=198, train loss <loss>=16.7699106216\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:07 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:07 INFO 140719711954752] Epoch[199] Batch[0] avg_epoch_loss=16.653465\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:07 INFO 140719711954752] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=16.653465271\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:07 INFO 140719711954752] Epoch[199] Batch[5] avg_epoch_loss=16.731899\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:07 INFO 140719711954752] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=16.7318986257\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:07 INFO 140719711954752] Epoch[199] Batch [5]#011Speed: 934.90 samples/sec#011loss=16.731899\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:08 INFO 140719711954752] processed a total of 566 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 958.4331512451172, \"sum\": 958.4331512451172, \"min\": 958.4331512451172}}, \"EndTime\": 1564688648.190565, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688647.231664}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:08 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=590.489898641 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:08 INFO 140719711954752] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=199, train loss <loss>=16.7349953122\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:08 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:08 INFO 140719711954752] Epoch[200] Batch[0] avg_epoch_loss=16.876192\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=16.8761920929\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:08 INFO 140719711954752] Epoch[200] Batch[5] avg_epoch_loss=16.795563\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:08 INFO 140719711954752] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=16.7955627441\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:08 INFO 140719711954752] Epoch[200] Batch [5]#011Speed: 953.96 samples/sec#011loss=16.795563\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] processed a total of 582 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1036.412000656128, \"sum\": 1036.412000656128, \"min\": 1036.412000656128}}, \"EndTime\": 1564688649.227472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688648.190627}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] #throughput_metric: host=algo-1, train throughput=561.488803967 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] #quality_metric: host=algo-1, epoch=200, train loss <loss>=16.7900470734\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] Loading parameters from best epoch (160)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 10.210037231445312, \"sum\": 10.210037231445312, \"min\": 10.210037231445312}}, \"EndTime\": 1564688649.238275, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688649.227553}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] stopping training now\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] Final loss: 16.6534341812 (occurred at epoch 160)\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] #quality_metric: host=algo-1, train final_loss <loss>=16.6534341812\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 WARNING 140719711954752] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 254.47607040405273, \"sum\": 254.47607040405273, \"min\": 254.47607040405273}}, \"EndTime\": 1564688649.493531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688649.238333}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 330.4440975189209, \"sum\": 330.4440975189209, \"min\": 330.4440975189209}}, \"EndTime\": 1564688649.569472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688649.49361}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 11.940956115722656, \"sum\": 11.940956115722656, \"min\": 11.940956115722656}}, \"EndTime\": 1564688649.581528, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688649.56955}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:09 INFO 140719711954752] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.026941299438476562, \"sum\": 0.026941299438476562, \"min\": 0.026941299438476562}}, \"EndTime\": 1564688649.582221, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688649.581575}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 2543.998956680298, \"sum\": 2543.998956680298, \"min\": 2543.998956680298}}, \"EndTime\": 1564688652.126194, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688649.582274}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #test_score (algo-1, RMSE): 7309602.06541\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #test_score (algo-1, mean_wQuantileLoss): 0.662176\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #test_score (algo-1, wQuantileLoss[0.1]): 0.577736\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #test_score (algo-1, wQuantileLoss[0.2]): 0.749428\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #test_score (algo-1, wQuantileLoss[0.3]): 0.795273\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #test_score (algo-1, wQuantileLoss[0.4]): 0.811999\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #test_score (algo-1, wQuantileLoss[0.5]): 0.801869\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #test_score (algo-1, wQuantileLoss[0.6]): 0.750555\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #test_score (algo-1, wQuantileLoss[0.7]): 0.653166\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #test_score (algo-1, wQuantileLoss[0.8]): 0.508603\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #test_score (algo-1, wQuantileLoss[0.9]): 0.310952\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.662175774574\u001b[0m\n",
      "\u001b[31m[08/01/2019 19:44:12 INFO 140719711954752] #quality_metric: host=algo-1, test RMSE <loss>=7309602.06541\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 210146.9180583954, \"sum\": 210146.9180583954, \"min\": 210146.9180583954}, \"setuptime\": {\"count\": 1, \"max\": 10.046005249023438, \"sum\": 10.046005249023438, \"min\": 10.046005249023438}}, \"EndTime\": 1564688652.148753, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564688652.126257}\n",
      "\u001b[0m\n",
      "\n",
      "2019-08-01 19:44:20 Uploading - Uploading generated training model\n",
      "2019-08-01 19:44:20 Completed - Training job completed\n",
      "Billable seconds: 256\n"
     ]
    }
   ],
   "source": [
    "data_channels = {\n",
    "    \"train\": \"s3://forecasting-do-not-delete/train/train.json\",\n",
    "    \"test\": \"s3://forecasting-do-not-delete/test/test.json\"\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run Inference\n",
    "If you made it this far, congratulations! None of this is easy. For your next steps, please open up the example notebook under the SageMakerExamples:\n",
    "\n",
    "- SageMakerExamples/Introduction To Amazon Algorithms/DeepAR-Electricity.\n",
    "- **Guys I found this online - not sure if this is the right thing or not but wanted to add it here in case we don't get to it before I have to leave**\n",
    "    - https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_electricity/DeepAR-Electricity.ipynb\n",
    "\n",
    "That will walk you through both how to add more timeseries to your model, and how to get inference results out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Extend Your Solution\n",
    "Now you're getting forecasts, how will you extend your solution? How good are your forecasts? What about getting forecasts for the other stations? Is your model cognizant of the weather?\n",
    "\n",
    "Spend your remaining time growing your modeling solution to leverage additional datasets. Then, think through how you'd set this up to run in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
