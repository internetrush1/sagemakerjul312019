{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train you first forecasting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you picked the forecasting project. Good for you! Forecasting is an important skill, and it's helpful to learn about the techchnologies out there that can make your life easier. Or worse, depending on how you look at it. ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the Kaggle forecasting data\n",
    "Use the notebook example for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Access your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sagemaker\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5113, 99)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ACME</th>\n",
       "      <th>ADAX</th>\n",
       "      <th>ALTU</th>\n",
       "      <th>APAC</th>\n",
       "      <th>ARNE</th>\n",
       "      <th>BEAV</th>\n",
       "      <th>BESS</th>\n",
       "      <th>BIXB</th>\n",
       "      <th>BLAC</th>\n",
       "      <th>...</th>\n",
       "      <th>VINI</th>\n",
       "      <th>WASH</th>\n",
       "      <th>WATO</th>\n",
       "      <th>WAUR</th>\n",
       "      <th>WEAT</th>\n",
       "      <th>WEST</th>\n",
       "      <th>WILB</th>\n",
       "      <th>WIST</th>\n",
       "      <th>WOOD</th>\n",
       "      <th>WYNO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19940101</td>\n",
       "      <td>12384900</td>\n",
       "      <td>11930700</td>\n",
       "      <td>12116700</td>\n",
       "      <td>12301200</td>\n",
       "      <td>10706100</td>\n",
       "      <td>10116900</td>\n",
       "      <td>11487900</td>\n",
       "      <td>11182800</td>\n",
       "      <td>10848300</td>\n",
       "      <td>...</td>\n",
       "      <td>10771800</td>\n",
       "      <td>12116400</td>\n",
       "      <td>11308800</td>\n",
       "      <td>12361800</td>\n",
       "      <td>11331600</td>\n",
       "      <td>10644300</td>\n",
       "      <td>11715600</td>\n",
       "      <td>11241000</td>\n",
       "      <td>10490100</td>\n",
       "      <td>10545300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19940102</td>\n",
       "      <td>11908500</td>\n",
       "      <td>9778500</td>\n",
       "      <td>10862700</td>\n",
       "      <td>11666400</td>\n",
       "      <td>8062500</td>\n",
       "      <td>9262800</td>\n",
       "      <td>9235200</td>\n",
       "      <td>3963300</td>\n",
       "      <td>3318300</td>\n",
       "      <td>...</td>\n",
       "      <td>4314300</td>\n",
       "      <td>10733400</td>\n",
       "      <td>9154800</td>\n",
       "      <td>12041400</td>\n",
       "      <td>9168300</td>\n",
       "      <td>4082700</td>\n",
       "      <td>9228000</td>\n",
       "      <td>5829900</td>\n",
       "      <td>7412100</td>\n",
       "      <td>3345300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940103</td>\n",
       "      <td>12470700</td>\n",
       "      <td>9771900</td>\n",
       "      <td>12627300</td>\n",
       "      <td>12782700</td>\n",
       "      <td>11618400</td>\n",
       "      <td>10789800</td>\n",
       "      <td>11895900</td>\n",
       "      <td>4512600</td>\n",
       "      <td>5266500</td>\n",
       "      <td>...</td>\n",
       "      <td>2976900</td>\n",
       "      <td>11775000</td>\n",
       "      <td>10700400</td>\n",
       "      <td>12687300</td>\n",
       "      <td>11324400</td>\n",
       "      <td>2746500</td>\n",
       "      <td>3686700</td>\n",
       "      <td>4488900</td>\n",
       "      <td>9712200</td>\n",
       "      <td>4442100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19940104</td>\n",
       "      <td>12725400</td>\n",
       "      <td>6466800</td>\n",
       "      <td>13065300</td>\n",
       "      <td>12817500</td>\n",
       "      <td>12134400</td>\n",
       "      <td>11816700</td>\n",
       "      <td>12186600</td>\n",
       "      <td>3212700</td>\n",
       "      <td>8270100</td>\n",
       "      <td>...</td>\n",
       "      <td>3476400</td>\n",
       "      <td>12159600</td>\n",
       "      <td>11907000</td>\n",
       "      <td>12953100</td>\n",
       "      <td>11903700</td>\n",
       "      <td>2741400</td>\n",
       "      <td>4905000</td>\n",
       "      <td>4089300</td>\n",
       "      <td>11401500</td>\n",
       "      <td>4365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19940105</td>\n",
       "      <td>10894800</td>\n",
       "      <td>11545200</td>\n",
       "      <td>8060400</td>\n",
       "      <td>10379400</td>\n",
       "      <td>6918600</td>\n",
       "      <td>9936300</td>\n",
       "      <td>6411300</td>\n",
       "      <td>9566100</td>\n",
       "      <td>8009400</td>\n",
       "      <td>...</td>\n",
       "      <td>6393300</td>\n",
       "      <td>11419500</td>\n",
       "      <td>7334400</td>\n",
       "      <td>10178700</td>\n",
       "      <td>7471500</td>\n",
       "      <td>8235300</td>\n",
       "      <td>11159100</td>\n",
       "      <td>10651500</td>\n",
       "      <td>10006200</td>\n",
       "      <td>8568300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date      ACME      ADAX      ALTU      APAC      ARNE      BEAV  \\\n",
       "0  19940101  12384900  11930700  12116700  12301200  10706100  10116900   \n",
       "1  19940102  11908500   9778500  10862700  11666400   8062500   9262800   \n",
       "2  19940103  12470700   9771900  12627300  12782700  11618400  10789800   \n",
       "3  19940104  12725400   6466800  13065300  12817500  12134400  11816700   \n",
       "4  19940105  10894800  11545200   8060400  10379400   6918600   9936300   \n",
       "\n",
       "       BESS      BIXB      BLAC  ...      VINI      WASH      WATO      WAUR  \\\n",
       "0  11487900  11182800  10848300  ...  10771800  12116400  11308800  12361800   \n",
       "1   9235200   3963300   3318300  ...   4314300  10733400   9154800  12041400   \n",
       "2  11895900   4512600   5266500  ...   2976900  11775000  10700400  12687300   \n",
       "3  12186600   3212700   8270100  ...   3476400  12159600  11907000  12953100   \n",
       "4   6411300   9566100   8009400  ...   6393300  11419500   7334400  10178700   \n",
       "\n",
       "       WEAT      WEST      WILB      WIST      WOOD      WYNO  \n",
       "0  11331600  10644300  11715600  11241000  10490100  10545300  \n",
       "1   9168300   4082700   9228000   5829900   7412100   3345300  \n",
       "2  11324400   2746500   3686700   4488900   9712200   4442100  \n",
       "3  11903700   2741400   4905000   4089300  11401500   4365000  \n",
       "4   7471500   8235300  11159100  10651500  10006200   8568300  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row here is a new point in time, and each column is an energy station. That means that each COLUMN is a unique time series data set. We are going to train our first model on a single column. Then, you can extend it by adding more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACME', 'ADAX', 'ALTU', 'APAC', 'ARNE', 'BEAV', 'BESS', 'BIXB', 'BLAC', 'BOIS', 'BOWL', 'BREC', 'BRIS', 'BUFF', 'BURB', 'BURN', 'BUTL', 'BYAR', 'CAMA', 'CENT', 'CHAN', 'CHER', 'CHEY', 'CHIC', 'CLAY', 'CLOU', 'COOK', 'COPA', 'DURA', 'ELRE', 'ERIC', 'EUFA', 'FAIR', 'FORA', 'FREE', 'FTCB', 'GOOD', 'GUTH', 'HASK', 'HINT', 'HOBA', 'HOLL', 'HOOK', 'HUGO', 'IDAB', 'JAYX', 'KENT', 'KETC', 'LAHO', 'LANE', 'MADI', 'MANG', 'MARE', 'MAYR', 'MCAL', 'MEDF', 'MEDI', 'MIAM', 'MINC', 'MTHE', 'NEWK', 'NINN', 'NOWA', 'OILT', 'OKEM', 'OKMU', 'PAUL', 'PAWN', 'PERK', 'PRYO', 'PUTN', 'REDR', 'RETR', 'RING', 'SALL', 'SEIL', 'SHAW', 'SKIA', 'SLAP', 'SPEN', 'STIG', 'STIL', 'STUA', 'SULP', 'TAHL', 'TALI', 'TIPT', 'TISH', 'VINI', 'WASH', 'WATO', 'WAUR', 'WEAT', 'WEST', 'WILB', 'WIST', 'WOOD', 'WYNO']\n"
     ]
    }
   ],
   "source": [
    "# build list of locations\n",
    "columns = list(df.columns[1:])\n",
    "columns = columns\n",
    "# columns[:85]\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['ACME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5113,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have 5113 obervations. That's well over the 300-limit on DeepAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Train and Test Sets\n",
    "Now, we'll build 2 datasets. One for training, another for testing. Both need to be written to json files, then copied over to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(df, freq='D', split_type = 'train', cols_to_use = ['ACME']):\n",
    "    rt_set = []\n",
    "    \n",
    "    # use 70% for training\n",
    "    if split_type == 'train':\n",
    "        lower_bound = 0\n",
    "        upper_bound = round(df.shape[0] * .7)\n",
    "        \n",
    "    # use 15% for validation\n",
    "    elif split_type == 'validation':\n",
    "        lower_bound = round(df.shape[0] * .7)\n",
    "        upper_bound = round(df.shape[0] * .85)\n",
    "        \n",
    "    # use 15% for test\n",
    "    elif split_type == 'test':\n",
    "        lower_bound = round(df.shape[0] * .85)\n",
    "        upper_bound = df.shape[0]\n",
    "            \n",
    "    # loop through columns you want to use\n",
    "    for h in list(df):\n",
    "        if h in cols_to_use:\n",
    "            \n",
    "            target_column = df[h].values.tolist()[lower_bound:upper_bound]\n",
    "            \n",
    "            date_str = str(df.iloc[0]['Date'])\n",
    "            \n",
    "            year = date_str[0:4]\n",
    "            month = date_str[4:6]\n",
    "            date = date_str[7:]\n",
    "                                                \n",
    "            start_dataset = pd.Timestamp(\"{}-{}-{} 00:00:00\".format(year, month, date, freq=freq))\n",
    "                        \n",
    "            # create a new json object for each column\n",
    "            json_obj = {\n",
    "                        'start': str(start_dataset),\n",
    "                        'target': target_column,\n",
    "                        'station': h\n",
    "                       }\n",
    "    \n",
    "            rt_set.append(json_obj)\n",
    "    \n",
    "    return rt_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_split(df, cols_to_use = columns)\n",
    "test_set = get_split(df, split_type = 'test', cols_to_use = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            # fp.write(json.dumps(data).encode(\"utf-8\"))\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dicts_to_file('train.json', train_set)\n",
    "write_dicts_to_file('test.json', test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./train.json to s3://forecasting-do-not-delete/train/train.json\n",
      "upload: ./test.json to s3://forecasting-do-not-delete/test/test.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp train.json s3://forecasting-do-not-delete/train/train.json\n",
    "!aws s3 cp test.json s3://forecasting-do-not-delete/test/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run a SageMaker Training Job\n",
    "Ok! If everything worked, we should be able to train a model in SageMaker straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "image = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "role = sagemaker.get_execution_role()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sess,\n",
    "    image_name=image,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deepar-electricity-demo',\n",
    "    output_path='s3://forecasting-do-not-delete/output'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \n",
    "    # frequency interval is once per day\n",
    "    \"time_freq\": 'D',\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \n",
    "    # let's use the last 30 days for context\n",
    "    \"context_length\": str(30),\n",
    "    \n",
    "    # let's forecast for 30 days\n",
    "    \"prediction_length\": str(30)\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-01 21:47:28 Starting - Starting the training job...\n",
      "2019-08-01 21:47:30 Starting - Launching requested ML instances......\n",
      "2019-08-01 21:48:35 Starting - Preparing the instances for training...\n",
      "2019-08-01 21:49:29 Downloading - Downloading input data......\n",
      "2019-08-01 21:50:20 Training - Training image download completed. Training in progress.\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'30', u'epochs': u'400', u'time_freq': u'D', u'context_length': u'30', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'30', u'time_freq': u'D', u'context_length': u'30', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Training set statistics:\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Integer time series\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] number of time series: 98\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] number of observations: 350742\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] mean target length: 3579\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] min/mean/max target: 300.0/16712640.5821/39442800.0\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] mean abs(target): 16712640.5821\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] contains missing values: no\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Small number of time series. Doing 6 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Test set statistics:\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Integer time series\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] number of time series: 98\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] number of observations: 75166\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] mean target length: 767\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] min/mean/max target: 4200.0/16273434.2382/32884800.0\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] mean abs(target): 16273434.2382\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] contains missing values: no\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] nvidia-smi took: 0.0251741409302 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:22 INFO 140504085288768] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 176.66006088256836, \"sum\": 176.66006088256836, \"min\": 176.66006088256836}}, \"EndTime\": 1564696223.001562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696222.824043}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:23 INFO 140504085288768] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 437.1001720428467, \"sum\": 437.1001720428467, \"min\": 437.1001720428467}}, \"EndTime\": 1564696223.261266, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696223.001649}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:23 INFO 140504085288768] Epoch[0] Batch[0] avg_epoch_loss=19.513237\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:23 INFO 140504085288768] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=19.5132369995\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:24 INFO 140504085288768] Epoch[0] Batch[5] avg_epoch_loss=19.283202\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:24 INFO 140504085288768] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=19.2832024892\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:24 INFO 140504085288768] Epoch[0] Batch [5]#011Speed: 957.58 samples/sec#011loss=19.283202\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:24 INFO 140504085288768] processed a total of 574 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 1137.6569271087646, \"sum\": 1137.6569271087646, \"min\": 1137.6569271087646}}, \"EndTime\": 1564696224.399148, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696223.261345}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:24 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=504.48593817 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:24 INFO 140504085288768] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:24 INFO 140504085288768] #quality_metric: host=algo-1, epoch=0, train loss <loss>=19.0580323537\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:24 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:24 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_9b422a80-8a04-412f-8497-511b56fce059-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 35.45713424682617, \"sum\": 35.45713424682617, \"min\": 35.45713424682617}}, \"EndTime\": 1564696224.43528, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696224.399244}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:24 INFO 140504085288768] Epoch[1] Batch[0] avg_epoch_loss=18.428280\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:24 INFO 140504085288768] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=18.4282798767\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:25 INFO 140504085288768] Epoch[1] Batch[5] avg_epoch_loss=18.210465\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:25 INFO 140504085288768] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=18.2104654312\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:25 INFO 140504085288768] Epoch[1] Batch [5]#011Speed: 738.87 samples/sec#011loss=18.210465\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:25 INFO 140504085288768] processed a total of 598 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1209.4240188598633, \"sum\": 1209.4240188598633, \"min\": 1209.4240188598633}}, \"EndTime\": 1564696225.644838, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696224.435351}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:25 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=494.406578963 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:25 INFO 140504085288768] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:25 INFO 140504085288768] #quality_metric: host=algo-1, epoch=1, train loss <loss>=18.046235466\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:25 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:25 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_62dcf350-e901-4495-bdbe-1305dfd2598f-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.53789520263672, \"sum\": 26.53789520263672, \"min\": 26.53789520263672}}, \"EndTime\": 1564696225.672001, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696225.644908}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:26 INFO 140504085288768] Epoch[2] Batch[0] avg_epoch_loss=17.566441\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=17.5664405823\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:26 INFO 140504085288768] Epoch[2] Batch[5] avg_epoch_loss=17.453325\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=17.4533252716\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:26 INFO 140504085288768] Epoch[2] Batch [5]#011Speed: 912.07 samples/sec#011loss=17.453325\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:26 INFO 140504085288768] processed a total of 596 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1092.8020477294922, \"sum\": 1092.8020477294922, \"min\": 1092.8020477294922}}, \"EndTime\": 1564696226.764945, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696225.672078}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:26 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=545.327127754 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:26 INFO 140504085288768] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=2, train loss <loss>=17.4340162277\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:26 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:26 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_aa6c6a19-d31b-44e1-82ba-e1648f5bd800-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 35.546064376831055, \"sum\": 35.546064376831055, \"min\": 35.546064376831055}}, \"EndTime\": 1564696226.801082, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696226.765028}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:27 INFO 140504085288768] Epoch[3] Batch[0] avg_epoch_loss=17.382019\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=17.382019043\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:27 INFO 140504085288768] Epoch[3] Batch[5] avg_epoch_loss=17.295551\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=17.2955513\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:27 INFO 140504085288768] Epoch[3] Batch [5]#011Speed: 954.43 samples/sec#011loss=17.295551\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:27 INFO 140504085288768] processed a total of 588 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1088.374137878418, \"sum\": 1088.374137878418, \"min\": 1088.374137878418}}, \"EndTime\": 1564696227.889601, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696226.801159}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:27 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=540.195399431 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:27 INFO 140504085288768] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=3, train loss <loss>=17.3074132919\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:27 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:27 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_0fcb5248-7864-4fae-b1c4-59212afa02d3-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.225069046020508, \"sum\": 23.225069046020508, \"min\": 23.225069046020508}}, \"EndTime\": 1564696227.913399, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696227.889683}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:28 INFO 140504085288768] Epoch[4] Batch[0] avg_epoch_loss=17.267715\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=17.2677154541\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:28 INFO 140504085288768] Epoch[4] Batch[5] avg_epoch_loss=17.219688\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=17.2196884155\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:28 INFO 140504085288768] Epoch[4] Batch [5]#011Speed: 949.26 samples/sec#011loss=17.219688\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:28 INFO 140504085288768] processed a total of 581 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1024.832010269165, \"sum\": 1024.832010269165, \"min\": 1024.832010269165}}, \"EndTime\": 1564696228.938352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696227.91347}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:28 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=566.866266826 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:28 INFO 140504085288768] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=4, train loss <loss>=17.2412164688\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:28 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:28 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_5a6a1dfd-f564-41a2-b0d9-3708376e865c-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.410154342651367, \"sum\": 22.410154342651367, \"min\": 22.410154342651367}}, \"EndTime\": 1564696228.961328, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696228.938425}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:29 INFO 140504085288768] Epoch[5] Batch[0] avg_epoch_loss=17.236885\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:29 INFO 140504085288768] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=17.2368850708\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:29 INFO 140504085288768] Epoch[5] Batch[5] avg_epoch_loss=17.177611\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:29 INFO 140504085288768] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=17.177611351\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:29 INFO 140504085288768] Epoch[5] Batch [5]#011Speed: 899.15 samples/sec#011loss=17.177611\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:30 INFO 140504085288768] processed a total of 584 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1067.6050186157227, \"sum\": 1067.6050186157227, \"min\": 1067.6050186157227}}, \"EndTime\": 1564696230.02905, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696228.961383}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:30 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=546.964550866 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:30 INFO 140504085288768] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=5, train loss <loss>=17.192890358\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:30 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:30 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_6bab3066-d8eb-4ae7-85d4-e53a3003d4dd-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.302865982055664, \"sum\": 22.302865982055664, \"min\": 22.302865982055664}}, \"EndTime\": 1564696230.051973, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696230.029124}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:30 INFO 140504085288768] Epoch[6] Batch[0] avg_epoch_loss=17.245314\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=17.2453136444\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:30 INFO 140504085288768] Epoch[6] Batch[5] avg_epoch_loss=17.154311\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=17.1543108622\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:30 INFO 140504085288768] Epoch[6] Batch [5]#011Speed: 884.14 samples/sec#011loss=17.154311\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:31 INFO 140504085288768] processed a total of 548 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1036.5350246429443, \"sum\": 1036.5350246429443, \"min\": 1036.5350246429443}}, \"EndTime\": 1564696231.08865, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696230.052048}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:31 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=528.624797322 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:31 INFO 140504085288768] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=6, train loss <loss>=17.1317842272\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:31 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:31 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_7eb83b45-a8cd-453a-9a4a-9c1731af4a97-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.84201431274414, \"sum\": 34.84201431274414, \"min\": 34.84201431274414}}, \"EndTime\": 1564696231.124078, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696231.08873}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:31 INFO 140504085288768] Epoch[7] Batch[0] avg_epoch_loss=17.160444\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=17.1604442596\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:31 INFO 140504085288768] Epoch[7] Batch[5] avg_epoch_loss=17.138220\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=17.1382198334\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:31 INFO 140504085288768] Epoch[7] Batch [5]#011Speed: 948.83 samples/sec#011loss=17.138220\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:32 INFO 140504085288768] processed a total of 593 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1079.6170234680176, \"sum\": 1079.6170234680176, \"min\": 1079.6170234680176}}, \"EndTime\": 1564696232.203837, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696231.124154}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:32 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=549.211359688 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:32 INFO 140504085288768] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:32 INFO 140504085288768] #quality_metric: host=algo-1, epoch=7, train loss <loss>=17.1338788986\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:32 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:32 INFO 140504085288768] Epoch[8] Batch[0] avg_epoch_loss=17.089544\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:32 INFO 140504085288768] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=17.0895442963\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:33 INFO 140504085288768] Epoch[8] Batch[5] avg_epoch_loss=17.097921\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=17.0979210536\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:33 INFO 140504085288768] Epoch[8] Batch [5]#011Speed: 889.13 samples/sec#011loss=17.097921\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:33 INFO 140504085288768] processed a total of 614 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1072.396993637085, \"sum\": 1072.396993637085, \"min\": 1072.396993637085}}, \"EndTime\": 1564696233.2768, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696232.203912}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:33 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=572.493032189 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:33 INFO 140504085288768] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=8, train loss <loss>=17.088334465\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:33 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:33 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_8ff3990f-fabb-40e3-a3b6-17e55d67d489-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.674083709716797, \"sum\": 22.674083709716797, \"min\": 22.674083709716797}}, \"EndTime\": 1564696233.300088, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696233.276873}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:33 INFO 140504085288768] Epoch[9] Batch[0] avg_epoch_loss=17.112608\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=17.1126079559\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:34 INFO 140504085288768] Epoch[9] Batch[5] avg_epoch_loss=17.143462\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=17.143462499\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:34 INFO 140504085288768] Epoch[9] Batch [5]#011Speed: 954.73 samples/sec#011loss=17.143462\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:34 INFO 140504085288768] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1035.6810092926025, \"sum\": 1035.6810092926025, \"min\": 1035.6810092926025}}, \"EndTime\": 1564696234.335909, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696233.300161}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:34 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=576.3727383 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:34 INFO 140504085288768] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=9, train loss <loss>=17.1411493301\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:34 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:34 INFO 140504085288768] Epoch[10] Batch[0] avg_epoch_loss=17.136038\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=17.1360378265\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:35 INFO 140504085288768] Epoch[10] Batch[5] avg_epoch_loss=17.081557\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:35 INFO 140504085288768] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=17.0815566381\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:35 INFO 140504085288768] Epoch[10] Batch [5]#011Speed: 958.90 samples/sec#011loss=17.081557\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:35 INFO 140504085288768] processed a total of 623 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1065.8178329467773, \"sum\": 1065.8178329467773, \"min\": 1065.8178329467773}}, \"EndTime\": 1564696235.402283, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696234.335978}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:35 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=584.46829909 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:35 INFO 140504085288768] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:35 INFO 140504085288768] #quality_metric: host=algo-1, epoch=10, train loss <loss>=17.0414800644\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:35 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:35 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_cdd82a1b-1f35-4f1d-97b9-310437d889a2-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.333932876586914, \"sum\": 21.333932876586914, \"min\": 21.333932876586914}}, \"EndTime\": 1564696235.424212, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696235.402358}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:50:35 INFO 140504085288768] Epoch[11] Batch[0] avg_epoch_loss=17.102144\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:35 INFO 140504085288768] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=17.1021442413\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:36 INFO 140504085288768] Epoch[11] Batch[5] avg_epoch_loss=17.078759\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=17.0787588755\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:36 INFO 140504085288768] Epoch[11] Batch [5]#011Speed: 918.15 samples/sec#011loss=17.078759\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:36 INFO 140504085288768] processed a total of 566 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 994.5809841156006, \"sum\": 994.5809841156006, \"min\": 994.5809841156006}}, \"EndTime\": 1564696236.418934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696235.424285}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:36 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=569.016354951 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:36 INFO 140504085288768] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=11, train loss <loss>=17.0668165419\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:36 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:36 INFO 140504085288768] Epoch[12] Batch[0] avg_epoch_loss=17.067034\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=17.0670337677\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:37 INFO 140504085288768] Epoch[12] Batch[5] avg_epoch_loss=17.031131\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=17.0311307907\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:37 INFO 140504085288768] Epoch[12] Batch [5]#011Speed: 957.78 samples/sec#011loss=17.031131\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:37 INFO 140504085288768] processed a total of 569 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 985.6679439544678, \"sum\": 985.6679439544678, \"min\": 985.6679439544678}}, \"EndTime\": 1564696237.405138, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696236.419015}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:37 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=577.205522655 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:37 INFO 140504085288768] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=12, train loss <loss>=17.0290696886\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:37 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:37 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_5d9242a4-8e59-4746-a47d-1c7fe7876002-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.9581241607666, \"sum\": 34.9581241607666, \"min\": 34.9581241607666}}, \"EndTime\": 1564696237.440706, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696237.405218}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:37 INFO 140504085288768] Epoch[13] Batch[0] avg_epoch_loss=17.116634\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=17.1166343689\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:38 INFO 140504085288768] Epoch[13] Batch[5] avg_epoch_loss=17.076004\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:38 INFO 140504085288768] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=17.0760040283\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:38 INFO 140504085288768] Epoch[13] Batch [5]#011Speed: 963.14 samples/sec#011loss=17.076004\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:38 INFO 140504085288768] processed a total of 546 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 959.8820209503174, \"sum\": 959.8820209503174, \"min\": 959.8820209503174}}, \"EndTime\": 1564696238.40073, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696237.440783}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:38 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=568.749412398 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:38 INFO 140504085288768] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:38 INFO 140504085288768] #quality_metric: host=algo-1, epoch=13, train loss <loss>=17.0658923255\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:38 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:38 INFO 140504085288768] Epoch[14] Batch[0] avg_epoch_loss=17.084066\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:38 INFO 140504085288768] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=17.084066391\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:39 INFO 140504085288768] Epoch[14] Batch[5] avg_epoch_loss=16.936203\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:39 INFO 140504085288768] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=16.9362030029\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:39 INFO 140504085288768] Epoch[14] Batch [5]#011Speed: 928.49 samples/sec#011loss=16.936203\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:39 INFO 140504085288768] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1082.428216934204, \"sum\": 1082.428216934204, \"min\": 1082.428216934204}}, \"EndTime\": 1564696239.483702, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696238.400812}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:39 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=573.659809829 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:39 INFO 140504085288768] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:39 INFO 140504085288768] #quality_metric: host=algo-1, epoch=14, train loss <loss>=16.9826702118\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:39 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:39 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_651bc077-9419-4648-aa43-971d93a372fe-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.26789093017578, \"sum\": 21.26789093017578, \"min\": 21.26789093017578}}, \"EndTime\": 1564696239.505502, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696239.483762}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:39 INFO 140504085288768] Epoch[15] Batch[0] avg_epoch_loss=16.811979\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:39 INFO 140504085288768] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=16.8119792938\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:40 INFO 140504085288768] Epoch[15] Batch[5] avg_epoch_loss=16.987201\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=16.9872010549\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:40 INFO 140504085288768] Epoch[15] Batch [5]#011Speed: 938.91 samples/sec#011loss=16.987201\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:40 INFO 140504085288768] processed a total of 585 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1041.4259433746338, \"sum\": 1041.4259433746338, \"min\": 1041.4259433746338}}, \"EndTime\": 1564696240.547078, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696239.505585}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:40 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=561.666677349 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:40 INFO 140504085288768] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=15, train loss <loss>=16.9892934799\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:40 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] Epoch[16] Batch[0] avg_epoch_loss=17.026583\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=17.0265827179\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] Epoch[16] Batch[5] avg_epoch_loss=17.000214\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=17.0002142588\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] Epoch[16] Batch [5]#011Speed: 965.13 samples/sec#011loss=17.000214\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] processed a total of 528 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1012.3279094696045, \"sum\": 1012.3279094696045, \"min\": 1012.3279094696045}}, \"EndTime\": 1564696241.559937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696240.547159}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=521.509333085 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=16, train loss <loss>=16.9943233066\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] Epoch[17] Batch[0] avg_epoch_loss=17.023623\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=17.0236225128\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:42 INFO 140504085288768] Epoch[17] Batch[5] avg_epoch_loss=16.979874\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:42 INFO 140504085288768] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=16.9798739751\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:42 INFO 140504085288768] Epoch[17] Batch [5]#011Speed: 951.38 samples/sec#011loss=16.979874\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:42 INFO 140504085288768] processed a total of 582 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1045.7627773284912, \"sum\": 1045.7627773284912, \"min\": 1045.7627773284912}}, \"EndTime\": 1564696242.606226, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696241.560019}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:42 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=556.469658641 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:42 INFO 140504085288768] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:42 INFO 140504085288768] #quality_metric: host=algo-1, epoch=17, train loss <loss>=16.984810257\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:42 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:43 INFO 140504085288768] Epoch[18] Batch[0] avg_epoch_loss=16.989872\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:43 INFO 140504085288768] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=16.9898719788\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:43 INFO 140504085288768] Epoch[18] Batch[5] avg_epoch_loss=16.962274\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:43 INFO 140504085288768] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=16.9622735977\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:43 INFO 140504085288768] Epoch[18] Batch [5]#011Speed: 945.35 samples/sec#011loss=16.962274\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:43 INFO 140504085288768] processed a total of 580 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1044.5530414581299, \"sum\": 1044.5530414581299, \"min\": 1044.5530414581299}}, \"EndTime\": 1564696243.651305, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696242.606305}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:43 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=555.197166384 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:43 INFO 140504085288768] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:43 INFO 140504085288768] #quality_metric: host=algo-1, epoch=18, train loss <loss>=16.9803693771\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:43 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:43 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_3bb01095-9f82-4ffa-acaf-f1a6afb9da9b-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.523880004882812, \"sum\": 22.523880004882812, \"min\": 22.523880004882812}}, \"EndTime\": 1564696243.674468, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696243.651388}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:44 INFO 140504085288768] Epoch[19] Batch[0] avg_epoch_loss=17.037231\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:44 INFO 140504085288768] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=17.0372314453\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:44 INFO 140504085288768] Epoch[19] Batch[5] avg_epoch_loss=16.979582\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:44 INFO 140504085288768] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=16.9795824687\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:44 INFO 140504085288768] Epoch[19] Batch [5]#011Speed: 946.96 samples/sec#011loss=16.979582\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:44 INFO 140504085288768] processed a total of 585 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1062.3161792755127, \"sum\": 1062.3161792755127, \"min\": 1062.3161792755127}}, \"EndTime\": 1564696244.736924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696243.674544}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:44 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=550.622460404 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:44 INFO 140504085288768] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:44 INFO 140504085288768] #quality_metric: host=algo-1, epoch=19, train loss <loss>=16.9902908325\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:44 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:45 INFO 140504085288768] Epoch[20] Batch[0] avg_epoch_loss=16.865746\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=16.8657455444\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:45 INFO 140504085288768] Epoch[20] Batch[5] avg_epoch_loss=16.956366\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=16.9563662211\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:45 INFO 140504085288768] Epoch[20] Batch [5]#011Speed: 891.29 samples/sec#011loss=16.956366\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:50:45 INFO 140504085288768] processed a total of 604 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1112.2829914093018, \"sum\": 1112.2829914093018, \"min\": 1112.2829914093018}}, \"EndTime\": 1564696245.849728, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696244.737006}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:45 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=542.969662314 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:45 INFO 140504085288768] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=20, train loss <loss>=16.9834011078\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:45 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:46 INFO 140504085288768] Epoch[21] Batch[0] avg_epoch_loss=16.998318\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=16.9983177185\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:46 INFO 140504085288768] Epoch[21] Batch[5] avg_epoch_loss=17.006851\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=17.0068508784\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:46 INFO 140504085288768] Epoch[21] Batch [5]#011Speed: 932.15 samples/sec#011loss=17.006851\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:46 INFO 140504085288768] processed a total of 640 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1125.5290508270264, \"sum\": 1125.5290508270264, \"min\": 1125.5290508270264}}, \"EndTime\": 1564696246.975792, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696245.849808}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:46 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=568.560302746 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:46 INFO 140504085288768] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=21, train loss <loss>=16.980153656\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:46 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:46 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_8db0a389-b2bd-4a23-ba87-398ca72d3a2a-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.03400230407715, \"sum\": 21.03400230407715, \"min\": 21.03400230407715}}, \"EndTime\": 1564696246.997449, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696246.975876}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:47 INFO 140504085288768] Epoch[22] Batch[0] avg_epoch_loss=17.031567\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:47 INFO 140504085288768] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=17.0315666199\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:47 INFO 140504085288768] Epoch[22] Batch[5] avg_epoch_loss=16.948432\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:47 INFO 140504085288768] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=16.9484322866\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:47 INFO 140504085288768] Epoch[22] Batch [5]#011Speed: 939.15 samples/sec#011loss=16.948432\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:48 INFO 140504085288768] processed a total of 590 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1103.5540103912354, \"sum\": 1103.5540103912354, \"min\": 1103.5540103912354}}, \"EndTime\": 1564696248.101155, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696246.997519}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:48 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=534.577253026 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:48 INFO 140504085288768] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:48 INFO 140504085288768] #quality_metric: host=algo-1, epoch=22, train loss <loss>=17.0028430939\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:48 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:48 INFO 140504085288768] Epoch[23] Batch[0] avg_epoch_loss=16.933517\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:48 INFO 140504085288768] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=16.9335174561\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:48 INFO 140504085288768] Epoch[23] Batch[5] avg_epoch_loss=17.004307\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:48 INFO 140504085288768] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=17.0043071111\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:48 INFO 140504085288768] Epoch[23] Batch [5]#011Speed: 928.40 samples/sec#011loss=17.004307\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:49 INFO 140504085288768] processed a total of 542 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 989.0789985656738, \"sum\": 989.0789985656738, \"min\": 989.0789985656738}}, \"EndTime\": 1564696249.090851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696248.101238}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:49 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=547.923652174 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:49 INFO 140504085288768] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=23, train loss <loss>=16.9855480194\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:49 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:49 INFO 140504085288768] Epoch[24] Batch[0] avg_epoch_loss=16.752625\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=16.7526245117\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:49 INFO 140504085288768] Epoch[24] Batch[5] avg_epoch_loss=16.925348\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=16.925347964\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:49 INFO 140504085288768] Epoch[24] Batch [5]#011Speed: 946.42 samples/sec#011loss=16.925348\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:50 INFO 140504085288768] processed a total of 583 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1036.5190505981445, \"sum\": 1036.5190505981445, \"min\": 1036.5190505981445}}, \"EndTime\": 1564696250.127928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696249.09092}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:50 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=562.399876539 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:50 INFO 140504085288768] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:50 INFO 140504085288768] #quality_metric: host=algo-1, epoch=24, train loss <loss>=16.9138233185\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:50 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:50 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_fbcda307-2073-4f13-8437-09d837232667-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.8121280670166, \"sum\": 22.8121280670166, \"min\": 22.8121280670166}}, \"EndTime\": 1564696250.151351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696250.128}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:50 INFO 140504085288768] Epoch[25] Batch[0] avg_epoch_loss=16.982141\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:50 INFO 140504085288768] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=16.9821414948\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:50 INFO 140504085288768] Epoch[25] Batch[5] avg_epoch_loss=16.918427\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:50 INFO 140504085288768] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=16.9184271495\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:50 INFO 140504085288768] Epoch[25] Batch [5]#011Speed: 769.54 samples/sec#011loss=16.918427\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:51 INFO 140504085288768] processed a total of 564 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1038.6078357696533, \"sum\": 1038.6078357696533, \"min\": 1038.6078357696533}}, \"EndTime\": 1564696251.190087, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696250.151426}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:51 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=542.985374023 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:51 INFO 140504085288768] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:51 INFO 140504085288768] #quality_metric: host=algo-1, epoch=25, train loss <loss>=16.9400908152\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:51 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:51 INFO 140504085288768] Epoch[26] Batch[0] avg_epoch_loss=16.980652\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:51 INFO 140504085288768] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=16.9806518555\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:51 INFO 140504085288768] Epoch[26] Batch[5] avg_epoch_loss=16.947926\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:51 INFO 140504085288768] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=16.9479255676\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:51 INFO 140504085288768] Epoch[26] Batch [5]#011Speed: 947.04 samples/sec#011loss=16.947926\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:52 INFO 140504085288768] processed a total of 607 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1041.3799285888672, \"sum\": 1041.3799285888672, \"min\": 1041.3799285888672}}, \"EndTime\": 1564696252.23203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696251.190149}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:52 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=582.831610973 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:52 INFO 140504085288768] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:52 INFO 140504085288768] #quality_metric: host=algo-1, epoch=26, train loss <loss>=16.9130599976\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:52 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:52 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_80ad0fc4-89c2-425c-bd2c-e44216258850-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.348880767822266, \"sum\": 22.348880767822266, \"min\": 22.348880767822266}}, \"EndTime\": 1564696252.254955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696252.232088}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:52 INFO 140504085288768] Epoch[27] Batch[0] avg_epoch_loss=16.918495\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:52 INFO 140504085288768] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=16.9184951782\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:53 INFO 140504085288768] Epoch[27] Batch[5] avg_epoch_loss=16.956666\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:53 INFO 140504085288768] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=16.9566659927\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:53 INFO 140504085288768] Epoch[27] Batch [5]#011Speed: 956.18 samples/sec#011loss=16.956666\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:53 INFO 140504085288768] processed a total of 604 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1032.113790512085, \"sum\": 1032.113790512085, \"min\": 1032.113790512085}}, \"EndTime\": 1564696253.2872, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696252.25502}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:53 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=585.14028924 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:53 INFO 140504085288768] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:53 INFO 140504085288768] #quality_metric: host=algo-1, epoch=27, train loss <loss>=16.9448125839\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:53 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:53 INFO 140504085288768] Epoch[28] Batch[0] avg_epoch_loss=16.875050\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:53 INFO 140504085288768] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=16.8750495911\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:54 INFO 140504085288768] Epoch[28] Batch[5] avg_epoch_loss=16.923833\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:54 INFO 140504085288768] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=16.9238328934\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:54 INFO 140504085288768] Epoch[28] Batch [5]#011Speed: 945.70 samples/sec#011loss=16.923833\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:54 INFO 140504085288768] processed a total of 580 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1062.8409385681152, \"sum\": 1062.8409385681152, \"min\": 1062.8409385681152}}, \"EndTime\": 1564696254.350576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696253.28728}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:54 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=545.647145851 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:54 INFO 140504085288768] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:54 INFO 140504085288768] #quality_metric: host=algo-1, epoch=28, train loss <loss>=16.9249610901\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:54 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:54 INFO 140504085288768] Epoch[29] Batch[0] avg_epoch_loss=16.961542\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:54 INFO 140504085288768] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=16.9615421295\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:55 INFO 140504085288768] Epoch[29] Batch[5] avg_epoch_loss=16.946661\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:55 INFO 140504085288768] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=16.9466609955\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:55 INFO 140504085288768] Epoch[29] Batch [5]#011Speed: 935.09 samples/sec#011loss=16.946661\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:55 INFO 140504085288768] processed a total of 615 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1067.2259330749512, \"sum\": 1067.2259330749512, \"min\": 1067.2259330749512}}, \"EndTime\": 1564696255.418326, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696254.350656}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:55 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=576.197671663 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:55 INFO 140504085288768] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:55 INFO 140504085288768] #quality_metric: host=algo-1, epoch=29, train loss <loss>=16.9605195999\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:55 INFO 140504085288768] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:50:55 INFO 140504085288768] Epoch[30] Batch[0] avg_epoch_loss=16.833092\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:55 INFO 140504085288768] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=16.8330917358\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:56 INFO 140504085288768] Epoch[30] Batch[5] avg_epoch_loss=16.908401\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:56 INFO 140504085288768] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=16.9084008535\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:56 INFO 140504085288768] Epoch[30] Batch [5]#011Speed: 917.19 samples/sec#011loss=16.908401\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:56 INFO 140504085288768] processed a total of 560 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 995.6960678100586, \"sum\": 995.6960678100586, \"min\": 995.6960678100586}}, \"EndTime\": 1564696256.414536, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696255.418407}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:56 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=562.356254773 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:56 INFO 140504085288768] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:56 INFO 140504085288768] #quality_metric: host=algo-1, epoch=30, train loss <loss>=16.9231639438\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:56 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:56 INFO 140504085288768] Epoch[31] Batch[0] avg_epoch_loss=17.017878\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:56 INFO 140504085288768] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=17.0178775787\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:57 INFO 140504085288768] Epoch[31] Batch[5] avg_epoch_loss=16.946844\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:57 INFO 140504085288768] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=16.9468437831\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:57 INFO 140504085288768] Epoch[31] Batch [5]#011Speed: 947.94 samples/sec#011loss=16.946844\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:57 INFO 140504085288768] processed a total of 590 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1049.5400428771973, \"sum\": 1049.5400428771973, \"min\": 1049.5400428771973}}, \"EndTime\": 1564696257.464602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696256.414615}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:57 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=562.085128672 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:57 INFO 140504085288768] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:57 INFO 140504085288768] #quality_metric: host=algo-1, epoch=31, train loss <loss>=16.9659887314\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:57 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:57 INFO 140504085288768] Epoch[32] Batch[0] avg_epoch_loss=16.856266\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:57 INFO 140504085288768] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=16.8562660217\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:58 INFO 140504085288768] Epoch[32] Batch[5] avg_epoch_loss=16.890800\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:58 INFO 140504085288768] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=16.8908001582\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:58 INFO 140504085288768] Epoch[32] Batch [5]#011Speed: 957.81 samples/sec#011loss=16.890800\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:58 INFO 140504085288768] processed a total of 558 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1011.6269588470459, \"sum\": 1011.6269588470459, \"min\": 1011.6269588470459}}, \"EndTime\": 1564696258.476761, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696257.464683}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:58 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=551.524072399 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:58 INFO 140504085288768] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:58 INFO 140504085288768] #quality_metric: host=algo-1, epoch=32, train loss <loss>=16.8935086992\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:58 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:58 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_1bc3457b-7210-42f3-8c8b-ae71486e1c2c-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.49710464477539, \"sum\": 23.49710464477539, \"min\": 23.49710464477539}}, \"EndTime\": 1564696258.500824, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696258.476841}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:58 INFO 140504085288768] Epoch[33] Batch[0] avg_epoch_loss=16.956717\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:58 INFO 140504085288768] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=16.9567165375\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:59 INFO 140504085288768] Epoch[33] Batch[5] avg_epoch_loss=16.922119\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:59 INFO 140504085288768] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=16.9221191406\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:59 INFO 140504085288768] Epoch[33] Batch [5]#011Speed: 957.20 samples/sec#011loss=16.922119\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:59 INFO 140504085288768] processed a total of 613 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1094.6202278137207, \"sum\": 1094.6202278137207, \"min\": 1094.6202278137207}}, \"EndTime\": 1564696259.595578, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696258.500893}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:59 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=559.929377392 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:59 INFO 140504085288768] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:59 INFO 140504085288768] #quality_metric: host=algo-1, epoch=33, train loss <loss>=16.8985639572\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:50:59 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:00 INFO 140504085288768] Epoch[34] Batch[0] avg_epoch_loss=16.911718\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:00 INFO 140504085288768] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=16.9117183685\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:00 INFO 140504085288768] Epoch[34] Batch[5] avg_epoch_loss=16.913178\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:00 INFO 140504085288768] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=16.913178126\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:00 INFO 140504085288768] Epoch[34] Batch [5]#011Speed: 948.45 samples/sec#011loss=16.913178\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:00 INFO 140504085288768] processed a total of 518 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 974.2789268493652, \"sum\": 974.2789268493652, \"min\": 974.2789268493652}}, \"EndTime\": 1564696260.570436, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696259.595701}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:00 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=531.611903275 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:00 INFO 140504085288768] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:00 INFO 140504085288768] #quality_metric: host=algo-1, epoch=34, train loss <loss>=16.9309753842\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:00 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:01 INFO 140504085288768] Epoch[35] Batch[0] avg_epoch_loss=16.949163\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:01 INFO 140504085288768] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=16.9491634369\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:01 INFO 140504085288768] Epoch[35] Batch[5] avg_epoch_loss=16.920804\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:01 INFO 140504085288768] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=16.9208040237\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:01 INFO 140504085288768] Epoch[35] Batch [5]#011Speed: 943.62 samples/sec#011loss=16.920804\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:01 INFO 140504085288768] processed a total of 584 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1085.8209133148193, \"sum\": 1085.8209133148193, \"min\": 1085.8209133148193}}, \"EndTime\": 1564696261.656793, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696260.570515}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:01 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=537.787950961 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:01 INFO 140504085288768] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:01 INFO 140504085288768] #quality_metric: host=algo-1, epoch=35, train loss <loss>=16.9016202927\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:01 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:02 INFO 140504085288768] Epoch[36] Batch[0] avg_epoch_loss=16.834526\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:02 INFO 140504085288768] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=16.834526062\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:02 INFO 140504085288768] Epoch[36] Batch[5] avg_epoch_loss=16.898213\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:02 INFO 140504085288768] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=16.8982130686\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:02 INFO 140504085288768] Epoch[36] Batch [5]#011Speed: 947.17 samples/sec#011loss=16.898213\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:02 INFO 140504085288768] processed a total of 617 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1029.6640396118164, \"sum\": 1029.6640396118164, \"min\": 1029.6640396118164}}, \"EndTime\": 1564696262.687005, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696261.656864}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:02 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=599.154795059 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:02 INFO 140504085288768] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:02 INFO 140504085288768] #quality_metric: host=algo-1, epoch=36, train loss <loss>=16.8862604141\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:02 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:02 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_c875370c-21db-48fc-a5ff-c76dbac86a82-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.21004867553711, \"sum\": 23.21004867553711, \"min\": 23.21004867553711}}, \"EndTime\": 1564696262.710842, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696262.687087}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:03 INFO 140504085288768] Epoch[37] Batch[0] avg_epoch_loss=16.951530\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:03 INFO 140504085288768] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=16.9515304565\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:03 INFO 140504085288768] Epoch[37] Batch[5] avg_epoch_loss=16.916027\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:03 INFO 140504085288768] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=16.916027387\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:03 INFO 140504085288768] Epoch[37] Batch [5]#011Speed: 955.71 samples/sec#011loss=16.916027\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:03 INFO 140504085288768] processed a total of 552 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 951.2209892272949, \"sum\": 951.2209892272949, \"min\": 951.2209892272949}}, \"EndTime\": 1564696263.662195, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696262.710913}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:03 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=580.253706509 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:03 INFO 140504085288768] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:03 INFO 140504085288768] #quality_metric: host=algo-1, epoch=37, train loss <loss>=16.9210542043\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:03 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:04 INFO 140504085288768] Epoch[38] Batch[0] avg_epoch_loss=16.768877\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:04 INFO 140504085288768] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=16.7688770294\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:04 INFO 140504085288768] Epoch[38] Batch[5] avg_epoch_loss=16.876915\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:04 INFO 140504085288768] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=16.876914978\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:04 INFO 140504085288768] Epoch[38] Batch [5]#011Speed: 956.40 samples/sec#011loss=16.876915\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:04 INFO 140504085288768] processed a total of 610 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1025.2609252929688, \"sum\": 1025.2609252929688, \"min\": 1025.2609252929688}}, \"EndTime\": 1564696264.687932, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696263.662252}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:04 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=594.908932288 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:04 INFO 140504085288768] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:04 INFO 140504085288768] #quality_metric: host=algo-1, epoch=38, train loss <loss>=16.875658226\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:04 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:04 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_4499ec64-be89-4ff1-8990-c60be1fdf5a5-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.66407012939453, \"sum\": 22.66407012939453, \"min\": 22.66407012939453}}, \"EndTime\": 1564696264.711142, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696264.688006}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:05 INFO 140504085288768] Epoch[39] Batch[0] avg_epoch_loss=16.868359\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:05 INFO 140504085288768] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=16.8683586121\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:05 INFO 140504085288768] Epoch[39] Batch[5] avg_epoch_loss=16.902189\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:05 INFO 140504085288768] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=16.902188619\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:05 INFO 140504085288768] Epoch[39] Batch [5]#011Speed: 914.00 samples/sec#011loss=16.902189\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:51:05 INFO 140504085288768] processed a total of 585 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1042.0000553131104, \"sum\": 1042.0000553131104, \"min\": 1042.0000553131104}}, \"EndTime\": 1564696265.753263, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696264.71121}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:05 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=561.366497029 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:05 INFO 140504085288768] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:05 INFO 140504085288768] #quality_metric: host=algo-1, epoch=39, train loss <loss>=16.8461891174\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:05 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:05 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_2315d263-b5a8-461a-a5cc-af5353c5005a-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.745920181274414, \"sum\": 21.745920181274414, \"min\": 21.745920181274414}}, \"EndTime\": 1564696265.775632, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696265.753331}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:06 INFO 140504085288768] Epoch[40] Batch[0] avg_epoch_loss=16.905012\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:06 INFO 140504085288768] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=16.9050121307\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:06 INFO 140504085288768] Epoch[40] Batch[5] avg_epoch_loss=16.891992\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:06 INFO 140504085288768] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=16.8919919332\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:06 INFO 140504085288768] Epoch[40] Batch [5]#011Speed: 922.87 samples/sec#011loss=16.891992\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:06 INFO 140504085288768] processed a total of 579 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1071.0389614105225, \"sum\": 1071.0389614105225, \"min\": 1071.0389614105225}}, \"EndTime\": 1564696266.846795, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696265.775697}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:06 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=540.538582982 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:06 INFO 140504085288768] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:06 INFO 140504085288768] #quality_metric: host=algo-1, epoch=40, train loss <loss>=16.8929714203\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:06 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:07 INFO 140504085288768] Epoch[41] Batch[0] avg_epoch_loss=16.927790\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:07 INFO 140504085288768] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=16.9277896881\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:07 INFO 140504085288768] Epoch[41] Batch[5] avg_epoch_loss=16.942901\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:07 INFO 140504085288768] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=16.9429012934\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:07 INFO 140504085288768] Epoch[41] Batch [5]#011Speed: 932.64 samples/sec#011loss=16.942901\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:07 INFO 140504085288768] processed a total of 576 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 996.9639778137207, \"sum\": 996.9639778137207, \"min\": 996.9639778137207}}, \"EndTime\": 1564696267.844292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696266.846875}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:07 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=577.690248186 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:07 INFO 140504085288768] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:07 INFO 140504085288768] #quality_metric: host=algo-1, epoch=41, train loss <loss>=16.9438667297\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:07 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:08 INFO 140504085288768] Epoch[42] Batch[0] avg_epoch_loss=16.835058\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:08 INFO 140504085288768] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=16.8350582123\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:08 INFO 140504085288768] Epoch[42] Batch[5] avg_epoch_loss=16.809084\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:08 INFO 140504085288768] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=16.8090839386\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:08 INFO 140504085288768] Epoch[42] Batch [5]#011Speed: 956.79 samples/sec#011loss=16.809084\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:08 INFO 140504085288768] processed a total of 576 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 966.6271209716797, \"sum\": 966.6271209716797, \"min\": 966.6271209716797}}, \"EndTime\": 1564696268.811469, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696267.844366}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:08 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=595.813849289 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:08 INFO 140504085288768] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:08 INFO 140504085288768] #quality_metric: host=algo-1, epoch=42, train loss <loss>=16.8464828067\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:08 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:09 INFO 140504085288768] Epoch[43] Batch[0] avg_epoch_loss=16.909653\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:09 INFO 140504085288768] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=16.90965271\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:09 INFO 140504085288768] Epoch[43] Batch[5] avg_epoch_loss=16.902650\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:09 INFO 140504085288768] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=16.9026498795\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:09 INFO 140504085288768] Epoch[43] Batch [5]#011Speed: 956.53 samples/sec#011loss=16.902650\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:09 INFO 140504085288768] processed a total of 567 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 973.7551212310791, \"sum\": 973.7551212310791, \"min\": 973.7551212310791}}, \"EndTime\": 1564696269.785775, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696268.811549}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:09 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=582.212068161 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:09 INFO 140504085288768] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:09 INFO 140504085288768] #quality_metric: host=algo-1, epoch=43, train loss <loss>=16.9009075165\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:09 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:10 INFO 140504085288768] Epoch[44] Batch[0] avg_epoch_loss=16.813025\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:10 INFO 140504085288768] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=16.8130245209\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:10 INFO 140504085288768] Epoch[44] Batch[5] avg_epoch_loss=16.888387\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:10 INFO 140504085288768] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=16.8883873622\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:10 INFO 140504085288768] Epoch[44] Batch [5]#011Speed: 926.26 samples/sec#011loss=16.888387\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:10 INFO 140504085288768] processed a total of 589 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1100.3601551055908, \"sum\": 1100.3601551055908, \"min\": 1100.3601551055908}}, \"EndTime\": 1564696270.886665, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696269.785856}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:10 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=535.222463161 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:10 INFO 140504085288768] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:10 INFO 140504085288768] #quality_metric: host=algo-1, epoch=44, train loss <loss>=16.8662794113\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:10 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:11 INFO 140504085288768] Epoch[45] Batch[0] avg_epoch_loss=16.935825\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:11 INFO 140504085288768] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=16.9358253479\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:11 INFO 140504085288768] Epoch[45] Batch[5] avg_epoch_loss=16.873812\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:11 INFO 140504085288768] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=16.8738117218\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:11 INFO 140504085288768] Epoch[45] Batch [5]#011Speed: 950.28 samples/sec#011loss=16.873812\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:11 INFO 140504085288768] processed a total of 571 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1008.6250305175781, \"sum\": 1008.6250305175781, \"min\": 1008.6250305175781}}, \"EndTime\": 1564696271.895821, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696270.886745}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:11 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=566.050989193 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:11 INFO 140504085288768] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:11 INFO 140504085288768] #quality_metric: host=algo-1, epoch=45, train loss <loss>=16.8712558746\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:11 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:12 INFO 140504085288768] Epoch[46] Batch[0] avg_epoch_loss=16.724356\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:12 INFO 140504085288768] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=16.7243556976\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:12 INFO 140504085288768] Epoch[46] Batch[5] avg_epoch_loss=16.807561\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:12 INFO 140504085288768] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=16.8075612386\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:12 INFO 140504085288768] Epoch[46] Batch [5]#011Speed: 957.54 samples/sec#011loss=16.807561\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:12 INFO 140504085288768] processed a total of 631 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1085.9849452972412, \"sum\": 1085.9849452972412, \"min\": 1085.9849452972412}}, \"EndTime\": 1564696272.982356, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696271.895903}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:12 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=580.977373631 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:12 INFO 140504085288768] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:12 INFO 140504085288768] #quality_metric: host=algo-1, epoch=46, train loss <loss>=16.8365097046\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:12 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:13 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_f9f5e0b5-16e9-4ce8-85c5-91ab2dca5ab5-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.775070190429688, \"sum\": 30.775070190429688, \"min\": 30.775070190429688}}, \"EndTime\": 1564696273.013713, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696272.982436}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:13 INFO 140504085288768] Epoch[47] Batch[0] avg_epoch_loss=16.892014\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:13 INFO 140504085288768] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=16.8920135498\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:13 INFO 140504085288768] Epoch[47] Batch[5] avg_epoch_loss=16.893093\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:13 INFO 140504085288768] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=16.893093427\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:13 INFO 140504085288768] Epoch[47] Batch [5]#011Speed: 949.53 samples/sec#011loss=16.893093\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:13 INFO 140504085288768] processed a total of 570 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 985.3811264038086, \"sum\": 985.3811264038086, \"min\": 985.3811264038086}}, \"EndTime\": 1564696273.999227, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696273.013777}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:13 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=578.394244783 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:13 INFO 140504085288768] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:13 INFO 140504085288768] #quality_metric: host=algo-1, epoch=47, train loss <loss>=16.9022468991\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:13 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:14 INFO 140504085288768] Epoch[48] Batch[0] avg_epoch_loss=16.902882\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:14 INFO 140504085288768] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=16.9028816223\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:14 INFO 140504085288768] Epoch[48] Batch[5] avg_epoch_loss=16.866367\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:14 INFO 140504085288768] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=16.8663673401\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:14 INFO 140504085288768] Epoch[48] Batch [5]#011Speed: 942.57 samples/sec#011loss=16.866367\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:15 INFO 140504085288768] processed a total of 586 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1048.4721660614014, \"sum\": 1048.4721660614014, \"min\": 1048.4721660614014}}, \"EndTime\": 1564696275.048245, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696273.999293}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:15 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=558.848385797 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:15 INFO 140504085288768] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:15 INFO 140504085288768] #quality_metric: host=algo-1, epoch=48, train loss <loss>=16.8549869537\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:15 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:15 INFO 140504085288768] Epoch[49] Batch[0] avg_epoch_loss=16.738253\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:15 INFO 140504085288768] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=16.7382526398\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:51:15 INFO 140504085288768] Epoch[49] Batch[5] avg_epoch_loss=16.792617\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:15 INFO 140504085288768] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=16.7926171621\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:15 INFO 140504085288768] Epoch[49] Batch [5]#011Speed: 897.71 samples/sec#011loss=16.792617\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:16 INFO 140504085288768] processed a total of 548 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1018.1059837341309, \"sum\": 1018.1059837341309, \"min\": 1018.1059837341309}}, \"EndTime\": 1564696276.06691, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696275.048321}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:16 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=538.193626941 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:16 INFO 140504085288768] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:16 INFO 140504085288768] #quality_metric: host=algo-1, epoch=49, train loss <loss>=16.8388370938\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:16 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:16 INFO 140504085288768] Epoch[50] Batch[0] avg_epoch_loss=16.855742\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:16 INFO 140504085288768] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=16.8557415009\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:16 INFO 140504085288768] Epoch[50] Batch[5] avg_epoch_loss=16.879806\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:16 INFO 140504085288768] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=16.8798055649\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:16 INFO 140504085288768] Epoch[50] Batch [5]#011Speed: 930.12 samples/sec#011loss=16.879806\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:17 INFO 140504085288768] processed a total of 588 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1136.1699104309082, \"sum\": 1136.1699104309082, \"min\": 1136.1699104309082}}, \"EndTime\": 1564696277.203607, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696276.066989}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:17 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=517.475778994 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:17 INFO 140504085288768] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:17 INFO 140504085288768] #quality_metric: host=algo-1, epoch=50, train loss <loss>=16.8615922928\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:17 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:17 INFO 140504085288768] Epoch[51] Batch[0] avg_epoch_loss=16.916800\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:17 INFO 140504085288768] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=16.9167995453\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:17 INFO 140504085288768] Epoch[51] Batch[5] avg_epoch_loss=16.859252\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:17 INFO 140504085288768] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=16.8592516581\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:17 INFO 140504085288768] Epoch[51] Batch [5]#011Speed: 951.34 samples/sec#011loss=16.859252\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:18 INFO 140504085288768] processed a total of 545 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 996.8681335449219, \"sum\": 996.8681335449219, \"min\": 996.8681335449219}}, \"EndTime\": 1564696278.200997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696277.203686}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:18 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=546.649212631 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:18 INFO 140504085288768] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:18 INFO 140504085288768] #quality_metric: host=algo-1, epoch=51, train loss <loss>=16.8661833869\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:18 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:18 INFO 140504085288768] Epoch[52] Batch[0] avg_epoch_loss=16.894760\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:18 INFO 140504085288768] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=16.8947601318\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:19 INFO 140504085288768] Epoch[52] Batch[5] avg_epoch_loss=16.841590\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:19 INFO 140504085288768] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=16.8415902456\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:19 INFO 140504085288768] Epoch[52] Batch [5]#011Speed: 942.44 samples/sec#011loss=16.841590\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:19 INFO 140504085288768] processed a total of 587 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1080.543041229248, \"sum\": 1080.543041229248, \"min\": 1080.543041229248}}, \"EndTime\": 1564696279.282064, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696278.201076}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:19 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=543.187597074 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:19 INFO 140504085288768] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:19 INFO 140504085288768] #quality_metric: host=algo-1, epoch=52, train loss <loss>=16.8615032196\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:19 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:19 INFO 140504085288768] Epoch[53] Batch[0] avg_epoch_loss=16.911257\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:19 INFO 140504085288768] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=16.9112567902\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:20 INFO 140504085288768] Epoch[53] Batch[5] avg_epoch_loss=16.856640\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:20 INFO 140504085288768] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=16.85664018\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:20 INFO 140504085288768] Epoch[53] Batch [5]#011Speed: 945.74 samples/sec#011loss=16.856640\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:20 INFO 140504085288768] processed a total of 601 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1074.1100311279297, \"sum\": 1074.1100311279297, \"min\": 1074.1100311279297}}, \"EndTime\": 1564696280.356706, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696279.282144}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:20 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=559.468913322 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:20 INFO 140504085288768] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:20 INFO 140504085288768] #quality_metric: host=algo-1, epoch=53, train loss <loss>=16.8774673462\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:20 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:20 INFO 140504085288768] Epoch[54] Batch[0] avg_epoch_loss=16.867060\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:20 INFO 140504085288768] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=16.8670597076\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:21 INFO 140504085288768] Epoch[54] Batch[5] avg_epoch_loss=16.885924\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:21 INFO 140504085288768] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=16.8859237035\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:21 INFO 140504085288768] Epoch[54] Batch [5]#011Speed: 895.63 samples/sec#011loss=16.885924\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:21 INFO 140504085288768] processed a total of 591 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1091.6199684143066, \"sum\": 1091.6199684143066, \"min\": 1091.6199684143066}}, \"EndTime\": 1564696281.448901, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696280.356791}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:21 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=541.345063919 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:21 INFO 140504085288768] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:21 INFO 140504085288768] #quality_metric: host=algo-1, epoch=54, train loss <loss>=16.8491891861\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:21 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:21 INFO 140504085288768] Epoch[55] Batch[0] avg_epoch_loss=16.866222\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:21 INFO 140504085288768] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=16.8662223816\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:22 INFO 140504085288768] Epoch[55] Batch[5] avg_epoch_loss=16.863859\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:22 INFO 140504085288768] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=16.8638585409\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:22 INFO 140504085288768] Epoch[55] Batch [5]#011Speed: 902.73 samples/sec#011loss=16.863859\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:22 INFO 140504085288768] processed a total of 593 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1042.470932006836, \"sum\": 1042.470932006836, \"min\": 1042.470932006836}}, \"EndTime\": 1564696282.491934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696281.44897}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:22 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=568.777450893 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:22 INFO 140504085288768] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:22 INFO 140504085288768] #quality_metric: host=algo-1, epoch=55, train loss <loss>=16.8743570328\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:22 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:22 INFO 140504085288768] Epoch[56] Batch[0] avg_epoch_loss=16.894606\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:22 INFO 140504085288768] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=16.8946056366\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:23 INFO 140504085288768] Epoch[56] Batch[5] avg_epoch_loss=16.885293\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:23 INFO 140504085288768] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=16.8852930069\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:23 INFO 140504085288768] Epoch[56] Batch [5]#011Speed: 961.93 samples/sec#011loss=16.885293\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:23 INFO 140504085288768] processed a total of 583 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1037.0450019836426, \"sum\": 1037.0450019836426, \"min\": 1037.0450019836426}}, \"EndTime\": 1564696283.529504, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696282.492013}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:23 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=562.090773845 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:23 INFO 140504085288768] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:23 INFO 140504085288768] #quality_metric: host=algo-1, epoch=56, train loss <loss>=16.8993642807\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:23 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:23 INFO 140504085288768] Epoch[57] Batch[0] avg_epoch_loss=16.900011\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:23 INFO 140504085288768] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=16.9000110626\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:24 INFO 140504085288768] Epoch[57] Batch[5] avg_epoch_loss=16.858884\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:24 INFO 140504085288768] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=16.8588835398\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:24 INFO 140504085288768] Epoch[57] Batch [5]#011Speed: 946.32 samples/sec#011loss=16.858884\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:24 INFO 140504085288768] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1068.7329769134521, \"sum\": 1068.7329769134521, \"min\": 1068.7329769134521}}, \"EndTime\": 1564696284.598803, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696283.529621}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:24 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=558.544208649 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:24 INFO 140504085288768] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:24 INFO 140504085288768] #quality_metric: host=algo-1, epoch=57, train loss <loss>=16.8012701035\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:24 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:24 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_b8647afc-e9d8-46e3-93cb-c0611d1865cb-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.75895118713379, \"sum\": 32.75895118713379, \"min\": 32.75895118713379}}, \"EndTime\": 1564696284.632141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696284.598884}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:25 INFO 140504085288768] Epoch[58] Batch[0] avg_epoch_loss=16.877924\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:25 INFO 140504085288768] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=16.8779239655\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:25 INFO 140504085288768] Epoch[58] Batch[5] avg_epoch_loss=16.885279\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:25 INFO 140504085288768] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=16.8852790197\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:25 INFO 140504085288768] Epoch[58] Batch [5]#011Speed: 960.95 samples/sec#011loss=16.885279\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:51:25 INFO 140504085288768] processed a total of 591 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1050.0168800354004, \"sum\": 1050.0168800354004, \"min\": 1050.0168800354004}}, \"EndTime\": 1564696285.682297, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696284.632216}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:25 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=562.785350979 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:25 INFO 140504085288768] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:25 INFO 140504085288768] #quality_metric: host=algo-1, epoch=58, train loss <loss>=16.85241642\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:25 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:26 INFO 140504085288768] Epoch[59] Batch[0] avg_epoch_loss=16.943478\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=16.9434776306\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:26 INFO 140504085288768] Epoch[59] Batch[5] avg_epoch_loss=16.906583\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=16.9065834681\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:26 INFO 140504085288768] Epoch[59] Batch [5]#011Speed: 951.19 samples/sec#011loss=16.906583\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:26 INFO 140504085288768] processed a total of 610 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1111.0789775848389, \"sum\": 1111.0789775848389, \"min\": 1111.0789775848389}}, \"EndTime\": 1564696286.793913, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696285.682378}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:26 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=548.957568158 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:26 INFO 140504085288768] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=59, train loss <loss>=16.8541873932\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:26 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:27 INFO 140504085288768] Epoch[60] Batch[0] avg_epoch_loss=16.763334\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=16.7633342743\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:27 INFO 140504085288768] Epoch[60] Batch[5] avg_epoch_loss=16.775553\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=16.7755527496\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:27 INFO 140504085288768] Epoch[60] Batch [5]#011Speed: 916.84 samples/sec#011loss=16.775553\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:27 INFO 140504085288768] processed a total of 573 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 984.3530654907227, \"sum\": 984.3530654907227, \"min\": 984.3530654907227}}, \"EndTime\": 1564696287.778782, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696286.793994}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:27 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=582.041386726 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:27 INFO 140504085288768] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=60, train loss <loss>=16.8153555128\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:27 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] Epoch[61] Batch[0] avg_epoch_loss=16.844072\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=16.8440723419\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] Epoch[61] Batch[5] avg_epoch_loss=16.841912\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=16.8419116338\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] Epoch[61] Batch [5]#011Speed: 944.30 samples/sec#011loss=16.841912\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] Epoch[61] Batch[10] avg_epoch_loss=16.845729\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=16.8503108978\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] Epoch[61] Batch [10]#011Speed: 919.78 samples/sec#011loss=16.850311\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] processed a total of 644 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1127.9020309448242, \"sum\": 1127.9020309448242, \"min\": 1127.9020309448242}}, \"EndTime\": 1564696288.907263, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696287.77886}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=570.913890264 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=61, train loss <loss>=16.8457294811\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:28 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:29 INFO 140504085288768] Epoch[62] Batch[0] avg_epoch_loss=16.854609\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:29 INFO 140504085288768] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=16.8546085358\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:29 INFO 140504085288768] Epoch[62] Batch[5] avg_epoch_loss=16.815783\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:29 INFO 140504085288768] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=16.8157831828\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:29 INFO 140504085288768] Epoch[62] Batch [5]#011Speed: 941.66 samples/sec#011loss=16.815783\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:29 INFO 140504085288768] processed a total of 605 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1066.1571025848389, \"sum\": 1066.1571025848389, \"min\": 1066.1571025848389}}, \"EndTime\": 1564696289.973914, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696288.907341}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:29 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=567.406941649 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:29 INFO 140504085288768] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:29 INFO 140504085288768] #quality_metric: host=algo-1, epoch=62, train loss <loss>=16.8031759262\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:29 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:30 INFO 140504085288768] Epoch[63] Batch[0] avg_epoch_loss=16.851604\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=16.8516044617\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:30 INFO 140504085288768] Epoch[63] Batch[5] avg_epoch_loss=16.834277\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=16.8342768351\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:30 INFO 140504085288768] Epoch[63] Batch [5]#011Speed: 948.31 samples/sec#011loss=16.834277\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:30 INFO 140504085288768] processed a total of 554 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 964.555025100708, \"sum\": 964.555025100708, \"min\": 964.555025100708}}, \"EndTime\": 1564696290.939043, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696289.973977}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:30 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=574.286139099 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:30 INFO 140504085288768] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=63, train loss <loss>=16.8615960015\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:30 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:31 INFO 140504085288768] Epoch[64] Batch[0] avg_epoch_loss=16.876631\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=16.8766307831\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:31 INFO 140504085288768] Epoch[64] Batch[5] avg_epoch_loss=16.855761\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=16.8557608922\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:31 INFO 140504085288768] Epoch[64] Batch [5]#011Speed: 942.83 samples/sec#011loss=16.855761\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:31 INFO 140504085288768] processed a total of 599 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1052.361011505127, \"sum\": 1052.361011505127, \"min\": 1052.361011505127}}, \"EndTime\": 1564696291.992024, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696290.939125}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:31 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=569.131964532 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:31 INFO 140504085288768] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=64, train loss <loss>=16.8300020218\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:31 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:32 INFO 140504085288768] Epoch[65] Batch[0] avg_epoch_loss=16.867641\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:32 INFO 140504085288768] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=16.867641449\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:32 INFO 140504085288768] Epoch[65] Batch[5] avg_epoch_loss=16.826934\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:32 INFO 140504085288768] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=16.8269338608\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:32 INFO 140504085288768] Epoch[65] Batch [5]#011Speed: 942.55 samples/sec#011loss=16.826934\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:32 INFO 140504085288768] processed a total of 572 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 974.7180938720703, \"sum\": 974.7180938720703, \"min\": 974.7180938720703}}, \"EndTime\": 1564696292.967276, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696291.992106}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:32 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=586.767162352 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:32 INFO 140504085288768] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:32 INFO 140504085288768] #quality_metric: host=algo-1, epoch=65, train loss <loss>=16.8242471483\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:32 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:33 INFO 140504085288768] Epoch[66] Batch[0] avg_epoch_loss=16.662834\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=16.6628341675\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:33 INFO 140504085288768] Epoch[66] Batch[5] avg_epoch_loss=16.809470\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=16.8094698588\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:33 INFO 140504085288768] Epoch[66] Batch [5]#011Speed: 950.39 samples/sec#011loss=16.809470\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:33 INFO 140504085288768] processed a total of 576 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 979.9680709838867, \"sum\": 979.9680709838867, \"min\": 979.9680709838867}}, \"EndTime\": 1564696293.947773, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696292.967355}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:33 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=587.704761882 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:33 INFO 140504085288768] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=66, train loss <loss>=16.8144279056\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:33 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:34 INFO 140504085288768] Epoch[67] Batch[0] avg_epoch_loss=16.924866\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=16.9248657227\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:34 INFO 140504085288768] Epoch[67] Batch[5] avg_epoch_loss=16.857793\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=16.8577931722\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:34 INFO 140504085288768] Epoch[67] Batch [5]#011Speed: 957.33 samples/sec#011loss=16.857793\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:34 INFO 140504085288768] processed a total of 578 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1051.0339736938477, \"sum\": 1051.0339736938477, \"min\": 1051.0339736938477}}, \"EndTime\": 1564696294.999336, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696293.947853}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:34 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=549.872906115 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:34 INFO 140504085288768] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=67, train loss <loss>=16.8335792542\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:34 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:35 INFO 140504085288768] Epoch[68] Batch[0] avg_epoch_loss=16.846020\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:35 INFO 140504085288768] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=16.8460197449\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:51:35 INFO 140504085288768] Epoch[68] Batch[5] avg_epoch_loss=16.884425\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:35 INFO 140504085288768] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=16.8844245275\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:35 INFO 140504085288768] Epoch[68] Batch [5]#011Speed: 951.33 samples/sec#011loss=16.884425\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:36 INFO 140504085288768] processed a total of 589 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1070.3129768371582, \"sum\": 1070.3129768371582, \"min\": 1070.3129768371582}}, \"EndTime\": 1564696296.070188, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696294.999418}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:36 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=550.252882739 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:36 INFO 140504085288768] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=68, train loss <loss>=16.8279920578\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:36 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:36 INFO 140504085288768] Epoch[69] Batch[0] avg_epoch_loss=16.943058\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=16.9430580139\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:36 INFO 140504085288768] Epoch[69] Batch[5] avg_epoch_loss=16.789255\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=16.7892545064\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:36 INFO 140504085288768] Epoch[69] Batch [5]#011Speed: 878.88 samples/sec#011loss=16.789255\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:37 INFO 140504085288768] processed a total of 587 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1065.2658939361572, \"sum\": 1065.2658939361572, \"min\": 1065.2658939361572}}, \"EndTime\": 1564696297.136006, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696296.070261}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:37 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=550.977682782 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:37 INFO 140504085288768] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=69, train loss <loss>=16.7973913193\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:37 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:37 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_e7070a26-e7b1-4897-b063-f0f41744cd4c-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 28.43308448791504, \"sum\": 28.43308448791504, \"min\": 28.43308448791504}}, \"EndTime\": 1564696297.165021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696297.136085}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:37 INFO 140504085288768] Epoch[70] Batch[0] avg_epoch_loss=16.848740\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=16.848739624\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:37 INFO 140504085288768] Epoch[70] Batch[5] avg_epoch_loss=16.839590\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=16.8395903905\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:37 INFO 140504085288768] Epoch[70] Batch [5]#011Speed: 949.22 samples/sec#011loss=16.839590\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:38 INFO 140504085288768] processed a total of 615 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1049.7729778289795, \"sum\": 1049.7729778289795, \"min\": 1049.7729778289795}}, \"EndTime\": 1564696298.214933, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696297.165092}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:38 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=585.775097523 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:38 INFO 140504085288768] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:38 INFO 140504085288768] #quality_metric: host=algo-1, epoch=70, train loss <loss>=16.8410402298\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:38 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:38 INFO 140504085288768] Epoch[71] Batch[0] avg_epoch_loss=16.680164\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:38 INFO 140504085288768] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=16.6801643372\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:38 INFO 140504085288768] Epoch[71] Batch[5] avg_epoch_loss=16.815645\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:38 INFO 140504085288768] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=16.8156452179\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:38 INFO 140504085288768] Epoch[71] Batch [5]#011Speed: 952.26 samples/sec#011loss=16.815645\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:39 INFO 140504085288768] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1039.5901203155518, \"sum\": 1039.5901203155518, \"min\": 1039.5901203155518}}, \"EndTime\": 1564696299.255047, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696298.215012}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:39 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=574.200788242 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:39 INFO 140504085288768] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:39 INFO 140504085288768] #quality_metric: host=algo-1, epoch=71, train loss <loss>=16.8200521469\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:39 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:39 INFO 140504085288768] Epoch[72] Batch[0] avg_epoch_loss=16.795626\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:39 INFO 140504085288768] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=16.7956256866\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] Epoch[72] Batch[5] avg_epoch_loss=16.800727\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=16.8007268906\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] Epoch[72] Batch [5]#011Speed: 949.22 samples/sec#011loss=16.800727\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] processed a total of 571 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 982.9480648040771, \"sum\": 982.9480648040771, \"min\": 982.9480648040771}}, \"EndTime\": 1564696300.238525, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696299.255126}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=580.849490721 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=72, train loss <loss>=16.802394655\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] Epoch[73] Batch[0] avg_epoch_loss=16.917381\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=16.9173812866\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] Epoch[73] Batch[5] avg_epoch_loss=16.856006\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=16.8560056686\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:40 INFO 140504085288768] Epoch[73] Batch [5]#011Speed: 929.63 samples/sec#011loss=16.856006\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:41 INFO 140504085288768] processed a total of 561 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 967.4761295318604, \"sum\": 967.4761295318604, \"min\": 967.4761295318604}}, \"EndTime\": 1564696301.206482, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696300.238585}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:41 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=579.808400922 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:41 INFO 140504085288768] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=73, train loss <loss>=16.8449516296\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:41 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:41 INFO 140504085288768] Epoch[74] Batch[0] avg_epoch_loss=16.872877\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=16.872877121\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:41 INFO 140504085288768] Epoch[74] Batch[5] avg_epoch_loss=16.849403\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=16.8494033813\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:41 INFO 140504085288768] Epoch[74] Batch [5]#011Speed: 939.62 samples/sec#011loss=16.849403\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:42 INFO 140504085288768] processed a total of 598 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1033.8001251220703, \"sum\": 1033.8001251220703, \"min\": 1033.8001251220703}}, \"EndTime\": 1564696302.240781, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696301.206539}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:42 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=578.386346525 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:42 INFO 140504085288768] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:42 INFO 140504085288768] #quality_metric: host=algo-1, epoch=74, train loss <loss>=16.8043384552\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:42 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:42 INFO 140504085288768] Epoch[75] Batch[0] avg_epoch_loss=16.756001\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:42 INFO 140504085288768] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=16.7560005188\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:42 INFO 140504085288768] Epoch[75] Batch[5] avg_epoch_loss=16.805070\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:42 INFO 140504085288768] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=16.8050702413\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:42 INFO 140504085288768] Epoch[75] Batch [5]#011Speed: 948.26 samples/sec#011loss=16.805070\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:43 INFO 140504085288768] processed a total of 586 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1027.8339385986328, \"sum\": 1027.8339385986328, \"min\": 1027.8339385986328}}, \"EndTime\": 1564696303.269159, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696302.240854}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:43 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=570.076659432 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:43 INFO 140504085288768] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:43 INFO 140504085288768] #quality_metric: host=algo-1, epoch=75, train loss <loss>=16.8446973801\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:43 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:43 INFO 140504085288768] Epoch[76] Batch[0] avg_epoch_loss=16.712282\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:43 INFO 140504085288768] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=16.7122821808\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:44 INFO 140504085288768] Epoch[76] Batch[5] avg_epoch_loss=16.822689\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:44 INFO 140504085288768] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=16.8226893743\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:44 INFO 140504085288768] Epoch[76] Batch [5]#011Speed: 947.35 samples/sec#011loss=16.822689\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:44 INFO 140504085288768] processed a total of 594 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1025.3241062164307, \"sum\": 1025.3241062164307, \"min\": 1025.3241062164307}}, \"EndTime\": 1564696304.295047, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696303.269223}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:44 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=579.267992785 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:44 INFO 140504085288768] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:44 INFO 140504085288768] #quality_metric: host=algo-1, epoch=76, train loss <loss>=16.8240520477\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:44 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:44 INFO 140504085288768] Epoch[77] Batch[0] avg_epoch_loss=16.743896\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:44 INFO 140504085288768] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=16.7438964844\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:45 INFO 140504085288768] Epoch[77] Batch[5] avg_epoch_loss=16.795389\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=16.7953891754\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:45 INFO 140504085288768] Epoch[77] Batch [5]#011Speed: 950.27 samples/sec#011loss=16.795389\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:45 INFO 140504085288768] processed a total of 598 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1028.3629894256592, \"sum\": 1028.3629894256592, \"min\": 1028.3629894256592}}, \"EndTime\": 1564696305.323946, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696304.295117}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:45 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=581.440542562 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:45 INFO 140504085288768] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=77, train loss <loss>=16.8500221252\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:45 INFO 140504085288768] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:51:45 INFO 140504085288768] Epoch[78] Batch[0] avg_epoch_loss=16.802319\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=16.802318573\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:46 INFO 140504085288768] Epoch[78] Batch[5] avg_epoch_loss=16.800374\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=16.800374349\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:46 INFO 140504085288768] Epoch[78] Batch [5]#011Speed: 944.13 samples/sec#011loss=16.800374\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:46 INFO 140504085288768] processed a total of 611 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1085.878849029541, \"sum\": 1085.878849029541, \"min\": 1085.878849029541}}, \"EndTime\": 1564696306.410352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696305.324026}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:46 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=562.617218848 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:46 INFO 140504085288768] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=78, train loss <loss>=16.7987085342\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:46 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:46 INFO 140504085288768] Epoch[79] Batch[0] avg_epoch_loss=16.747824\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=16.7478237152\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:47 INFO 140504085288768] Epoch[79] Batch[5] avg_epoch_loss=16.822760\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:47 INFO 140504085288768] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=16.8227602641\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:47 INFO 140504085288768] Epoch[79] Batch [5]#011Speed: 956.44 samples/sec#011loss=16.822760\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:47 INFO 140504085288768] processed a total of 580 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1054.2941093444824, \"sum\": 1054.2941093444824, \"min\": 1054.2941093444824}}, \"EndTime\": 1564696307.465189, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696306.410432}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:47 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=550.068053699 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:47 INFO 140504085288768] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:47 INFO 140504085288768] #quality_metric: host=algo-1, epoch=79, train loss <loss>=16.8577869415\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:47 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:47 INFO 140504085288768] Epoch[80] Batch[0] avg_epoch_loss=16.869371\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:47 INFO 140504085288768] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=16.8693714142\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:48 INFO 140504085288768] Epoch[80] Batch[5] avg_epoch_loss=16.848338\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:48 INFO 140504085288768] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=16.8483381271\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:48 INFO 140504085288768] Epoch[80] Batch [5]#011Speed: 869.41 samples/sec#011loss=16.848338\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:48 INFO 140504085288768] processed a total of 609 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1118.2119846343994, \"sum\": 1118.2119846343994, \"min\": 1118.2119846343994}}, \"EndTime\": 1564696308.583978, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696307.465272}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:48 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=544.562908689 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:48 INFO 140504085288768] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:48 INFO 140504085288768] #quality_metric: host=algo-1, epoch=80, train loss <loss>=16.8532022476\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:48 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:49 INFO 140504085288768] Epoch[81] Batch[0] avg_epoch_loss=16.828646\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=16.8286457062\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:49 INFO 140504085288768] Epoch[81] Batch[5] avg_epoch_loss=16.808219\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=16.8082186381\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:49 INFO 140504085288768] Epoch[81] Batch [5]#011Speed: 935.61 samples/sec#011loss=16.808219\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:49 INFO 140504085288768] processed a total of 600 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1047.4069118499756, \"sum\": 1047.4069118499756, \"min\": 1047.4069118499756}}, \"EndTime\": 1564696309.631922, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696308.584058}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:49 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=572.780426186 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:49 INFO 140504085288768] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=81, train loss <loss>=16.7896747589\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:49 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:49 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_28217c46-859a-4cb8-a950-1c7f9b87f1af-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.259069442749023, \"sum\": 21.259069442749023, \"min\": 21.259069442749023}}, \"EndTime\": 1564696309.653757, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696309.632001}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:50 INFO 140504085288768] Epoch[82] Batch[0] avg_epoch_loss=16.848234\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:50 INFO 140504085288768] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=16.8482341766\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:50 INFO 140504085288768] Epoch[82] Batch[5] avg_epoch_loss=16.829073\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:50 INFO 140504085288768] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=16.8290732702\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:50 INFO 140504085288768] Epoch[82] Batch [5]#011Speed: 913.24 samples/sec#011loss=16.829073\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:50 INFO 140504085288768] processed a total of 588 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1075.7989883422852, \"sum\": 1075.7989883422852, \"min\": 1075.7989883422852}}, \"EndTime\": 1564696310.729684, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696309.653821}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:50 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=546.51116068 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:50 INFO 140504085288768] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:50 INFO 140504085288768] #quality_metric: host=algo-1, epoch=82, train loss <loss>=16.8115159988\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:50 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:51 INFO 140504085288768] Epoch[83] Batch[0] avg_epoch_loss=16.784681\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:51 INFO 140504085288768] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=16.7846813202\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:51 INFO 140504085288768] Epoch[83] Batch[5] avg_epoch_loss=16.821237\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:51 INFO 140504085288768] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=16.8212369283\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:51 INFO 140504085288768] Epoch[83] Batch [5]#011Speed: 878.53 samples/sec#011loss=16.821237\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:51 INFO 140504085288768] processed a total of 569 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1015.2089595794678, \"sum\": 1015.2089595794678, \"min\": 1015.2089595794678}}, \"EndTime\": 1564696311.745424, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696310.729765}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:51 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=560.421653979 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:51 INFO 140504085288768] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:51 INFO 140504085288768] #quality_metric: host=algo-1, epoch=83, train loss <loss>=16.8127301534\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:51 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:52 INFO 140504085288768] Epoch[84] Batch[0] avg_epoch_loss=16.704672\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:52 INFO 140504085288768] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=16.7046718597\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:52 INFO 140504085288768] Epoch[84] Batch[5] avg_epoch_loss=16.793077\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:52 INFO 140504085288768] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=16.793077151\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:52 INFO 140504085288768] Epoch[84] Batch [5]#011Speed: 927.02 samples/sec#011loss=16.793077\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:52 INFO 140504085288768] processed a total of 549 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 961.8740081787109, \"sum\": 961.8740081787109, \"min\": 961.8740081787109}}, \"EndTime\": 1564696312.707838, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696311.745486}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:52 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=570.698014987 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:52 INFO 140504085288768] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:52 INFO 140504085288768] #quality_metric: host=algo-1, epoch=84, train loss <loss>=16.7991008759\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:52 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:53 INFO 140504085288768] Epoch[85] Batch[0] avg_epoch_loss=16.611927\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:53 INFO 140504085288768] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=16.6119270325\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:53 INFO 140504085288768] Epoch[85] Batch[5] avg_epoch_loss=16.775286\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:53 INFO 140504085288768] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=16.7752860387\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:53 INFO 140504085288768] Epoch[85] Batch [5]#011Speed: 962.61 samples/sec#011loss=16.775286\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:53 INFO 140504085288768] processed a total of 574 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 961.4849090576172, \"sum\": 961.4849090576172, \"min\": 961.4849090576172}}, \"EndTime\": 1564696313.669924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696312.707912}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:53 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=596.919388514 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:53 INFO 140504085288768] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:53 INFO 140504085288768] #quality_metric: host=algo-1, epoch=85, train loss <loss>=16.7855695089\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:53 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:53 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_61724b1c-ad29-4af7-919b-a3d7cb85625d-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.4079475402832, \"sum\": 36.4079475402832, \"min\": 36.4079475402832}}, \"EndTime\": 1564696313.706927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696313.670006}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:54 INFO 140504085288768] Epoch[86] Batch[0] avg_epoch_loss=16.841684\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:54 INFO 140504085288768] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=16.8416843414\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:54 INFO 140504085288768] Epoch[86] Batch[5] avg_epoch_loss=16.832716\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:54 INFO 140504085288768] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=16.8327159882\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:54 INFO 140504085288768] Epoch[86] Batch [5]#011Speed: 955.60 samples/sec#011loss=16.832716\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:54 INFO 140504085288768] processed a total of 568 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 988.5201454162598, \"sum\": 988.5201454162598, \"min\": 988.5201454162598}}, \"EndTime\": 1564696314.695592, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696313.707006}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:54 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=574.528382953 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:54 INFO 140504085288768] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:54 INFO 140504085288768] #quality_metric: host=algo-1, epoch=86, train loss <loss>=16.7989143795\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:54 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:55 INFO 140504085288768] Epoch[87] Batch[0] avg_epoch_loss=16.740343\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:55 INFO 140504085288768] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=16.7403430939\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:55 INFO 140504085288768] Epoch[87] Batch[5] avg_epoch_loss=16.769795\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:55 INFO 140504085288768] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=16.7697954178\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:55 INFO 140504085288768] Epoch[87] Batch [5]#011Speed: 927.16 samples/sec#011loss=16.769795\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:55 INFO 140504085288768] processed a total of 603 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1053.718090057373, \"sum\": 1053.718090057373, \"min\": 1053.718090057373}}, \"EndTime\": 1564696315.749845, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696314.695672}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:55 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=572.196400986 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:55 INFO 140504085288768] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:55 INFO 140504085288768] #quality_metric: host=algo-1, epoch=87, train loss <loss>=16.8086557388\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:55 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:56 INFO 140504085288768] Epoch[88] Batch[0] avg_epoch_loss=16.722576\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:56 INFO 140504085288768] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=16.7225761414\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:56 INFO 140504085288768] Epoch[88] Batch[5] avg_epoch_loss=16.806022\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:56 INFO 140504085288768] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=16.8060220083\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:56 INFO 140504085288768] Epoch[88] Batch [5]#011Speed: 946.68 samples/sec#011loss=16.806022\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:56 INFO 140504085288768] processed a total of 567 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 987.8940582275391, \"sum\": 987.8940582275391, \"min\": 987.8940582275391}}, \"EndTime\": 1564696316.738279, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696315.749924}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:56 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=573.881425962 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:56 INFO 140504085288768] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:56 INFO 140504085288768] #quality_metric: host=algo-1, epoch=88, train loss <loss>=16.7815568712\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:56 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:56 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_c6887998-bd27-460a-8eba-7f4aebbd8cec-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.802913665771484, \"sum\": 34.802913665771484, \"min\": 34.802913665771484}}, \"EndTime\": 1564696316.773683, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696316.738359}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:57 INFO 140504085288768] Epoch[89] Batch[0] avg_epoch_loss=16.863827\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:57 INFO 140504085288768] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=16.8638267517\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:57 INFO 140504085288768] Epoch[89] Batch[5] avg_epoch_loss=16.806328\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:57 INFO 140504085288768] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=16.8063284556\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:57 INFO 140504085288768] Epoch[89] Batch [5]#011Speed: 918.30 samples/sec#011loss=16.806328\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:57 INFO 140504085288768] processed a total of 625 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1065.5608177185059, \"sum\": 1065.5608177185059, \"min\": 1065.5608177185059}}, \"EndTime\": 1564696317.839387, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696316.77376}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:57 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=586.479979131 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:57 INFO 140504085288768] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:57 INFO 140504085288768] #quality_metric: host=algo-1, epoch=89, train loss <loss>=16.7939292908\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:57 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:58 INFO 140504085288768] Epoch[90] Batch[0] avg_epoch_loss=16.826950\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:58 INFO 140504085288768] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=16.8269500732\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:58 INFO 140504085288768] Epoch[90] Batch[5] avg_epoch_loss=16.815864\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:58 INFO 140504085288768] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=16.8158642451\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:58 INFO 140504085288768] Epoch[90] Batch [5]#011Speed: 958.49 samples/sec#011loss=16.815864\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:58 INFO 140504085288768] processed a total of 593 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1070.1727867126465, \"sum\": 1070.1727867126465, \"min\": 1070.1727867126465}}, \"EndTime\": 1564696318.910095, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696317.839467}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:58 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=554.055890712 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:58 INFO 140504085288768] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:58 INFO 140504085288768] #quality_metric: host=algo-1, epoch=90, train loss <loss>=16.7860225677\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:58 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:59 INFO 140504085288768] Epoch[91] Batch[0] avg_epoch_loss=16.832602\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:59 INFO 140504085288768] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=16.8326015472\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:59 INFO 140504085288768] Epoch[91] Batch[5] avg_epoch_loss=16.755295\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:59 INFO 140504085288768] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=16.7552951177\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:59 INFO 140504085288768] Epoch[91] Batch [5]#011Speed: 932.48 samples/sec#011loss=16.755295\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:59 INFO 140504085288768] processed a total of 601 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1044.6391105651855, \"sum\": 1044.6391105651855, \"min\": 1044.6391105651855}}, \"EndTime\": 1564696319.955268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696318.910175}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:59 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=575.254495415 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:59 INFO 140504085288768] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:59 INFO 140504085288768] #quality_metric: host=algo-1, epoch=91, train loss <loss>=16.7710296631\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:59 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:51:59 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_a34384d4-dce3-4f1f-8822-4f50c293de76-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 35.45713424682617, \"sum\": 35.45713424682617, \"min\": 35.45713424682617}}, \"EndTime\": 1564696319.991335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696319.955349}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:00 INFO 140504085288768] Epoch[92] Batch[0] avg_epoch_loss=16.740643\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:00 INFO 140504085288768] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=16.7406425476\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:52:00 INFO 140504085288768] Epoch[92] Batch[5] avg_epoch_loss=16.814258\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:00 INFO 140504085288768] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=16.8142579397\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:00 INFO 140504085288768] Epoch[92] Batch [5]#011Speed: 944.93 samples/sec#011loss=16.814258\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:01 INFO 140504085288768] processed a total of 588 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1044.9700355529785, \"sum\": 1044.9700355529785, \"min\": 1044.9700355529785}}, \"EndTime\": 1564696321.03644, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696319.99141}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:01 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=562.633686514 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:01 INFO 140504085288768] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:01 INFO 140504085288768] #quality_metric: host=algo-1, epoch=92, train loss <loss>=16.8093050003\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:01 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:01 INFO 140504085288768] Epoch[93] Batch[0] avg_epoch_loss=16.729849\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:01 INFO 140504085288768] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=16.7298488617\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:01 INFO 140504085288768] Epoch[93] Batch[5] avg_epoch_loss=16.820209\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:01 INFO 140504085288768] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=16.8202088674\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:01 INFO 140504085288768] Epoch[93] Batch [5]#011Speed: 920.78 samples/sec#011loss=16.820209\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:02 INFO 140504085288768] processed a total of 595 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1051.5010356903076, \"sum\": 1051.5010356903076, \"min\": 1051.5010356903076}}, \"EndTime\": 1564696322.088512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696321.036516}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:02 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=565.795904912 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:02 INFO 140504085288768] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:02 INFO 140504085288768] #quality_metric: host=algo-1, epoch=93, train loss <loss>=16.7957521439\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:02 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:02 INFO 140504085288768] Epoch[94] Batch[0] avg_epoch_loss=16.809170\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:02 INFO 140504085288768] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=16.8091697693\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:02 INFO 140504085288768] Epoch[94] Batch[5] avg_epoch_loss=16.814470\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:02 INFO 140504085288768] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=16.8144699732\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:02 INFO 140504085288768] Epoch[94] Batch [5]#011Speed: 950.81 samples/sec#011loss=16.814470\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:03 INFO 140504085288768] processed a total of 603 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1084.6569538116455, \"sum\": 1084.6569538116455, \"min\": 1084.6569538116455}}, \"EndTime\": 1564696323.173702, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696322.088591}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:03 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=555.882743442 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:03 INFO 140504085288768] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:03 INFO 140504085288768] #quality_metric: host=algo-1, epoch=94, train loss <loss>=16.7637664795\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:03 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:03 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_43c04c83-c600-48e0-883d-0812c7ae5fc3-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.5369930267334, \"sum\": 22.5369930267334, \"min\": 22.5369930267334}}, \"EndTime\": 1564696323.196804, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696323.173768}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:03 INFO 140504085288768] Epoch[95] Batch[0] avg_epoch_loss=16.799084\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:03 INFO 140504085288768] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=16.7990837097\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:03 INFO 140504085288768] Epoch[95] Batch[5] avg_epoch_loss=16.806612\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:03 INFO 140504085288768] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=16.8066116969\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:03 INFO 140504085288768] Epoch[95] Batch [5]#011Speed: 931.52 samples/sec#011loss=16.806612\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:04 INFO 140504085288768] Epoch[95] Batch[10] avg_epoch_loss=16.814268\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:04 INFO 140504085288768] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=16.8234550476\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:04 INFO 140504085288768] Epoch[95] Batch [10]#011Speed: 947.69 samples/sec#011loss=16.823455\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:04 INFO 140504085288768] processed a total of 658 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1106.5990924835205, \"sum\": 1106.5990924835205, \"min\": 1106.5990924835205}}, \"EndTime\": 1564696324.303531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696323.196871}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:04 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=594.555055518 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:04 INFO 140504085288768] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:04 INFO 140504085288768] #quality_metric: host=algo-1, epoch=95, train loss <loss>=16.8142677654\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:04 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:04 INFO 140504085288768] Epoch[96] Batch[0] avg_epoch_loss=16.828829\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:04 INFO 140504085288768] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=16.8288288116\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:05 INFO 140504085288768] Epoch[96] Batch[5] avg_epoch_loss=16.816879\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:05 INFO 140504085288768] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=16.8168786367\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:05 INFO 140504085288768] Epoch[96] Batch [5]#011Speed: 961.45 samples/sec#011loss=16.816879\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:05 INFO 140504085288768] processed a total of 578 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1028.6791324615479, \"sum\": 1028.6791324615479, \"min\": 1028.6791324615479}}, \"EndTime\": 1564696325.332724, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696324.303607}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:05 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=561.822844794 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:05 INFO 140504085288768] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:05 INFO 140504085288768] #quality_metric: host=algo-1, epoch=96, train loss <loss>=16.8303249359\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:05 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:05 INFO 140504085288768] Epoch[97] Batch[0] avg_epoch_loss=16.784126\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:05 INFO 140504085288768] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=16.7841262817\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:06 INFO 140504085288768] Epoch[97] Batch[5] avg_epoch_loss=16.832314\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:06 INFO 140504085288768] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=16.8323144913\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:06 INFO 140504085288768] Epoch[97] Batch [5]#011Speed: 937.65 samples/sec#011loss=16.832314\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:06 INFO 140504085288768] processed a total of 630 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1068.626880645752, \"sum\": 1068.626880645752, \"min\": 1068.626880645752}}, \"EndTime\": 1564696326.401881, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696325.332803}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:06 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=589.477550359 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:06 INFO 140504085288768] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:06 INFO 140504085288768] #quality_metric: host=algo-1, epoch=97, train loss <loss>=16.8339054108\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:06 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:06 INFO 140504085288768] Epoch[98] Batch[0] avg_epoch_loss=16.705133\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:06 INFO 140504085288768] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=16.7051334381\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:07 INFO 140504085288768] Epoch[98] Batch[5] avg_epoch_loss=16.791858\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:07 INFO 140504085288768] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=16.7918583552\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:07 INFO 140504085288768] Epoch[98] Batch [5]#011Speed: 935.98 samples/sec#011loss=16.791858\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:07 INFO 140504085288768] processed a total of 602 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1046.3149547576904, \"sum\": 1046.3149547576904, \"min\": 1046.3149547576904}}, \"EndTime\": 1564696327.448723, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696326.401961}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:07 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=575.288207436 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:07 INFO 140504085288768] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:07 INFO 140504085288768] #quality_metric: host=algo-1, epoch=98, train loss <loss>=16.8062879562\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:07 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:07 INFO 140504085288768] Epoch[99] Batch[0] avg_epoch_loss=16.791462\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:07 INFO 140504085288768] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=16.7914619446\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:08 INFO 140504085288768] Epoch[99] Batch[5] avg_epoch_loss=16.808065\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:08 INFO 140504085288768] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=16.8080647786\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:08 INFO 140504085288768] Epoch[99] Batch [5]#011Speed: 942.18 samples/sec#011loss=16.808065\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:08 INFO 140504085288768] processed a total of 578 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1027.9510021209717, \"sum\": 1027.9510021209717, \"min\": 1027.9510021209717}}, \"EndTime\": 1564696328.477234, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696327.448803}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:08 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=562.23431759 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:08 INFO 140504085288768] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:08 INFO 140504085288768] #quality_metric: host=algo-1, epoch=99, train loss <loss>=16.7637495041\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:08 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:08 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_643e7334-fe9c-44a0-bf5b-b56eb903ba4c-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.786855697631836, \"sum\": 22.786855697631836, \"min\": 22.786855697631836}}, \"EndTime\": 1564696328.500596, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696328.477293}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:08 INFO 140504085288768] Epoch[100] Batch[0] avg_epoch_loss=16.847353\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:08 INFO 140504085288768] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=16.8473529816\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:09 INFO 140504085288768] Epoch[100] Batch[5] avg_epoch_loss=16.789198\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:09 INFO 140504085288768] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=16.7891976039\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:09 INFO 140504085288768] Epoch[100] Batch [5]#011Speed: 908.63 samples/sec#011loss=16.789198\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:09 INFO 140504085288768] processed a total of 604 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1042.1350002288818, \"sum\": 1042.1350002288818, \"min\": 1042.1350002288818}}, \"EndTime\": 1564696329.542878, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696328.500672}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:09 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=579.518300846 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:09 INFO 140504085288768] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:09 INFO 140504085288768] #quality_metric: host=algo-1, epoch=100, train loss <loss>=16.8124391556\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:09 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:09 INFO 140504085288768] Epoch[101] Batch[0] avg_epoch_loss=16.837229\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:09 INFO 140504085288768] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=16.837228775\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:10 INFO 140504085288768] Epoch[101] Batch[5] avg_epoch_loss=16.829898\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:10 INFO 140504085288768] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=16.8298981984\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:10 INFO 140504085288768] Epoch[101] Batch [5]#011Speed: 943.97 samples/sec#011loss=16.829898\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:10 INFO 140504085288768] processed a total of 576 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 951.3180255889893, \"sum\": 951.3180255889893, \"min\": 951.3180255889893}}, \"EndTime\": 1564696330.494749, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696329.542947}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:10 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=605.401258154 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:10 INFO 140504085288768] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:10 INFO 140504085288768] #quality_metric: host=algo-1, epoch=101, train loss <loss>=16.8161178165\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:10 INFO 140504085288768] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:52:10 INFO 140504085288768] Epoch[102] Batch[0] avg_epoch_loss=16.876825\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:10 INFO 140504085288768] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=16.8768253326\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:11 INFO 140504085288768] Epoch[102] Batch[5] avg_epoch_loss=16.812776\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:11 INFO 140504085288768] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=16.8127756119\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:11 INFO 140504085288768] Epoch[102] Batch [5]#011Speed: 940.04 samples/sec#011loss=16.812776\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:11 INFO 140504085288768] processed a total of 606 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1048.7520694732666, \"sum\": 1048.7520694732666, \"min\": 1048.7520694732666}}, \"EndTime\": 1564696331.544056, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696330.49483}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:11 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=577.763018231 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:11 INFO 140504085288768] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:11 INFO 140504085288768] #quality_metric: host=algo-1, epoch=102, train loss <loss>=16.858782959\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:11 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:11 INFO 140504085288768] Epoch[103] Batch[0] avg_epoch_loss=16.803478\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:11 INFO 140504085288768] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=16.803478241\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:12 INFO 140504085288768] Epoch[103] Batch[5] avg_epoch_loss=16.844109\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:12 INFO 140504085288768] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=16.8441085815\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:12 INFO 140504085288768] Epoch[103] Batch [5]#011Speed: 942.38 samples/sec#011loss=16.844109\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:12 INFO 140504085288768] processed a total of 632 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1048.9709377288818, \"sum\": 1048.9709377288818, \"min\": 1048.9709377288818}}, \"EndTime\": 1564696332.593575, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696331.54414}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:12 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=602.426917673 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:12 INFO 140504085288768] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:12 INFO 140504085288768] #quality_metric: host=algo-1, epoch=103, train loss <loss>=16.8195701599\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:12 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:13 INFO 140504085288768] Epoch[104] Batch[0] avg_epoch_loss=16.814949\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:13 INFO 140504085288768] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=16.8149490356\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:13 INFO 140504085288768] Epoch[104] Batch[5] avg_epoch_loss=16.781059\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:13 INFO 140504085288768] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=16.7810592651\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:13 INFO 140504085288768] Epoch[104] Batch [5]#011Speed: 947.38 samples/sec#011loss=16.781059\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:13 INFO 140504085288768] processed a total of 555 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 974.7998714447021, \"sum\": 974.7998714447021, \"min\": 974.7998714447021}}, \"EndTime\": 1564696333.568906, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696332.593656}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:13 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=569.2798258 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:13 INFO 140504085288768] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:13 INFO 140504085288768] #quality_metric: host=algo-1, epoch=104, train loss <loss>=16.7802901798\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:13 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:14 INFO 140504085288768] Epoch[105] Batch[0] avg_epoch_loss=16.672386\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:14 INFO 140504085288768] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=16.6723861694\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:14 INFO 140504085288768] Epoch[105] Batch[5] avg_epoch_loss=16.794246\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:14 INFO 140504085288768] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=16.7942463557\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:14 INFO 140504085288768] Epoch[105] Batch [5]#011Speed: 913.61 samples/sec#011loss=16.794246\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:14 INFO 140504085288768] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1072.4778175354004, \"sum\": 1072.4778175354004, \"min\": 1072.4778175354004}}, \"EndTime\": 1564696334.641911, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696333.568986}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:14 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=556.594487491 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:14 INFO 140504085288768] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:14 INFO 140504085288768] #quality_metric: host=algo-1, epoch=105, train loss <loss>=16.780024147\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:14 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:15 INFO 140504085288768] Epoch[106] Batch[0] avg_epoch_loss=16.839947\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:15 INFO 140504085288768] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=16.8399467468\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:15 INFO 140504085288768] Epoch[106] Batch[5] avg_epoch_loss=16.773815\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:15 INFO 140504085288768] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=16.7738145192\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:15 INFO 140504085288768] Epoch[106] Batch [5]#011Speed: 952.84 samples/sec#011loss=16.773815\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:15 INFO 140504085288768] processed a total of 628 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1082.4379920959473, \"sum\": 1082.4379920959473, \"min\": 1082.4379920959473}}, \"EndTime\": 1564696335.724883, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696334.641992}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:15 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=580.110724863 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:15 INFO 140504085288768] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:15 INFO 140504085288768] #quality_metric: host=algo-1, epoch=106, train loss <loss>=16.7838676453\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:15 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:16 INFO 140504085288768] Epoch[107] Batch[0] avg_epoch_loss=16.860731\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:16 INFO 140504085288768] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=16.8607311249\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:16 INFO 140504085288768] Epoch[107] Batch[5] avg_epoch_loss=16.760302\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:16 INFO 140504085288768] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=16.76030159\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:16 INFO 140504085288768] Epoch[107] Batch [5]#011Speed: 873.01 samples/sec#011loss=16.760302\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:16 INFO 140504085288768] processed a total of 602 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1099.3177890777588, \"sum\": 1099.3177890777588, \"min\": 1099.3177890777588}}, \"EndTime\": 1564696336.824735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696335.724963}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:16 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=547.552975654 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:16 INFO 140504085288768] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:16 INFO 140504085288768] #quality_metric: host=algo-1, epoch=107, train loss <loss>=16.7468097687\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:16 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:16 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_70f6d1e9-a530-4161-84bf-0a1d78275f74-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.84511375427246, \"sum\": 34.84511375427246, \"min\": 34.84511375427246}}, \"EndTime\": 1564696336.860167, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696336.824817}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:17 INFO 140504085288768] Epoch[108] Batch[0] avg_epoch_loss=16.867329\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:17 INFO 140504085288768] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=16.8673286438\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:17 INFO 140504085288768] Epoch[108] Batch[5] avg_epoch_loss=16.816848\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:17 INFO 140504085288768] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=16.8168481191\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:17 INFO 140504085288768] Epoch[108] Batch [5]#011Speed: 956.26 samples/sec#011loss=16.816848\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:17 INFO 140504085288768] processed a total of 617 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1059.424877166748, \"sum\": 1059.424877166748, \"min\": 1059.424877166748}}, \"EndTime\": 1564696337.919734, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696336.860244}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:17 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=582.326982508 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:17 INFO 140504085288768] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:17 INFO 140504085288768] #quality_metric: host=algo-1, epoch=108, train loss <loss>=16.824032402\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:17 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:18 INFO 140504085288768] Epoch[109] Batch[0] avg_epoch_loss=16.871502\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:18 INFO 140504085288768] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=16.8715019226\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:18 INFO 140504085288768] Epoch[109] Batch[5] avg_epoch_loss=16.832763\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:18 INFO 140504085288768] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=16.832763354\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:18 INFO 140504085288768] Epoch[109] Batch [5]#011Speed: 945.71 samples/sec#011loss=16.832763\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:18 INFO 140504085288768] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1058.1080913543701, \"sum\": 1058.1080913543701, \"min\": 1058.1080913543701}}, \"EndTime\": 1564696338.978367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696337.919813}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:18 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=564.152789008 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:18 INFO 140504085288768] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:18 INFO 140504085288768] #quality_metric: host=algo-1, epoch=109, train loss <loss>=16.8555028915\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:18 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:19 INFO 140504085288768] Epoch[110] Batch[0] avg_epoch_loss=16.843206\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:19 INFO 140504085288768] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=16.8432064056\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:19 INFO 140504085288768] Epoch[110] Batch[5] avg_epoch_loss=16.801937\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:19 INFO 140504085288768] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=16.8019374212\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:19 INFO 140504085288768] Epoch[110] Batch [5]#011Speed: 899.10 samples/sec#011loss=16.801937\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:20 INFO 140504085288768] processed a total of 620 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1068.5560703277588, \"sum\": 1068.5560703277588, \"min\": 1068.5560703277588}}, \"EndTime\": 1564696340.047459, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696338.978446}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:20 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=580.158167245 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:20 INFO 140504085288768] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:20 INFO 140504085288768] #quality_metric: host=algo-1, epoch=110, train loss <loss>=16.8044063568\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:20 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:20 INFO 140504085288768] Epoch[111] Batch[0] avg_epoch_loss=16.834070\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:20 INFO 140504085288768] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=16.8340702057\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:52:20 INFO 140504085288768] Epoch[111] Batch[5] avg_epoch_loss=16.753536\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:20 INFO 140504085288768] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=16.7535355886\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:20 INFO 140504085288768] Epoch[111] Batch [5]#011Speed: 940.90 samples/sec#011loss=16.753536\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:21 INFO 140504085288768] processed a total of 566 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 980.6020259857178, \"sum\": 980.6020259857178, \"min\": 980.6020259857178}}, \"EndTime\": 1564696341.028597, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696340.047541}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:21 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=577.127684832 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:21 INFO 140504085288768] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:21 INFO 140504085288768] #quality_metric: host=algo-1, epoch=111, train loss <loss>=16.7473555671\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:21 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:21 INFO 140504085288768] Epoch[112] Batch[0] avg_epoch_loss=16.815477\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:21 INFO 140504085288768] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=16.8154773712\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:21 INFO 140504085288768] Epoch[112] Batch[5] avg_epoch_loss=16.770481\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:21 INFO 140504085288768] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=16.7704807917\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:21 INFO 140504085288768] Epoch[112] Batch [5]#011Speed: 939.26 samples/sec#011loss=16.770481\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:22 INFO 140504085288768] processed a total of 575 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 989.9508953094482, \"sum\": 989.9508953094482, \"min\": 989.9508953094482}}, \"EndTime\": 1564696342.019097, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696341.028677}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:22 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=580.769332651 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:22 INFO 140504085288768] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:22 INFO 140504085288768] #quality_metric: host=algo-1, epoch=112, train loss <loss>=16.7843475342\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:22 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:22 INFO 140504085288768] Epoch[113] Batch[0] avg_epoch_loss=16.845034\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:22 INFO 140504085288768] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=16.8450336456\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:22 INFO 140504085288768] Epoch[113] Batch[5] avg_epoch_loss=16.807080\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:22 INFO 140504085288768] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=16.8070796331\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:22 INFO 140504085288768] Epoch[113] Batch [5]#011Speed: 895.91 samples/sec#011loss=16.807080\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:23 INFO 140504085288768] processed a total of 590 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1088.37890625, \"sum\": 1088.37890625, \"min\": 1088.37890625}}, \"EndTime\": 1564696343.107994, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696342.019176}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:23 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=542.032798749 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:23 INFO 140504085288768] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:23 INFO 140504085288768] #quality_metric: host=algo-1, epoch=113, train loss <loss>=16.8434970856\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:23 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:23 INFO 140504085288768] Epoch[114] Batch[0] avg_epoch_loss=16.821815\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:23 INFO 140504085288768] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=16.8218154907\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:23 INFO 140504085288768] Epoch[114] Batch[5] avg_epoch_loss=16.788633\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:23 INFO 140504085288768] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=16.7886327108\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:23 INFO 140504085288768] Epoch[114] Batch [5]#011Speed: 959.85 samples/sec#011loss=16.788633\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:24 INFO 140504085288768] processed a total of 579 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1064.6350383758545, \"sum\": 1064.6350383758545, \"min\": 1064.6350383758545}}, \"EndTime\": 1564696344.173161, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696343.108074}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:24 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=543.785017341 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:24 INFO 140504085288768] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:24 INFO 140504085288768] #quality_metric: host=algo-1, epoch=114, train loss <loss>=16.7911201477\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:24 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:24 INFO 140504085288768] Epoch[115] Batch[0] avg_epoch_loss=16.723835\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:24 INFO 140504085288768] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=16.7238349915\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:24 INFO 140504085288768] Epoch[115] Batch[5] avg_epoch_loss=16.782827\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:24 INFO 140504085288768] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=16.7828273773\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:24 INFO 140504085288768] Epoch[115] Batch [5]#011Speed: 956.52 samples/sec#011loss=16.782827\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:25 INFO 140504085288768] processed a total of 582 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1049.3900775909424, \"sum\": 1049.3900775909424, \"min\": 1049.3900775909424}}, \"EndTime\": 1564696345.223127, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696344.173243}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:25 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=554.546012315 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:25 INFO 140504085288768] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:25 INFO 140504085288768] #quality_metric: host=algo-1, epoch=115, train loss <loss>=16.8065481186\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:25 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:25 INFO 140504085288768] Epoch[116] Batch[0] avg_epoch_loss=16.801130\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:25 INFO 140504085288768] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=16.8011302948\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] Epoch[116] Batch[5] avg_epoch_loss=16.785116\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=16.7851161957\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] Epoch[116] Batch [5]#011Speed: 950.11 samples/sec#011loss=16.785116\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] Epoch[116] Batch[10] avg_epoch_loss=16.776875\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=16.7669857025\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] Epoch[116] Batch [10]#011Speed: 910.41 samples/sec#011loss=16.766986\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] processed a total of 669 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1144.9739933013916, \"sum\": 1144.9739933013916, \"min\": 1144.9739933013916}}, \"EndTime\": 1564696346.368639, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696345.223209}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=584.212579231 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=116, train loss <loss>=16.7768750624\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] Epoch[117] Batch[0] avg_epoch_loss=16.842342\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=16.8423423767\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:27 INFO 140504085288768] Epoch[117] Batch[5] avg_epoch_loss=16.779422\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=16.7794221242\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:27 INFO 140504085288768] Epoch[117] Batch [5]#011Speed: 925.24 samples/sec#011loss=16.779422\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:27 INFO 140504085288768] processed a total of 572 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1016.9970989227295, \"sum\": 1016.9970989227295, \"min\": 1016.9970989227295}}, \"EndTime\": 1564696347.386419, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696346.36871}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:27 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=562.373306592 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:27 INFO 140504085288768] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=117, train loss <loss>=16.781695048\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:27 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:27 INFO 140504085288768] Epoch[118] Batch[0] avg_epoch_loss=16.817604\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=16.8176040649\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:28 INFO 140504085288768] Epoch[118] Batch[5] avg_epoch_loss=16.752838\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=16.7528381348\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:28 INFO 140504085288768] Epoch[118] Batch [5]#011Speed: 928.52 samples/sec#011loss=16.752838\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:28 INFO 140504085288768] processed a total of 570 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 976.5880107879639, \"sum\": 976.5880107879639, \"min\": 976.5880107879639}}, \"EndTime\": 1564696348.363545, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696347.386498}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:28 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=583.593657774 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:28 INFO 140504085288768] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=118, train loss <loss>=16.7410062154\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:28 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:28 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_7a685844-a9d4-44fb-b08e-ab532b1133fb-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.012161254882812, \"sum\": 23.012161254882812, \"min\": 23.012161254882812}}, \"EndTime\": 1564696348.387179, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696348.363625}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:28 INFO 140504085288768] Epoch[119] Batch[0] avg_epoch_loss=16.809566\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=16.8095664978\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:29 INFO 140504085288768] Epoch[119] Batch[5] avg_epoch_loss=16.789248\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:29 INFO 140504085288768] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=16.7892481486\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:29 INFO 140504085288768] Epoch[119] Batch [5]#011Speed: 948.95 samples/sec#011loss=16.789248\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:29 INFO 140504085288768] processed a total of 627 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1027.3380279541016, \"sum\": 1027.3380279541016, \"min\": 1027.3380279541016}}, \"EndTime\": 1564696349.414644, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696348.387244}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:29 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=610.25938596 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:29 INFO 140504085288768] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:29 INFO 140504085288768] #quality_metric: host=algo-1, epoch=119, train loss <loss>=16.7810016632\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:29 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:29 INFO 140504085288768] Epoch[120] Batch[0] avg_epoch_loss=16.816969\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:29 INFO 140504085288768] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=16.8169689178\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:30 INFO 140504085288768] Epoch[120] Batch[5] avg_epoch_loss=16.815626\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=16.8156255086\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:30 INFO 140504085288768] Epoch[120] Batch [5]#011Speed: 906.40 samples/sec#011loss=16.815626\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:30 INFO 140504085288768] processed a total of 579 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1038.9258861541748, \"sum\": 1038.9258861541748, \"min\": 1038.9258861541748}}, \"EndTime\": 1564696350.454096, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696349.414706}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:30 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=557.245229689 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:30 INFO 140504085288768] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=120, train loss <loss>=16.8464559555\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:30 INFO 140504085288768] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:52:30 INFO 140504085288768] Epoch[121] Batch[0] avg_epoch_loss=16.674484\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=16.6744842529\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:31 INFO 140504085288768] Epoch[121] Batch[5] avg_epoch_loss=16.799065\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=16.7990649541\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:31 INFO 140504085288768] Epoch[121] Batch [5]#011Speed: 937.78 samples/sec#011loss=16.799065\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:31 INFO 140504085288768] processed a total of 593 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1076.3239860534668, \"sum\": 1076.3239860534668, \"min\": 1076.3239860534668}}, \"EndTime\": 1564696351.530973, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696350.454175}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:31 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=550.889921564 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:31 INFO 140504085288768] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=121, train loss <loss>=16.8162473679\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:31 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:31 INFO 140504085288768] Epoch[122] Batch[0] avg_epoch_loss=16.675320\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=16.6753196716\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:32 INFO 140504085288768] Epoch[122] Batch[5] avg_epoch_loss=16.769515\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:32 INFO 140504085288768] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=16.7695147196\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:32 INFO 140504085288768] Epoch[122] Batch [5]#011Speed: 915.91 samples/sec#011loss=16.769515\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:32 INFO 140504085288768] processed a total of 594 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1070.9810256958008, \"sum\": 1070.9810256958008, \"min\": 1070.9810256958008}}, \"EndTime\": 1564696352.602516, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696351.531054}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:32 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=554.574267026 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:32 INFO 140504085288768] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:32 INFO 140504085288768] #quality_metric: host=algo-1, epoch=122, train loss <loss>=16.7328536987\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:32 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:32 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_3693f43d-269e-4d9f-9b20-e2a71f71b81e-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.998167037963867, \"sum\": 21.998167037963867, \"min\": 21.998167037963867}}, \"EndTime\": 1564696352.625117, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696352.602591}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:33 INFO 140504085288768] Epoch[123] Batch[0] avg_epoch_loss=16.760553\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=16.76055336\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:33 INFO 140504085288768] Epoch[123] Batch[5] avg_epoch_loss=16.770895\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=16.7708950043\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:33 INFO 140504085288768] Epoch[123] Batch [5]#011Speed: 950.68 samples/sec#011loss=16.770895\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:33 INFO 140504085288768] processed a total of 617 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1036.9679927825928, \"sum\": 1036.9679927825928, \"min\": 1036.9679927825928}}, \"EndTime\": 1564696353.662208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696352.625181}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:33 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=594.944807233 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:33 INFO 140504085288768] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=123, train loss <loss>=16.7719820023\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:33 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:34 INFO 140504085288768] Epoch[124] Batch[0] avg_epoch_loss=16.794744\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=16.7947444916\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:34 INFO 140504085288768] Epoch[124] Batch[5] avg_epoch_loss=16.788475\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=16.7884747187\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:34 INFO 140504085288768] Epoch[124] Batch [5]#011Speed: 945.02 samples/sec#011loss=16.788475\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:34 INFO 140504085288768] processed a total of 571 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 953.0150890350342, \"sum\": 953.0150890350342, \"min\": 953.0150890350342}}, \"EndTime\": 1564696354.615734, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696353.662281}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:34 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=599.075673293 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:34 INFO 140504085288768] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=124, train loss <loss>=16.7874747382\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:34 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:35 INFO 140504085288768] Epoch[125] Batch[0] avg_epoch_loss=16.701355\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:35 INFO 140504085288768] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=16.7013549805\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:35 INFO 140504085288768] Epoch[125] Batch[5] avg_epoch_loss=16.814051\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:35 INFO 140504085288768] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=16.8140509923\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:35 INFO 140504085288768] Epoch[125] Batch [5]#011Speed: 930.84 samples/sec#011loss=16.814051\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:35 INFO 140504085288768] processed a total of 605 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1075.5019187927246, \"sum\": 1075.5019187927246, \"min\": 1075.5019187927246}}, \"EndTime\": 1564696355.691779, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696354.615815}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:35 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=562.468950157 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:35 INFO 140504085288768] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:35 INFO 140504085288768] #quality_metric: host=algo-1, epoch=125, train loss <loss>=16.8290830612\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:35 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:36 INFO 140504085288768] Epoch[126] Batch[0] avg_epoch_loss=16.851337\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=16.8513374329\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:36 INFO 140504085288768] Epoch[126] Batch[5] avg_epoch_loss=16.756016\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=16.7560157776\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:36 INFO 140504085288768] Epoch[126] Batch [5]#011Speed: 920.12 samples/sec#011loss=16.756016\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:36 INFO 140504085288768] processed a total of 631 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1053.2479286193848, \"sum\": 1053.2479286193848, \"min\": 1053.2479286193848}}, \"EndTime\": 1564696356.745522, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696355.691859}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:36 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=599.020697244 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:36 INFO 140504085288768] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=126, train loss <loss>=16.7720273972\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:36 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:37 INFO 140504085288768] Epoch[127] Batch[0] avg_epoch_loss=16.867332\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=16.8673324585\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:37 INFO 140504085288768] Epoch[127] Batch[5] avg_epoch_loss=16.803091\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=16.8030910492\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:37 INFO 140504085288768] Epoch[127] Batch [5]#011Speed: 945.21 samples/sec#011loss=16.803091\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:37 INFO 140504085288768] processed a total of 617 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1039.6571159362793, \"sum\": 1039.6571159362793, \"min\": 1039.6571159362793}}, \"EndTime\": 1564696357.78574, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696356.745625}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:37 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=593.401479992 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:37 INFO 140504085288768] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=127, train loss <loss>=16.7770875931\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:37 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:38 INFO 140504085288768] Epoch[128] Batch[0] avg_epoch_loss=16.807150\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:38 INFO 140504085288768] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=16.8071498871\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:38 INFO 140504085288768] Epoch[128] Batch[5] avg_epoch_loss=16.786836\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:38 INFO 140504085288768] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=16.7868363063\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:38 INFO 140504085288768] Epoch[128] Batch [5]#011Speed: 946.68 samples/sec#011loss=16.786836\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:38 INFO 140504085288768] processed a total of 555 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 964.259147644043, \"sum\": 964.259147644043, \"min\": 964.259147644043}}, \"EndTime\": 1564696358.750498, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696357.785817}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:38 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=575.506952175 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:38 INFO 140504085288768] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:38 INFO 140504085288768] #quality_metric: host=algo-1, epoch=128, train loss <loss>=16.791973114\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:38 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:39 INFO 140504085288768] Epoch[129] Batch[0] avg_epoch_loss=16.769791\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:39 INFO 140504085288768] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=16.7697906494\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:39 INFO 140504085288768] Epoch[129] Batch[5] avg_epoch_loss=16.786191\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:39 INFO 140504085288768] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=16.7861913045\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:39 INFO 140504085288768] Epoch[129] Batch [5]#011Speed: 845.60 samples/sec#011loss=16.786191\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:39 INFO 140504085288768] processed a total of 589 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1061.7470741271973, \"sum\": 1061.7470741271973, \"min\": 1061.7470741271973}}, \"EndTime\": 1564696359.812789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696358.750568}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:39 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=554.686640769 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:39 INFO 140504085288768] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:39 INFO 140504085288768] #quality_metric: host=algo-1, epoch=129, train loss <loss>=16.7923952103\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:39 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:40 INFO 140504085288768] Epoch[130] Batch[0] avg_epoch_loss=16.829418\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=16.8294181824\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:40 INFO 140504085288768] Epoch[130] Batch[5] avg_epoch_loss=16.732470\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=16.7324701945\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:40 INFO 140504085288768] Epoch[130] Batch [5]#011Speed: 953.43 samples/sec#011loss=16.732470\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:52:40 INFO 140504085288768] processed a total of 596 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1053.7519454956055, \"sum\": 1053.7519454956055, \"min\": 1053.7519454956055}}, \"EndTime\": 1564696360.867047, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696359.812867}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:40 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=565.536331754 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:40 INFO 140504085288768] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=130, train loss <loss>=16.7680467606\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:40 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:41 INFO 140504085288768] Epoch[131] Batch[0] avg_epoch_loss=16.864527\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=16.8645267487\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:41 INFO 140504085288768] Epoch[131] Batch[5] avg_epoch_loss=16.805035\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=16.8050346375\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:41 INFO 140504085288768] Epoch[131] Batch [5]#011Speed: 911.57 samples/sec#011loss=16.805035\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:41 INFO 140504085288768] processed a total of 595 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1078.0961513519287, \"sum\": 1078.0961513519287, \"min\": 1078.0961513519287}}, \"EndTime\": 1564696361.945674, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696360.867128}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:41 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=551.841467667 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:41 INFO 140504085288768] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=131, train loss <loss>=16.7875993729\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:41 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:42 INFO 140504085288768] Epoch[132] Batch[0] avg_epoch_loss=16.773146\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:42 INFO 140504085288768] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=16.7731456757\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:42 INFO 140504085288768] Epoch[132] Batch[5] avg_epoch_loss=16.827701\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:42 INFO 140504085288768] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=16.8277009328\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:42 INFO 140504085288768] Epoch[132] Batch [5]#011Speed: 949.78 samples/sec#011loss=16.827701\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:43 INFO 140504085288768] processed a total of 591 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1076.1148929595947, \"sum\": 1076.1148929595947, \"min\": 1076.1148929595947}}, \"EndTime\": 1564696363.022294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696361.945752}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:43 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=549.138126257 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:43 INFO 140504085288768] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:43 INFO 140504085288768] #quality_metric: host=algo-1, epoch=132, train loss <loss>=16.8361045837\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:43 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:43 INFO 140504085288768] Epoch[133] Batch[0] avg_epoch_loss=16.637011\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:43 INFO 140504085288768] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=16.6370105743\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:43 INFO 140504085288768] Epoch[133] Batch[5] avg_epoch_loss=16.767630\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:43 INFO 140504085288768] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=16.7676302592\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:43 INFO 140504085288768] Epoch[133] Batch [5]#011Speed: 937.33 samples/sec#011loss=16.767630\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:44 INFO 140504085288768] processed a total of 574 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1011.5129947662354, \"sum\": 1011.5129947662354, \"min\": 1011.5129947662354}}, \"EndTime\": 1564696364.034325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696363.022375}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:44 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=567.404435338 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:44 INFO 140504085288768] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:44 INFO 140504085288768] #quality_metric: host=algo-1, epoch=133, train loss <loss>=16.7760857476\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:44 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:44 INFO 140504085288768] Epoch[134] Batch[0] avg_epoch_loss=16.886642\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:44 INFO 140504085288768] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=16.8866424561\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:44 INFO 140504085288768] Epoch[134] Batch[5] avg_epoch_loss=16.814152\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:44 INFO 140504085288768] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=16.8141520818\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:44 INFO 140504085288768] Epoch[134] Batch [5]#011Speed: 937.90 samples/sec#011loss=16.814152\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:45 INFO 140504085288768] processed a total of 624 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1049.010992050171, \"sum\": 1049.010992050171, \"min\": 1049.010992050171}}, \"EndTime\": 1564696365.083844, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696364.034402}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:45 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=594.779768296 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:45 INFO 140504085288768] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=134, train loss <loss>=16.7843919754\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:45 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:45 INFO 140504085288768] Epoch[135] Batch[0] avg_epoch_loss=16.701414\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=16.7014141083\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:45 INFO 140504085288768] Epoch[135] Batch[5] avg_epoch_loss=16.786840\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=16.7868398031\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:45 INFO 140504085288768] Epoch[135] Batch [5]#011Speed: 945.98 samples/sec#011loss=16.786840\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:46 INFO 140504085288768] processed a total of 582 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1045.2320575714111, \"sum\": 1045.2320575714111, \"min\": 1045.2320575714111}}, \"EndTime\": 1564696366.1296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696365.083926}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:46 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=556.757763604 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:46 INFO 140504085288768] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=135, train loss <loss>=16.738502121\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:46 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:46 INFO 140504085288768] Epoch[136] Batch[0] avg_epoch_loss=16.800749\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=16.8007488251\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:46 INFO 140504085288768] Epoch[136] Batch[5] avg_epoch_loss=16.783475\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=16.7834749222\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:46 INFO 140504085288768] Epoch[136] Batch [5]#011Speed: 917.16 samples/sec#011loss=16.783475\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:47 INFO 140504085288768] processed a total of 579 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1052.9282093048096, \"sum\": 1052.9282093048096, \"min\": 1052.9282093048096}}, \"EndTime\": 1564696367.183115, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696366.129668}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:47 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=549.833536761 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:47 INFO 140504085288768] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:47 INFO 140504085288768] #quality_metric: host=algo-1, epoch=136, train loss <loss>=16.7783430099\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:47 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:47 INFO 140504085288768] Epoch[137] Batch[0] avg_epoch_loss=16.747816\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:47 INFO 140504085288768] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=16.7478160858\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:47 INFO 140504085288768] Epoch[137] Batch[5] avg_epoch_loss=16.783866\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:47 INFO 140504085288768] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=16.7838656108\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:47 INFO 140504085288768] Epoch[137] Batch [5]#011Speed: 946.20 samples/sec#011loss=16.783866\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:48 INFO 140504085288768] processed a total of 610 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1061.2268447875977, \"sum\": 1061.2268447875977, \"min\": 1061.2268447875977}}, \"EndTime\": 1564696368.244871, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696367.183197}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:48 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=574.7472777 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:48 INFO 140504085288768] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:48 INFO 140504085288768] #quality_metric: host=algo-1, epoch=137, train loss <loss>=16.7605852127\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:48 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:48 INFO 140504085288768] Epoch[138] Batch[0] avg_epoch_loss=16.733564\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:48 INFO 140504085288768] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=16.7335643768\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] Epoch[138] Batch[5] avg_epoch_loss=16.799209\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=16.7992092768\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] Epoch[138] Batch [5]#011Speed: 940.97 samples/sec#011loss=16.799209\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] processed a total of 546 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 970.2589511871338, \"sum\": 970.2589511871338, \"min\": 970.2589511871338}}, \"EndTime\": 1564696369.215693, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696368.244942}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=562.667929876 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=138, train loss <loss>=16.8254419963\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] Epoch[139] Batch[0] avg_epoch_loss=16.803762\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=16.8037624359\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] Epoch[139] Batch[5] avg_epoch_loss=16.762552\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=16.7625522614\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:49 INFO 140504085288768] Epoch[139] Batch [5]#011Speed: 953.86 samples/sec#011loss=16.762552\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:50 INFO 140504085288768] processed a total of 576 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 981.2030792236328, \"sum\": 981.2030792236328, \"min\": 981.2030792236328}}, \"EndTime\": 1564696370.197419, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696369.215772}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:50 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=586.965694885 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:50 INFO 140504085288768] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:50 INFO 140504085288768] #quality_metric: host=algo-1, epoch=139, train loss <loss>=16.758462694\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:50 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:50 INFO 140504085288768] Epoch[140] Batch[0] avg_epoch_loss=16.789028\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:50 INFO 140504085288768] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=16.7890281677\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:52:50 INFO 140504085288768] Epoch[140] Batch[5] avg_epoch_loss=16.799605\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:50 INFO 140504085288768] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=16.7996053696\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:50 INFO 140504085288768] Epoch[140] Batch [5]#011Speed: 959.97 samples/sec#011loss=16.799605\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:51 INFO 140504085288768] processed a total of 561 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 990.0181293487549, \"sum\": 990.0181293487549, \"min\": 990.0181293487549}}, \"EndTime\": 1564696371.187985, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696370.197498}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:51 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=566.589976265 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:51 INFO 140504085288768] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:51 INFO 140504085288768] #quality_metric: host=algo-1, epoch=140, train loss <loss>=16.7882092794\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:51 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:51 INFO 140504085288768] Epoch[141] Batch[0] avg_epoch_loss=16.830574\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:51 INFO 140504085288768] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=16.8305740356\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:52 INFO 140504085288768] Epoch[141] Batch[5] avg_epoch_loss=16.800468\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:52 INFO 140504085288768] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=16.800467809\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:52 INFO 140504085288768] Epoch[141] Batch [5]#011Speed: 898.39 samples/sec#011loss=16.800468\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:52 INFO 140504085288768] processed a total of 592 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1098.7789630889893, \"sum\": 1098.7789630889893, \"min\": 1098.7789630889893}}, \"EndTime\": 1564696372.287297, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696371.188065}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:52 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=538.722954156 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:52 INFO 140504085288768] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:52 INFO 140504085288768] #quality_metric: host=algo-1, epoch=141, train loss <loss>=16.8029972076\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:52 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:52 INFO 140504085288768] Epoch[142] Batch[0] avg_epoch_loss=16.859030\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:52 INFO 140504085288768] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=16.8590297699\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:53 INFO 140504085288768] Epoch[142] Batch[5] avg_epoch_loss=16.794318\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:53 INFO 140504085288768] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=16.7943181992\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:53 INFO 140504085288768] Epoch[142] Batch [5]#011Speed: 936.29 samples/sec#011loss=16.794318\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:53 INFO 140504085288768] processed a total of 584 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1042.133092880249, \"sum\": 1042.133092880249, \"min\": 1042.133092880249}}, \"EndTime\": 1564696373.329954, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696372.287377}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:53 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=560.33100399 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:53 INFO 140504085288768] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:53 INFO 140504085288768] #quality_metric: host=algo-1, epoch=142, train loss <loss>=16.7747444153\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:53 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:53 INFO 140504085288768] Epoch[143] Batch[0] avg_epoch_loss=16.829151\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:53 INFO 140504085288768] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=16.8291511536\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:54 INFO 140504085288768] Epoch[143] Batch[5] avg_epoch_loss=16.778297\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:54 INFO 140504085288768] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=16.7782971064\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:54 INFO 140504085288768] Epoch[143] Batch [5]#011Speed: 942.45 samples/sec#011loss=16.778297\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:54 INFO 140504085288768] processed a total of 606 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1040.1549339294434, \"sum\": 1040.1549339294434, \"min\": 1040.1549339294434}}, \"EndTime\": 1564696374.370707, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696373.33003}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:54 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=582.535413159 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:54 INFO 140504085288768] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:54 INFO 140504085288768] #quality_metric: host=algo-1, epoch=143, train loss <loss>=16.7789823532\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:54 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:54 INFO 140504085288768] Epoch[144] Batch[0] avg_epoch_loss=16.810152\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:54 INFO 140504085288768] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=16.8101520538\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:55 INFO 140504085288768] Epoch[144] Batch[5] avg_epoch_loss=16.822033\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:55 INFO 140504085288768] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=16.8220329285\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:55 INFO 140504085288768] Epoch[144] Batch [5]#011Speed: 935.88 samples/sec#011loss=16.822033\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:55 INFO 140504085288768] processed a total of 546 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 963.6609554290771, \"sum\": 963.6609554290771, \"min\": 963.6609554290771}}, \"EndTime\": 1564696375.334928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696374.370792}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:55 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=566.528202719 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:55 INFO 140504085288768] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:55 INFO 140504085288768] #quality_metric: host=algo-1, epoch=144, train loss <loss>=16.7796999613\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:55 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:55 INFO 140504085288768] Epoch[145] Batch[0] avg_epoch_loss=16.652159\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:55 INFO 140504085288768] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=16.6521587372\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:56 INFO 140504085288768] Epoch[145] Batch[5] avg_epoch_loss=16.797448\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:56 INFO 140504085288768] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=16.7974484762\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:56 INFO 140504085288768] Epoch[145] Batch [5]#011Speed: 929.97 samples/sec#011loss=16.797448\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:56 INFO 140504085288768] processed a total of 595 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1043.247938156128, \"sum\": 1043.247938156128, \"min\": 1043.247938156128}}, \"EndTime\": 1564696376.378739, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696375.335002}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:56 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=570.274530827 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:56 INFO 140504085288768] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:56 INFO 140504085288768] #quality_metric: host=algo-1, epoch=145, train loss <loss>=16.8191913605\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:56 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:56 INFO 140504085288768] Epoch[146] Batch[0] avg_epoch_loss=16.760931\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:56 INFO 140504085288768] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=16.760931015\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:57 INFO 140504085288768] Epoch[146] Batch[5] avg_epoch_loss=16.793416\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:57 INFO 140504085288768] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=16.7934160233\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:57 INFO 140504085288768] Epoch[146] Batch [5]#011Speed: 945.63 samples/sec#011loss=16.793416\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:57 INFO 140504085288768] processed a total of 559 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 988.7640476226807, \"sum\": 988.7640476226807, \"min\": 988.7640476226807}}, \"EndTime\": 1564696377.368084, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696376.378809}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:57 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=565.282073982 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:57 INFO 140504085288768] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:57 INFO 140504085288768] #quality_metric: host=algo-1, epoch=146, train loss <loss>=16.8045525021\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:57 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:57 INFO 140504085288768] Epoch[147] Batch[0] avg_epoch_loss=16.818335\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:57 INFO 140504085288768] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=16.8183345795\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:58 INFO 140504085288768] Epoch[147] Batch[5] avg_epoch_loss=16.823101\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:58 INFO 140504085288768] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=16.8231013616\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:58 INFO 140504085288768] Epoch[147] Batch [5]#011Speed: 941.74 samples/sec#011loss=16.823101\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:58 INFO 140504085288768] processed a total of 560 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 985.4471683502197, \"sum\": 985.4471683502197, \"min\": 985.4471683502197}}, \"EndTime\": 1564696378.35411, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696377.368167}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:58 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=568.215772032 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:58 INFO 140504085288768] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:58 INFO 140504085288768] #quality_metric: host=algo-1, epoch=147, train loss <loss>=16.8173785739\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:58 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:58 INFO 140504085288768] Epoch[148] Batch[0] avg_epoch_loss=16.791449\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:58 INFO 140504085288768] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=16.7914485931\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:59 INFO 140504085288768] Epoch[148] Batch[5] avg_epoch_loss=16.819439\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:59 INFO 140504085288768] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=16.8194389343\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:59 INFO 140504085288768] Epoch[148] Batch [5]#011Speed: 949.94 samples/sec#011loss=16.819439\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:59 INFO 140504085288768] processed a total of 568 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 962.6269340515137, \"sum\": 962.6269340515137, \"min\": 962.6269340515137}}, \"EndTime\": 1564696379.317284, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696378.354171}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:59 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=589.987028711 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:59 INFO 140504085288768] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:59 INFO 140504085288768] #quality_metric: host=algo-1, epoch=148, train loss <loss>=16.8018555111\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:59 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:59 INFO 140504085288768] Epoch[149] Batch[0] avg_epoch_loss=16.765150\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:52:59 INFO 140504085288768] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=16.7651500702\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:00 INFO 140504085288768] Epoch[149] Batch[5] avg_epoch_loss=16.798546\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:00 INFO 140504085288768] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=16.7985464732\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:00 INFO 140504085288768] Epoch[149] Batch [5]#011Speed: 944.44 samples/sec#011loss=16.798546\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:00 INFO 140504085288768] processed a total of 578 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1034.3999862670898, \"sum\": 1034.3999862670898, \"min\": 1034.3999862670898}}, \"EndTime\": 1564696380.352254, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696379.317349}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:00 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=558.720349866 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:00 INFO 140504085288768] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:00 INFO 140504085288768] #quality_metric: host=algo-1, epoch=149, train loss <loss>=16.793469429\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:00 INFO 140504085288768] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:53:00 INFO 140504085288768] Epoch[150] Batch[0] avg_epoch_loss=16.814499\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:00 INFO 140504085288768] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=16.8144989014\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:01 INFO 140504085288768] Epoch[150] Batch[5] avg_epoch_loss=16.751002\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:01 INFO 140504085288768] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=16.7510019938\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:01 INFO 140504085288768] Epoch[150] Batch [5]#011Speed: 910.00 samples/sec#011loss=16.751002\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:01 INFO 140504085288768] processed a total of 596 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1054.4798374176025, \"sum\": 1054.4798374176025, \"min\": 1054.4798374176025}}, \"EndTime\": 1564696381.407297, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696380.35233}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:01 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=565.141521109 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:01 INFO 140504085288768] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:01 INFO 140504085288768] #quality_metric: host=algo-1, epoch=150, train loss <loss>=16.7898887634\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:01 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:01 INFO 140504085288768] Epoch[151] Batch[0] avg_epoch_loss=16.665915\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:01 INFO 140504085288768] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=16.6659145355\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:02 INFO 140504085288768] Epoch[151] Batch[5] avg_epoch_loss=16.703687\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:02 INFO 140504085288768] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=16.7036870321\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:02 INFO 140504085288768] Epoch[151] Batch [5]#011Speed: 941.97 samples/sec#011loss=16.703687\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:02 INFO 140504085288768] processed a total of 551 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 982.2251796722412, \"sum\": 982.2251796722412, \"min\": 982.2251796722412}}, \"EndTime\": 1564696382.390092, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696381.407381}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:02 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=560.90267833 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:02 INFO 140504085288768] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:02 INFO 140504085288768] #quality_metric: host=algo-1, epoch=151, train loss <loss>=16.7208156586\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:02 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:02 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_4ba69839-7edc-493b-ad2d-7560e1d01644-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.105144500732422, \"sum\": 23.105144500732422, \"min\": 23.105144500732422}}, \"EndTime\": 1564696382.413835, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696382.390175}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:02 INFO 140504085288768] Epoch[152] Batch[0] avg_epoch_loss=16.808531\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:02 INFO 140504085288768] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=16.8085308075\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:03 INFO 140504085288768] Epoch[152] Batch[5] avg_epoch_loss=16.780746\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:03 INFO 140504085288768] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=16.7807455063\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:03 INFO 140504085288768] Epoch[152] Batch [5]#011Speed: 948.65 samples/sec#011loss=16.780746\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:03 INFO 140504085288768] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1040.0099754333496, \"sum\": 1040.0099754333496, \"min\": 1040.0099754333496}}, \"EndTime\": 1564696383.453971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696382.413905}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:03 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=573.967823412 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:03 INFO 140504085288768] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:03 INFO 140504085288768] #quality_metric: host=algo-1, epoch=152, train loss <loss>=16.8082145691\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:03 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:03 INFO 140504085288768] Epoch[153] Batch[0] avg_epoch_loss=16.840759\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:03 INFO 140504085288768] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=16.8407592773\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:04 INFO 140504085288768] Epoch[153] Batch[5] avg_epoch_loss=16.774577\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:04 INFO 140504085288768] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=16.7745774587\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:04 INFO 140504085288768] Epoch[153] Batch [5]#011Speed: 952.71 samples/sec#011loss=16.774577\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:04 INFO 140504085288768] processed a total of 596 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1055.778980255127, \"sum\": 1055.778980255127, \"min\": 1055.778980255127}}, \"EndTime\": 1564696384.510298, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696383.454054}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:04 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=564.449505718 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:04 INFO 140504085288768] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:04 INFO 140504085288768] #quality_metric: host=algo-1, epoch=153, train loss <loss>=16.7353887558\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:04 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:04 INFO 140504085288768] Epoch[154] Batch[0] avg_epoch_loss=16.765408\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:04 INFO 140504085288768] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=16.7654075623\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:05 INFO 140504085288768] Epoch[154] Batch[5] avg_epoch_loss=16.765785\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:05 INFO 140504085288768] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=16.7657852173\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:05 INFO 140504085288768] Epoch[154] Batch [5]#011Speed: 954.92 samples/sec#011loss=16.765785\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:05 INFO 140504085288768] processed a total of 605 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1051.206111907959, \"sum\": 1051.206111907959, \"min\": 1051.206111907959}}, \"EndTime\": 1564696385.562059, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696384.510378}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:05 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=575.465946293 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:05 INFO 140504085288768] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:05 INFO 140504085288768] #quality_metric: host=algo-1, epoch=154, train loss <loss>=16.7924404144\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:05 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:06 INFO 140504085288768] Epoch[155] Batch[0] avg_epoch_loss=16.832420\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:06 INFO 140504085288768] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=16.8324203491\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:06 INFO 140504085288768] Epoch[155] Batch[5] avg_epoch_loss=16.779141\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:06 INFO 140504085288768] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=16.7791414261\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:06 INFO 140504085288768] Epoch[155] Batch [5]#011Speed: 950.13 samples/sec#011loss=16.779141\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:06 INFO 140504085288768] processed a total of 628 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1103.482961654663, \"sum\": 1103.482961654663, \"min\": 1103.482961654663}}, \"EndTime\": 1564696386.666064, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696385.56214}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:06 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=569.047849508 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:06 INFO 140504085288768] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:06 INFO 140504085288768] #quality_metric: host=algo-1, epoch=155, train loss <loss>=16.758685112\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:06 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:07 INFO 140504085288768] Epoch[156] Batch[0] avg_epoch_loss=16.815582\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:07 INFO 140504085288768] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=16.8155822754\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:07 INFO 140504085288768] Epoch[156] Batch[5] avg_epoch_loss=16.802944\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:07 INFO 140504085288768] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=16.8029435476\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:07 INFO 140504085288768] Epoch[156] Batch [5]#011Speed: 880.60 samples/sec#011loss=16.802944\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:07 INFO 140504085288768] processed a total of 568 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1064.795970916748, \"sum\": 1064.795970916748, \"min\": 1064.795970916748}}, \"EndTime\": 1564696387.731452, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696386.666143}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:07 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=533.376887719 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:07 INFO 140504085288768] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:07 INFO 140504085288768] #quality_metric: host=algo-1, epoch=156, train loss <loss>=16.7786943648\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:07 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:08 INFO 140504085288768] Epoch[157] Batch[0] avg_epoch_loss=16.634960\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:08 INFO 140504085288768] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=16.6349601746\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:08 INFO 140504085288768] Epoch[157] Batch[5] avg_epoch_loss=16.643729\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:08 INFO 140504085288768] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=16.6437292099\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:08 INFO 140504085288768] Epoch[157] Batch [5]#011Speed: 953.97 samples/sec#011loss=16.643729\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:08 INFO 140504085288768] processed a total of 588 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.8170471191406, \"sum\": 1089.8170471191406, \"min\": 1089.8170471191406}}, \"EndTime\": 1564696388.82183, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696387.731532}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:08 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=539.481208341 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:08 INFO 140504085288768] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:08 INFO 140504085288768] #quality_metric: host=algo-1, epoch=157, train loss <loss>=16.6650564194\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:08 INFO 140504085288768] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:08 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/state_90a25e6a-4ac4-494c-93e2-68f6211347d6-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.865917205810547, \"sum\": 20.865917205810547, \"min\": 20.865917205810547}}, \"EndTime\": 1564696388.843314, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696388.821912}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:09 INFO 140504085288768] Epoch[158] Batch[0] avg_epoch_loss=16.818289\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:09 INFO 140504085288768] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=16.8182888031\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:09 INFO 140504085288768] Epoch[158] Batch[5] avg_epoch_loss=16.788755\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:09 INFO 140504085288768] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=16.7887547811\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:09 INFO 140504085288768] Epoch[158] Batch [5]#011Speed: 915.79 samples/sec#011loss=16.788755\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:09 INFO 140504085288768] processed a total of 563 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 993.9661026000977, \"sum\": 993.9661026000977, \"min\": 993.9661026000977}}, \"EndTime\": 1564696389.83742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696388.843389}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:09 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=566.356166486 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:09 INFO 140504085288768] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:09 INFO 140504085288768] #quality_metric: host=algo-1, epoch=158, train loss <loss>=16.7791540358\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:09 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:10 INFO 140504085288768] Epoch[159] Batch[0] avg_epoch_loss=16.798573\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:10 INFO 140504085288768] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=16.7985725403\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:10 INFO 140504085288768] Epoch[159] Batch[5] avg_epoch_loss=16.797132\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:10 INFO 140504085288768] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=16.7971315384\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:10 INFO 140504085288768] Epoch[159] Batch [5]#011Speed: 951.96 samples/sec#011loss=16.797132\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:10 INFO 140504085288768] processed a total of 629 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1033.519983291626, \"sum\": 1033.519983291626, \"min\": 1033.519983291626}}, \"EndTime\": 1564696390.871493, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696389.83749}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:10 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=608.528995588 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:10 INFO 140504085288768] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:10 INFO 140504085288768] #quality_metric: host=algo-1, epoch=159, train loss <loss>=16.7878164291\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:10 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:11 INFO 140504085288768] Epoch[160] Batch[0] avg_epoch_loss=16.834389\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:11 INFO 140504085288768] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=16.8343887329\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:11 INFO 140504085288768] Epoch[160] Batch[5] avg_epoch_loss=16.747404\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:11 INFO 140504085288768] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=16.7474040985\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:11 INFO 140504085288768] Epoch[160] Batch [5]#011Speed: 951.23 samples/sec#011loss=16.747404\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:11 INFO 140504085288768] processed a total of 595 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1037.8310680389404, \"sum\": 1037.8310680389404, \"min\": 1037.8310680389404}}, \"EndTime\": 1564696391.909902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696390.871575}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:11 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=573.253613542 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:11 INFO 140504085288768] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:11 INFO 140504085288768] #quality_metric: host=algo-1, epoch=160, train loss <loss>=16.7453830719\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:11 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:12 INFO 140504085288768] Epoch[161] Batch[0] avg_epoch_loss=16.828136\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:12 INFO 140504085288768] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=16.8281364441\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:12 INFO 140504085288768] Epoch[161] Batch[5] avg_epoch_loss=16.784188\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:12 INFO 140504085288768] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=16.7841876348\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:12 INFO 140504085288768] Epoch[161] Batch [5]#011Speed: 926.59 samples/sec#011loss=16.784188\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:12 INFO 140504085288768] processed a total of 583 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1035.1660251617432, \"sum\": 1035.1660251617432, \"min\": 1035.1660251617432}}, \"EndTime\": 1564696392.945584, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696391.909976}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:12 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=563.138131797 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:12 INFO 140504085288768] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:12 INFO 140504085288768] #quality_metric: host=algo-1, epoch=161, train loss <loss>=16.7301311493\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:12 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:13 INFO 140504085288768] Epoch[162] Batch[0] avg_epoch_loss=16.705433\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:13 INFO 140504085288768] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=16.7054328918\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:13 INFO 140504085288768] Epoch[162] Batch[5] avg_epoch_loss=16.806136\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:13 INFO 140504085288768] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=16.8061361313\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:13 INFO 140504085288768] Epoch[162] Batch [5]#011Speed: 920.33 samples/sec#011loss=16.806136\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:13 INFO 140504085288768] processed a total of 592 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1032.3169231414795, \"sum\": 1032.3169231414795, \"min\": 1032.3169231414795}}, \"EndTime\": 1564696393.978451, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696392.945657}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:13 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=573.401218839 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:13 INFO 140504085288768] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:13 INFO 140504085288768] #quality_metric: host=algo-1, epoch=162, train loss <loss>=16.8289369583\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:13 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:14 INFO 140504085288768] Epoch[163] Batch[0] avg_epoch_loss=16.763079\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:14 INFO 140504085288768] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=16.7630786896\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:14 INFO 140504085288768] Epoch[163] Batch[5] avg_epoch_loss=16.732304\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:14 INFO 140504085288768] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=16.7323039373\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:14 INFO 140504085288768] Epoch[163] Batch [5]#011Speed: 946.11 samples/sec#011loss=16.732304\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:15 INFO 140504085288768] processed a total of 585 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1051.481008529663, \"sum\": 1051.481008529663, \"min\": 1051.481008529663}}, \"EndTime\": 1564696395.030477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696393.978532}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:15 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=556.29505404 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:15 INFO 140504085288768] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:15 INFO 140504085288768] #quality_metric: host=algo-1, epoch=163, train loss <loss>=16.7405376434\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:15 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:15 INFO 140504085288768] Epoch[164] Batch[0] avg_epoch_loss=16.783516\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:15 INFO 140504085288768] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=16.7835159302\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:53:15 INFO 140504085288768] Epoch[164] Batch[5] avg_epoch_loss=16.770687\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:15 INFO 140504085288768] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=16.7706871033\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:15 INFO 140504085288768] Epoch[164] Batch [5]#011Speed: 952.23 samples/sec#011loss=16.770687\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:16 INFO 140504085288768] processed a total of 611 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1027.3947715759277, \"sum\": 1027.3947715759277, \"min\": 1027.3947715759277}}, \"EndTime\": 1564696396.058436, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696395.030557}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:16 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=594.655254596 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:16 INFO 140504085288768] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:16 INFO 140504085288768] #quality_metric: host=algo-1, epoch=164, train loss <loss>=16.7500436783\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:16 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:16 INFO 140504085288768] Epoch[165] Batch[0] avg_epoch_loss=16.812380\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:16 INFO 140504085288768] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=16.812379837\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:16 INFO 140504085288768] Epoch[165] Batch[5] avg_epoch_loss=16.779193\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:16 INFO 140504085288768] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=16.7791926066\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:16 INFO 140504085288768] Epoch[165] Batch [5]#011Speed: 914.63 samples/sec#011loss=16.779193\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:17 INFO 140504085288768] processed a total of 621 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1043.7119007110596, \"sum\": 1043.7119007110596, \"min\": 1043.7119007110596}}, \"EndTime\": 1564696397.102686, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696396.058496}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:17 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=594.929127801 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:17 INFO 140504085288768] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:17 INFO 140504085288768] #quality_metric: host=algo-1, epoch=165, train loss <loss>=16.7491043091\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:17 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:17 INFO 140504085288768] Epoch[166] Batch[0] avg_epoch_loss=16.835871\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:17 INFO 140504085288768] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=16.8358707428\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:17 INFO 140504085288768] Epoch[166] Batch[5] avg_epoch_loss=16.792535\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:17 INFO 140504085288768] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=16.7925351461\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:17 INFO 140504085288768] Epoch[166] Batch [5]#011Speed: 944.89 samples/sec#011loss=16.792535\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:18 INFO 140504085288768] processed a total of 565 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 955.0669193267822, \"sum\": 955.0669193267822, \"min\": 955.0669193267822}}, \"EndTime\": 1564696398.058305, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696397.102758}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:18 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=591.504651851 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:18 INFO 140504085288768] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:18 INFO 140504085288768] #quality_metric: host=algo-1, epoch=166, train loss <loss>=16.7871856689\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:18 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:18 INFO 140504085288768] Epoch[167] Batch[0] avg_epoch_loss=16.605650\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:18 INFO 140504085288768] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=16.6056499481\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:18 INFO 140504085288768] Epoch[167] Batch[5] avg_epoch_loss=16.750725\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:18 INFO 140504085288768] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=16.7507251104\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:18 INFO 140504085288768] Epoch[167] Batch [5]#011Speed: 939.26 samples/sec#011loss=16.750725\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:19 INFO 140504085288768] processed a total of 531 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 960.723876953125, \"sum\": 960.723876953125, \"min\": 960.723876953125}}, \"EndTime\": 1564696399.019561, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696398.058385}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:19 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=552.649399987 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:19 INFO 140504085288768] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:19 INFO 140504085288768] #quality_metric: host=algo-1, epoch=167, train loss <loss>=16.7501735687\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:19 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:19 INFO 140504085288768] Epoch[168] Batch[0] avg_epoch_loss=16.722944\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:19 INFO 140504085288768] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=16.7229442596\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:19 INFO 140504085288768] Epoch[168] Batch[5] avg_epoch_loss=16.739007\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:19 INFO 140504085288768] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=16.7390069962\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:19 INFO 140504085288768] Epoch[168] Batch [5]#011Speed: 945.19 samples/sec#011loss=16.739007\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:20 INFO 140504085288768] processed a total of 605 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1093.350887298584, \"sum\": 1093.350887298584, \"min\": 1093.350887298584}}, \"EndTime\": 1564696400.113472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696399.019628}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:20 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=553.288550801 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:20 INFO 140504085288768] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:20 INFO 140504085288768] #quality_metric: host=algo-1, epoch=168, train loss <loss>=16.7582363129\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:20 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:20 INFO 140504085288768] Epoch[169] Batch[0] avg_epoch_loss=16.804516\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:20 INFO 140504085288768] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=16.8045158386\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:20 INFO 140504085288768] Epoch[169] Batch[5] avg_epoch_loss=16.789730\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:20 INFO 140504085288768] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=16.7897303899\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:20 INFO 140504085288768] Epoch[169] Batch [5]#011Speed: 952.93 samples/sec#011loss=16.789730\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:21 INFO 140504085288768] processed a total of 613 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1049.9870777130127, \"sum\": 1049.9870777130127, \"min\": 1049.9870777130127}}, \"EndTime\": 1564696401.163982, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696400.113551}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:21 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=583.751758617 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:21 INFO 140504085288768] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:21 INFO 140504085288768] #quality_metric: host=algo-1, epoch=169, train loss <loss>=16.7897449493\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:21 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:21 INFO 140504085288768] Epoch[170] Batch[0] avg_epoch_loss=16.798185\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:21 INFO 140504085288768] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=16.7981853485\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:21 INFO 140504085288768] Epoch[170] Batch[5] avg_epoch_loss=16.748368\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:21 INFO 140504085288768] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=16.7483679454\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:21 INFO 140504085288768] Epoch[170] Batch [5]#011Speed: 915.47 samples/sec#011loss=16.748368\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:22 INFO 140504085288768] processed a total of 555 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 989.6020889282227, \"sum\": 989.6020889282227, \"min\": 989.6020889282227}}, \"EndTime\": 1564696402.154169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696401.164061}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:22 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=560.770274226 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:22 INFO 140504085288768] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:22 INFO 140504085288768] #quality_metric: host=algo-1, epoch=170, train loss <loss>=16.7736655341\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:22 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:22 INFO 140504085288768] Epoch[171] Batch[0] avg_epoch_loss=16.856262\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:22 INFO 140504085288768] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=16.856262207\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:22 INFO 140504085288768] Epoch[171] Batch[5] avg_epoch_loss=16.772926\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:22 INFO 140504085288768] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=16.7729263306\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:22 INFO 140504085288768] Epoch[171] Batch [5]#011Speed: 921.97 samples/sec#011loss=16.772926\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:23 INFO 140504085288768] processed a total of 613 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1041.4128303527832, \"sum\": 1041.4128303527832, \"min\": 1041.4128303527832}}, \"EndTime\": 1564696403.196167, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696402.154239}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:23 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=588.557682861 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:23 INFO 140504085288768] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:23 INFO 140504085288768] #quality_metric: host=algo-1, epoch=171, train loss <loss>=16.7930786133\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:23 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:23 INFO 140504085288768] Epoch[172] Batch[0] avg_epoch_loss=16.759754\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:23 INFO 140504085288768] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=16.7597541809\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:23 INFO 140504085288768] Epoch[172] Batch[5] avg_epoch_loss=16.794146\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:23 INFO 140504085288768] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=16.7941462199\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:23 INFO 140504085288768] Epoch[172] Batch [5]#011Speed: 921.32 samples/sec#011loss=16.794146\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:24 INFO 140504085288768] processed a total of 631 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1052.5310039520264, \"sum\": 1052.5310039520264, \"min\": 1052.5310039520264}}, \"EndTime\": 1564696404.249222, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696403.196246}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:24 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=599.441153381 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:24 INFO 140504085288768] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:24 INFO 140504085288768] #quality_metric: host=algo-1, epoch=172, train loss <loss>=16.7918233871\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:24 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:24 INFO 140504085288768] Epoch[173] Batch[0] avg_epoch_loss=16.740084\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:24 INFO 140504085288768] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=16.7400836945\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:25 INFO 140504085288768] Epoch[173] Batch[5] avg_epoch_loss=16.775771\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:25 INFO 140504085288768] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=16.7757708232\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:25 INFO 140504085288768] Epoch[173] Batch [5]#011Speed: 937.20 samples/sec#011loss=16.775771\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:25 INFO 140504085288768] processed a total of 579 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1043.0538654327393, \"sum\": 1043.0538654327393, \"min\": 1043.0538654327393}}, \"EndTime\": 1564696405.292845, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696404.249302}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:25 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=555.038980914 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:25 INFO 140504085288768] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:25 INFO 140504085288768] #quality_metric: host=algo-1, epoch=173, train loss <loss>=16.7903778076\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:25 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:25 INFO 140504085288768] Epoch[174] Batch[0] avg_epoch_loss=16.643814\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:25 INFO 140504085288768] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=16.6438140869\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:53:26 INFO 140504085288768] Epoch[174] Batch[5] avg_epoch_loss=16.773691\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=16.7736908595\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:26 INFO 140504085288768] Epoch[174] Batch [5]#011Speed: 934.19 samples/sec#011loss=16.773691\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:26 INFO 140504085288768] processed a total of 596 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1076.362133026123, \"sum\": 1076.362133026123, \"min\": 1076.362133026123}}, \"EndTime\": 1564696406.369737, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696405.292925}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:26 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=553.656897331 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:26 INFO 140504085288768] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=174, train loss <loss>=16.7373064041\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:26 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:26 INFO 140504085288768] Epoch[175] Batch[0] avg_epoch_loss=16.660074\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:26 INFO 140504085288768] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=16.660074234\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:27 INFO 140504085288768] Epoch[175] Batch[5] avg_epoch_loss=16.794594\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=16.7945944468\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:27 INFO 140504085288768] Epoch[175] Batch [5]#011Speed: 906.84 samples/sec#011loss=16.794594\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:27 INFO 140504085288768] processed a total of 549 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1050.8990287780762, \"sum\": 1050.8990287780762, \"min\": 1050.8990287780762}}, \"EndTime\": 1564696407.421173, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696406.369817}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:27 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=522.352251991 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:27 INFO 140504085288768] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=175, train loss <loss>=16.8159597185\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:27 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:27 INFO 140504085288768] Epoch[176] Batch[0] avg_epoch_loss=16.786060\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:27 INFO 140504085288768] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=16.7860603333\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:28 INFO 140504085288768] Epoch[176] Batch[5] avg_epoch_loss=16.709098\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=16.7090981801\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:28 INFO 140504085288768] Epoch[176] Batch [5]#011Speed: 956.89 samples/sec#011loss=16.709098\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:28 INFO 140504085288768] processed a total of 577 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1088.4110927581787, \"sum\": 1088.4110927581787, \"min\": 1088.4110927581787}}, \"EndTime\": 1564696408.510159, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696407.421253}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:28 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=530.073100226 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:28 INFO 140504085288768] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=176, train loss <loss>=16.7922597885\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:28 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:28 INFO 140504085288768] Epoch[177] Batch[0] avg_epoch_loss=16.788555\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:28 INFO 140504085288768] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=16.7885551453\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:29 INFO 140504085288768] Epoch[177] Batch[5] avg_epoch_loss=16.755046\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:29 INFO 140504085288768] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=16.7550462087\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:29 INFO 140504085288768] Epoch[177] Batch [5]#011Speed: 951.85 samples/sec#011loss=16.755046\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:29 INFO 140504085288768] processed a total of 596 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1039.0169620513916, \"sum\": 1039.0169620513916, \"min\": 1039.0169620513916}}, \"EndTime\": 1564696409.549691, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696408.510239}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:29 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=573.554503396 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:29 INFO 140504085288768] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:29 INFO 140504085288768] #quality_metric: host=algo-1, epoch=177, train loss <loss>=16.7393373489\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:29 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:29 INFO 140504085288768] Epoch[178] Batch[0] avg_epoch_loss=16.830580\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:29 INFO 140504085288768] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=16.8305797577\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:30 INFO 140504085288768] Epoch[178] Batch[5] avg_epoch_loss=16.767474\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=16.7674744924\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:30 INFO 140504085288768] Epoch[178] Batch [5]#011Speed: 941.76 samples/sec#011loss=16.767474\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:30 INFO 140504085288768] processed a total of 561 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 979.0370464324951, \"sum\": 979.0370464324951, \"min\": 979.0370464324951}}, \"EndTime\": 1564696410.529272, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696409.549771}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:30 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=572.944773149 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:30 INFO 140504085288768] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=178, train loss <loss>=16.7732510037\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:30 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:30 INFO 140504085288768] Epoch[179] Batch[0] avg_epoch_loss=16.748529\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:30 INFO 140504085288768] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=16.7485294342\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:31 INFO 140504085288768] Epoch[179] Batch[5] avg_epoch_loss=16.761436\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=16.7614358266\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:31 INFO 140504085288768] Epoch[179] Batch [5]#011Speed: 948.75 samples/sec#011loss=16.761436\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:31 INFO 140504085288768] processed a total of 541 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 998.3038902282715, \"sum\": 998.3038902282715, \"min\": 998.3038902282715}}, \"EndTime\": 1564696411.528147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696410.529352}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:31 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=541.855615077 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:31 INFO 140504085288768] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=179, train loss <loss>=16.7487597995\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:31 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:31 INFO 140504085288768] Epoch[180] Batch[0] avg_epoch_loss=16.805120\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:31 INFO 140504085288768] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=16.8051204681\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:32 INFO 140504085288768] Epoch[180] Batch[5] avg_epoch_loss=16.727553\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:32 INFO 140504085288768] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=16.7275533676\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:32 INFO 140504085288768] Epoch[180] Batch [5]#011Speed: 962.29 samples/sec#011loss=16.727553\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:32 INFO 140504085288768] processed a total of 556 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1002.8870105743408, \"sum\": 1002.8870105743408, \"min\": 1002.8870105743408}}, \"EndTime\": 1564696412.531621, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696411.528227}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:32 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=554.34027168 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:32 INFO 140504085288768] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:32 INFO 140504085288768] #quality_metric: host=algo-1, epoch=180, train loss <loss>=16.7269405789\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:32 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:32 INFO 140504085288768] Epoch[181] Batch[0] avg_epoch_loss=16.765417\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:32 INFO 140504085288768] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=16.765417099\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:33 INFO 140504085288768] Epoch[181] Batch[5] avg_epoch_loss=16.775158\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=16.7751579285\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:33 INFO 140504085288768] Epoch[181] Batch [5]#011Speed: 950.56 samples/sec#011loss=16.775158\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:33 INFO 140504085288768] processed a total of 565 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 954.002857208252, \"sum\": 954.002857208252, \"min\": 954.002857208252}}, \"EndTime\": 1564696413.486131, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696412.531694}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:33 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=592.173040401 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:33 INFO 140504085288768] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=181, train loss <loss>=16.7708646986\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:33 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:33 INFO 140504085288768] Epoch[182] Batch[0] avg_epoch_loss=16.767675\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:33 INFO 140504085288768] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=16.7676753998\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:34 INFO 140504085288768] Epoch[182] Batch[5] avg_epoch_loss=16.784149\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=16.7841491699\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:34 INFO 140504085288768] Epoch[182] Batch [5]#011Speed: 952.65 samples/sec#011loss=16.784149\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:34 INFO 140504085288768] processed a total of 558 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 951.6921043395996, \"sum\": 951.6921043395996, \"min\": 951.6921043395996}}, \"EndTime\": 1564696414.438417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696413.486204}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:34 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=586.261956274 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:34 INFO 140504085288768] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=182, train loss <loss>=16.7571487427\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:34 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:34 INFO 140504085288768] Epoch[183] Batch[0] avg_epoch_loss=16.750658\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:34 INFO 140504085288768] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=16.7506580353\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:35 INFO 140504085288768] Epoch[183] Batch[5] avg_epoch_loss=16.759674\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:35 INFO 140504085288768] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=16.7596740723\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:35 INFO 140504085288768] Epoch[183] Batch [5]#011Speed: 937.92 samples/sec#011loss=16.759674\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:35 INFO 140504085288768] processed a total of 586 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1029.7138690948486, \"sum\": 1029.7138690948486, \"min\": 1029.7138690948486}}, \"EndTime\": 1564696415.468632, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696414.438484}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:35 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=569.031500369 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:35 INFO 140504085288768] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:35 INFO 140504085288768] #quality_metric: host=algo-1, epoch=183, train loss <loss>=16.7670385361\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:35 INFO 140504085288768] loss did not improve\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:53:35 INFO 140504085288768] Epoch[184] Batch[0] avg_epoch_loss=16.781076\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:35 INFO 140504085288768] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=16.7810764313\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:36 INFO 140504085288768] Epoch[184] Batch[5] avg_epoch_loss=16.714217\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=16.714217186\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:36 INFO 140504085288768] Epoch[184] Batch [5]#011Speed: 927.79 samples/sec#011loss=16.714217\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:36 INFO 140504085288768] processed a total of 565 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 972.0330238342285, \"sum\": 972.0330238342285, \"min\": 972.0330238342285}}, \"EndTime\": 1564696416.441211, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696415.468708}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:36 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=581.1878317 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:36 INFO 140504085288768] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=184, train loss <loss>=16.720076455\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:36 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:36 INFO 140504085288768] Epoch[185] Batch[0] avg_epoch_loss=16.794031\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:36 INFO 140504085288768] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=16.7940311432\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:37 INFO 140504085288768] Epoch[185] Batch[5] avg_epoch_loss=16.758856\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=16.7588561376\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:37 INFO 140504085288768] Epoch[185] Batch [5]#011Speed: 931.15 samples/sec#011loss=16.758856\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:37 INFO 140504085288768] processed a total of 594 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1070.059061050415, \"sum\": 1070.059061050415, \"min\": 1070.059061050415}}, \"EndTime\": 1564696417.511834, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696416.44129}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:37 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=555.049442827 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:37 INFO 140504085288768] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=185, train loss <loss>=16.7534761429\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:37 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:37 INFO 140504085288768] Epoch[186] Batch[0] avg_epoch_loss=16.674828\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:37 INFO 140504085288768] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=16.6748275757\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:38 INFO 140504085288768] Epoch[186] Batch[5] avg_epoch_loss=16.752235\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:38 INFO 140504085288768] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=16.7522350947\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:38 INFO 140504085288768] Epoch[186] Batch [5]#011Speed: 949.00 samples/sec#011loss=16.752235\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:38 INFO 140504085288768] processed a total of 596 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1054.54683303833, \"sum\": 1054.54683303833, \"min\": 1054.54683303833}}, \"EndTime\": 1564696418.566921, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696417.511913}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:38 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=565.108815435 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:38 INFO 140504085288768] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:38 INFO 140504085288768] #quality_metric: host=algo-1, epoch=186, train loss <loss>=16.7069019318\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:38 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:39 INFO 140504085288768] Epoch[187] Batch[0] avg_epoch_loss=16.708181\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:39 INFO 140504085288768] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=16.7081813812\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:39 INFO 140504085288768] Epoch[187] Batch[5] avg_epoch_loss=16.705676\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:39 INFO 140504085288768] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=16.7056763967\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:39 INFO 140504085288768] Epoch[187] Batch [5]#011Speed: 949.00 samples/sec#011loss=16.705676\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:39 INFO 140504085288768] processed a total of 597 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1096.2419509887695, \"sum\": 1096.2419509887695, \"min\": 1096.2419509887695}}, \"EndTime\": 1564696419.663703, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696418.567002}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:39 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=544.529185052 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:39 INFO 140504085288768] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:39 INFO 140504085288768] #quality_metric: host=algo-1, epoch=187, train loss <loss>=16.7003854752\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:39 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:40 INFO 140504085288768] Epoch[188] Batch[0] avg_epoch_loss=16.597609\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=16.5976085663\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:40 INFO 140504085288768] Epoch[188] Batch[5] avg_epoch_loss=16.684183\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=16.6841834386\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:40 INFO 140504085288768] Epoch[188] Batch [5]#011Speed: 947.32 samples/sec#011loss=16.684183\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:40 INFO 140504085288768] processed a total of 599 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1069.8339939117432, \"sum\": 1069.8339939117432, \"min\": 1069.8339939117432}}, \"EndTime\": 1564696420.734069, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696419.663784}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:40 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=559.837194544 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:40 INFO 140504085288768] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:40 INFO 140504085288768] #quality_metric: host=algo-1, epoch=188, train loss <loss>=16.7214738846\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:40 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:41 INFO 140504085288768] Epoch[189] Batch[0] avg_epoch_loss=16.801119\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=16.8011188507\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:41 INFO 140504085288768] Epoch[189] Batch[5] avg_epoch_loss=16.771437\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=16.7714366913\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:41 INFO 140504085288768] Epoch[189] Batch [5]#011Speed: 950.15 samples/sec#011loss=16.771437\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:41 INFO 140504085288768] processed a total of 609 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1046.5819835662842, \"sum\": 1046.5819835662842, \"min\": 1046.5819835662842}}, \"EndTime\": 1564696421.781181, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696420.734152}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:41 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=581.82860376 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:41 INFO 140504085288768] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:41 INFO 140504085288768] #quality_metric: host=algo-1, epoch=189, train loss <loss>=16.7709499359\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:41 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:42 INFO 140504085288768] Epoch[190] Batch[0] avg_epoch_loss=16.799894\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:42 INFO 140504085288768] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=16.7998943329\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:42 INFO 140504085288768] Epoch[190] Batch[5] avg_epoch_loss=16.782084\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:42 INFO 140504085288768] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=16.7820841471\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:42 INFO 140504085288768] Epoch[190] Batch [5]#011Speed: 957.41 samples/sec#011loss=16.782084\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:42 INFO 140504085288768] processed a total of 607 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1091.8209552764893, \"sum\": 1091.8209552764893, \"min\": 1091.8209552764893}}, \"EndTime\": 1564696422.873579, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696421.781263}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:42 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=555.890901437 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:42 INFO 140504085288768] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:42 INFO 140504085288768] #quality_metric: host=algo-1, epoch=190, train loss <loss>=16.7610824585\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:42 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:43 INFO 140504085288768] Epoch[191] Batch[0] avg_epoch_loss=16.810459\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:43 INFO 140504085288768] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=16.810459137\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:43 INFO 140504085288768] Epoch[191] Batch[5] avg_epoch_loss=16.749084\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:43 INFO 140504085288768] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=16.7490844727\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:43 INFO 140504085288768] Epoch[191] Batch [5]#011Speed: 954.46 samples/sec#011loss=16.749084\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:43 INFO 140504085288768] processed a total of 585 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1089.745044708252, \"sum\": 1089.745044708252, \"min\": 1089.745044708252}}, \"EndTime\": 1564696423.963849, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696422.873662}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:43 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=536.766678334 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:43 INFO 140504085288768] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:43 INFO 140504085288768] #quality_metric: host=algo-1, epoch=191, train loss <loss>=16.7621883392\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:43 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:44 INFO 140504085288768] Epoch[192] Batch[0] avg_epoch_loss=16.574448\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:44 INFO 140504085288768] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=16.5744476318\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:44 INFO 140504085288768] Epoch[192] Batch[5] avg_epoch_loss=16.740390\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:44 INFO 140504085288768] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=16.7403901418\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:44 INFO 140504085288768] Epoch[192] Batch [5]#011Speed: 954.29 samples/sec#011loss=16.740390\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:45 INFO 140504085288768] processed a total of 586 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1083.0090045928955, \"sum\": 1083.0090045928955, \"min\": 1083.0090045928955}}, \"EndTime\": 1564696425.047389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696423.963929}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:45 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=541.023133448 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:45 INFO 140504085288768] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=192, train loss <loss>=16.6964857101\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:45 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:45 INFO 140504085288768] Epoch[193] Batch[0] avg_epoch_loss=16.758093\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=16.7580928802\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[08/01/2019 21:53:45 INFO 140504085288768] Epoch[193] Batch[5] avg_epoch_loss=16.740213\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:45 INFO 140504085288768] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=16.7402127584\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:45 INFO 140504085288768] Epoch[193] Batch [5]#011Speed: 957.89 samples/sec#011loss=16.740213\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:46 INFO 140504085288768] processed a total of 633 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1060.379981994629, \"sum\": 1060.379981994629, \"min\": 1060.379981994629}}, \"EndTime\": 1564696426.108314, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696425.047476}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:46 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=596.889923151 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:46 INFO 140504085288768] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=193, train loss <loss>=16.7349523544\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:46 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:46 INFO 140504085288768] Epoch[194] Batch[0] avg_epoch_loss=16.780172\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=16.780172348\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:46 INFO 140504085288768] Epoch[194] Batch[5] avg_epoch_loss=16.711748\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:46 INFO 140504085288768] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=16.7117478053\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:46 INFO 140504085288768] Epoch[194] Batch [5]#011Speed: 944.14 samples/sec#011loss=16.711748\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:47 INFO 140504085288768] processed a total of 607 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1081.4909934997559, \"sum\": 1081.4909934997559, \"min\": 1081.4909934997559}}, \"EndTime\": 1564696427.190352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696426.108395}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:47 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=561.203543818 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:47 INFO 140504085288768] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:47 INFO 140504085288768] #quality_metric: host=algo-1, epoch=194, train loss <loss>=16.7452869415\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:47 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:47 INFO 140504085288768] Epoch[195] Batch[0] avg_epoch_loss=16.725002\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:47 INFO 140504085288768] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=16.7250022888\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:47 INFO 140504085288768] Epoch[195] Batch[5] avg_epoch_loss=16.759578\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:47 INFO 140504085288768] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=16.7595777512\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:47 INFO 140504085288768] Epoch[195] Batch [5]#011Speed: 876.31 samples/sec#011loss=16.759578\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:48 INFO 140504085288768] processed a total of 546 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1015.949010848999, \"sum\": 1015.949010848999, \"min\": 1015.949010848999}}, \"EndTime\": 1564696428.206815, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696427.190429}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:48 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=537.367762627 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:48 INFO 140504085288768] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:48 INFO 140504085288768] #quality_metric: host=algo-1, epoch=195, train loss <loss>=16.7756905026\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:48 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:48 INFO 140504085288768] Epoch[196] Batch[0] avg_epoch_loss=16.826799\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:48 INFO 140504085288768] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=16.8267993927\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:48 INFO 140504085288768] Epoch[196] Batch[5] avg_epoch_loss=16.821224\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:48 INFO 140504085288768] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=16.8212235769\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:48 INFO 140504085288768] Epoch[196] Batch [5]#011Speed: 958.15 samples/sec#011loss=16.821224\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:49 INFO 140504085288768] processed a total of 572 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 969.8078632354736, \"sum\": 969.8078632354736, \"min\": 969.8078632354736}}, \"EndTime\": 1564696429.177151, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696428.206894}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:49 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=589.736944208 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:49 INFO 140504085288768] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=196, train loss <loss>=16.8028687371\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:49 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:49 INFO 140504085288768] Epoch[197] Batch[0] avg_epoch_loss=16.704643\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=16.7046432495\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:49 INFO 140504085288768] Epoch[197] Batch[5] avg_epoch_loss=16.726949\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:49 INFO 140504085288768] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=16.7269487381\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:49 INFO 140504085288768] Epoch[197] Batch [5]#011Speed: 961.89 samples/sec#011loss=16.726949\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] processed a total of 561 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 962.0699882507324, \"sum\": 962.0699882507324, \"min\": 962.0699882507324}}, \"EndTime\": 1564696430.139786, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696429.177231}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] #throughput_metric: host=algo-1, train throughput=583.047293147 records/second\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] #quality_metric: host=algo-1, epoch=197, train loss <loss>=16.7289193471\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] loss did not improve\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] Loading parameters from best epoch (157)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 13.028860092163086, \"sum\": 13.028860092163086, \"min\": 13.028860092163086}}, \"EndTime\": 1564696430.153409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696430.139866}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] stopping training now\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] Final loss: 16.6650564194 (occurred at epoch 157)\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] #quality_metric: host=algo-1, train final_loss <loss>=16.6650564194\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 WARNING 140504085288768] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 261.99984550476074, \"sum\": 261.99984550476074, \"min\": 261.99984550476074}}, \"EndTime\": 1564696430.416144, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696430.153477}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 341.5048122406006, \"sum\": 341.5048122406006, \"min\": 341.5048122406006}}, \"EndTime\": 1564696430.495618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696430.41623}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 13.228893280029297, \"sum\": 13.228893280029297, \"min\": 13.228893280029297}}, \"EndTime\": 1564696430.508961, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696430.495687}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:50 INFO 140504085288768] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.028848648071289062, \"sum\": 0.028848648071289062, \"min\": 0.028848648071289062}}, \"EndTime\": 1564696430.509678, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696430.509015}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 2563.764810562134, \"sum\": 2563.764810562134, \"min\": 2563.764810562134}}, \"EndTime\": 1564696433.07342, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696430.509724}\n",
      "\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #test_score (algo-1, RMSE): 7717425.4403\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #test_score (algo-1, mean_wQuantileLoss): 0.721016\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #test_score (algo-1, wQuantileLoss[0.1]): 0.753419\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #test_score (algo-1, wQuantileLoss[0.2]): 0.868934\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #test_score (algo-1, wQuantileLoss[0.3]): 0.910639\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #test_score (algo-1, wQuantileLoss[0.4]): 0.908495\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #test_score (algo-1, wQuantileLoss[0.5]): 0.860676\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #test_score (algo-1, wQuantileLoss[0.6]): 0.770933\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #test_score (algo-1, wQuantileLoss[0.7]): 0.64501\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #test_score (algo-1, wQuantileLoss[0.8]): 0.485453\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #test_score (algo-1, wQuantileLoss[0.9]): 0.285581\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.721015691757\u001b[0m\n",
      "\u001b[31m[08/01/2019 21:53:53 INFO 140504085288768] #quality_metric: host=algo-1, test RMSE <loss>=7717425.4403\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 210585.73698997498, \"sum\": 210585.73698997498, \"min\": 210585.73698997498}, \"setuptime\": {\"count\": 1, \"max\": 8.621931076049805, \"sum\": 8.621931076049805, \"min\": 8.621931076049805}}, \"EndTime\": 1564696433.1023, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1564696433.073504}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-08-01 21:54:03 Uploading - Uploading generated training model\n",
      "2019-08-01 21:54:03 Completed - Training job completed\n",
      "Billable seconds: 275\n"
     ]
    }
   ],
   "source": [
    "data_channels = {\n",
    "    \"train\": \"s3://forecasting-do-not-delete/train/train.json\",\n",
    "    \"test\": \"s3://forecasting-do-not-delete/test/test.json\"\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run Inference\n",
    "If you made it this far, congratulations! None of this is easy. For your next steps, please open up the example notebook under the SageMakerExamples:\n",
    "\n",
    "- SageMakerExamples/Introduction To Amazon Algorithms/DeepAR-Electricity.\n",
    "- **Guys I found this online - not sure if this is the right thing or not but wanted to add it here in case we don't get to it before I have to leave**\n",
    "    - https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_electricity/DeepAR-Electricity.ipynb\n",
    "\n",
    "That will walk you through both how to add more timeseries to your model, and how to get inference results out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + 1\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: deepar-electricity-demo-2019-08-01-19-36-37-058\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateEndpoint operation: Cannot create already existing endpoint \"arn:aws:sagemaker:us-east-1:023375022819:endpoint/deepar-electricity-demo-2019-08-01-19-36-37-058\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f1842638a85d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     predictor_cls=DeepARPredictor)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, use_compiled_model, update_endpoint, wait, model_name, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         )\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, accelerator_type, endpoint_name, update_endpoint, tags, kms_key, wait)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             self.sagemaker_session.endpoint_from_production_variants(\n\u001b[0;32m--> 388\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mproduction_variant\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         self.sagemaker_client.create_endpoint(\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m         )\n\u001b[1;32m    881\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateEndpoint operation: Cannot create already existing endpoint \"arn:aws:sagemaker:us-east-1:023375022819:endpoint/deepar-electricity-demo-2019-08-01-19-36-37-058\"."
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeseries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-74ca5ceda86e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.90\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'timeseries' is not defined"
     ]
    }
   ],
   "source": [
    "predictor.predict(ts=timeseries[120], quantiles=[0.10, 0.5, 0.90]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACME',\n",
       " 'ADAX',\n",
       " 'ALTU',\n",
       " 'APAC',\n",
       " 'ARNE',\n",
       " 'BEAV',\n",
       " 'BESS',\n",
       " 'BIXB',\n",
       " 'BLAC',\n",
       " 'BOIS',\n",
       " 'BOWL',\n",
       " 'BREC',\n",
       " 'BRIS',\n",
       " 'BUFF',\n",
       " 'BURB',\n",
       " 'BURN',\n",
       " 'BUTL',\n",
       " 'BYAR',\n",
       " 'CAMA',\n",
       " 'CENT',\n",
       " 'CHAN',\n",
       " 'CHER',\n",
       " 'CHEY',\n",
       " 'CHIC',\n",
       " 'CLAY',\n",
       " 'CLOU',\n",
       " 'COOK',\n",
       " 'COPA',\n",
       " 'DURA',\n",
       " 'Date',\n",
       " 'ELRE',\n",
       " 'ERIC',\n",
       " 'EUFA',\n",
       " 'FAIR',\n",
       " 'FORA',\n",
       " 'FREE',\n",
       " 'FTCB',\n",
       " 'GOOD',\n",
       " 'GUTH',\n",
       " 'HASK',\n",
       " 'HINT',\n",
       " 'HOBA',\n",
       " 'HOLL',\n",
       " 'HOOK',\n",
       " 'HUGO',\n",
       " 'IDAB',\n",
       " 'JAYX',\n",
       " 'KENT',\n",
       " 'KETC',\n",
       " 'LAHO',\n",
       " 'LANE',\n",
       " 'MADI',\n",
       " 'MANG',\n",
       " 'MARE',\n",
       " 'MAYR',\n",
       " 'MCAL',\n",
       " 'MEDF',\n",
       " 'MEDI',\n",
       " 'MIAM',\n",
       " 'MINC',\n",
       " 'MTHE',\n",
       " 'NEWK',\n",
       " 'NINN',\n",
       " 'NOWA',\n",
       " 'OILT',\n",
       " 'OKEM',\n",
       " 'OKMU',\n",
       " 'PAUL',\n",
       " 'PAWN',\n",
       " 'PERK',\n",
       " 'PRYO',\n",
       " 'PUTN',\n",
       " 'REDR',\n",
       " 'RETR',\n",
       " 'RING',\n",
       " 'SALL',\n",
       " 'SEIL',\n",
       " 'SHAW',\n",
       " 'SKIA',\n",
       " 'SLAP',\n",
       " 'SPEN',\n",
       " 'STIG',\n",
       " 'STIL',\n",
       " 'STUA',\n",
       " 'SULP',\n",
       " 'T',\n",
       " 'TAHL',\n",
       " 'TALI',\n",
       " 'TIPT',\n",
       " 'TISH',\n",
       " 'VINI',\n",
       " 'WASH',\n",
       " 'WATO',\n",
       " 'WAUR',\n",
       " 'WEAT',\n",
       " 'WEST',\n",
       " 'WILB',\n",
       " 'WIST',\n",
       " 'WOOD',\n",
       " 'WYNO',\n",
       " '_AXIS_ALIASES',\n",
       " '_AXIS_IALIASES',\n",
       " '_AXIS_LEN',\n",
       " '_AXIS_NAMES',\n",
       " '_AXIS_NUMBERS',\n",
       " '_AXIS_ORDERS',\n",
       " '_AXIS_REVERSED',\n",
       " '_AXIS_SLICEMAP',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__finalize__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__ifloordiv__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rfloordiv__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__round__',\n",
       " '__rpow__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_accessors',\n",
       " '_add_numeric_operations',\n",
       " '_add_series_only_operations',\n",
       " '_add_series_or_dataframe_operations',\n",
       " '_agg_by_level',\n",
       " '_agg_examples_doc',\n",
       " '_agg_summary_and_see_also_doc',\n",
       " '_aggregate',\n",
       " '_aggregate_multiple_funcs',\n",
       " '_align_frame',\n",
       " '_align_series',\n",
       " '_box_col_values',\n",
       " '_box_item_values',\n",
       " '_builtin_table',\n",
       " '_check_inplace_setting',\n",
       " '_check_is_chained_assignment_possible',\n",
       " '_check_label_or_level_ambiguity',\n",
       " '_check_percentile',\n",
       " '_check_setitem_copy',\n",
       " '_clear_item_cache',\n",
       " '_clip_with_one_bound',\n",
       " '_clip_with_scalar',\n",
       " '_combine_const',\n",
       " '_combine_frame',\n",
       " '_combine_match_columns',\n",
       " '_combine_match_index',\n",
       " '_consolidate',\n",
       " '_consolidate_inplace',\n",
       " '_construct_axes_dict',\n",
       " '_construct_axes_dict_for_slice',\n",
       " '_construct_axes_dict_from',\n",
       " '_construct_axes_from_arguments',\n",
       " '_constructor',\n",
       " '_constructor_expanddim',\n",
       " '_constructor_sliced',\n",
       " '_convert',\n",
       " '_count_level',\n",
       " '_create_indexer',\n",
       " '_cython_table',\n",
       " '_deprecations',\n",
       " '_dir_additions',\n",
       " '_dir_deletions',\n",
       " '_drop_axis',\n",
       " '_drop_labels_or_levels',\n",
       " '_ensure_valid_index',\n",
       " '_expand_axes',\n",
       " '_find_valid_index',\n",
       " '_from_arrays',\n",
       " '_from_axes',\n",
       " '_get_agg_axis',\n",
       " '_get_axis',\n",
       " '_get_axis_name',\n",
       " '_get_axis_number',\n",
       " '_get_axis_resolvers',\n",
       " '_get_block_manager_axis',\n",
       " '_get_bool_data',\n",
       " '_get_cacher',\n",
       " '_get_index_resolvers',\n",
       " '_get_item_cache',\n",
       " '_get_label_or_level_values',\n",
       " '_get_numeric_data',\n",
       " '_get_value',\n",
       " '_get_values',\n",
       " '_getitem_bool_array',\n",
       " '_getitem_frame',\n",
       " '_getitem_multilevel',\n",
       " '_gotitem',\n",
       " '_iget_item_cache',\n",
       " '_indexed_same',\n",
       " '_info_axis',\n",
       " '_info_axis_name',\n",
       " '_info_axis_number',\n",
       " '_info_repr',\n",
       " '_init_mgr',\n",
       " '_internal_names',\n",
       " '_internal_names_set',\n",
       " '_is_builtin_func',\n",
       " '_is_cached',\n",
       " '_is_copy',\n",
       " '_is_cython_func',\n",
       " '_is_datelike_mixed_type',\n",
       " '_is_homogeneous_type',\n",
       " '_is_label_or_level_reference',\n",
       " '_is_label_reference',\n",
       " '_is_level_reference',\n",
       " '_is_mixed_type',\n",
       " '_is_numeric_mixed_type',\n",
       " '_is_view',\n",
       " '_ix',\n",
       " '_ixs',\n",
       " '_join_compat',\n",
       " '_maybe_cache_changed',\n",
       " '_maybe_update_cacher',\n",
       " '_metadata',\n",
       " '_needs_reindex_multi',\n",
       " '_obj_with_exclusions',\n",
       " '_protect_consolidate',\n",
       " '_reduce',\n",
       " '_reindex_axes',\n",
       " '_reindex_columns',\n",
       " '_reindex_index',\n",
       " '_reindex_multi',\n",
       " '_reindex_with_indexers',\n",
       " '_repr_data_resource_',\n",
       " '_repr_fits_horizontal_',\n",
       " '_repr_fits_vertical_',\n",
       " '_repr_html_',\n",
       " '_repr_latex_',\n",
       " '_reset_cache',\n",
       " '_reset_cacher',\n",
       " '_sanitize_column',\n",
       " '_selected_obj',\n",
       " '_selection',\n",
       " '_selection_list',\n",
       " '_selection_name',\n",
       " '_series',\n",
       " '_set_as_cached',\n",
       " '_set_axis',\n",
       " '_set_axis_name',\n",
       " '_set_is_copy',\n",
       " '_set_item',\n",
       " '_set_value',\n",
       " '_setitem_array',\n",
       " '_setitem_frame',\n",
       " '_setitem_slice',\n",
       " '_setup_axes',\n",
       " '_shallow_copy',\n",
       " '_slice',\n",
       " '_stat_axis',\n",
       " '_stat_axis_name',\n",
       " '_stat_axis_number',\n",
       " '_take',\n",
       " '_to_dict_of_blocks',\n",
       " '_try_aggregate_string_function',\n",
       " '_typ',\n",
       " '_unpickle_frame_compat',\n",
       " '_unpickle_matrix_compat',\n",
       " '_update_inplace',\n",
       " '_validate_dtype',\n",
       " '_values',\n",
       " '_where',\n",
       " '_xs',\n",
       " 'abs',\n",
       " 'add',\n",
       " 'add_prefix',\n",
       " 'add_suffix',\n",
       " 'agg',\n",
       " 'aggregate',\n",
       " 'align',\n",
       " 'all',\n",
       " 'any',\n",
       " 'append',\n",
       " 'apply',\n",
       " 'applymap',\n",
       " 'as_matrix',\n",
       " 'asfreq',\n",
       " 'asof',\n",
       " 'assign',\n",
       " 'astype',\n",
       " 'at',\n",
       " 'at_time',\n",
       " 'axes',\n",
       " 'between_time',\n",
       " 'bfill',\n",
       " 'bool',\n",
       " 'boxplot',\n",
       " 'clip',\n",
       " 'clip_lower',\n",
       " 'clip_upper',\n",
       " 'columns',\n",
       " 'combine',\n",
       " 'combine_first',\n",
       " 'compound',\n",
       " 'copy',\n",
       " 'corr',\n",
       " 'corrwith',\n",
       " 'count',\n",
       " 'cov',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'describe',\n",
       " 'diff',\n",
       " 'div',\n",
       " 'divide',\n",
       " 'dot',\n",
       " 'drop',\n",
       " 'drop_duplicates',\n",
       " 'droplevel',\n",
       " 'dropna',\n",
       " 'dtypes',\n",
       " 'duplicated',\n",
       " 'empty',\n",
       " 'eq',\n",
       " 'equals',\n",
       " 'eval',\n",
       " 'ewm',\n",
       " 'expanding',\n",
       " 'ffill',\n",
       " 'fillna',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'first_valid_index',\n",
       " 'floordiv',\n",
       " 'from_dict',\n",
       " 'from_records',\n",
       " 'ftypes',\n",
       " 'ge',\n",
       " 'get',\n",
       " 'get_dtype_counts',\n",
       " 'get_ftype_counts',\n",
       " 'get_values',\n",
       " 'groupby',\n",
       " 'gt',\n",
       " 'head',\n",
       " 'hist',\n",
       " 'iat',\n",
       " 'idxmax',\n",
       " 'idxmin',\n",
       " 'iloc',\n",
       " 'index',\n",
       " 'infer_objects',\n",
       " 'info',\n",
       " 'insert',\n",
       " 'interpolate',\n",
       " 'isin',\n",
       " 'isna',\n",
       " 'isnull',\n",
       " 'items',\n",
       " 'iteritems',\n",
       " 'iterrows',\n",
       " 'itertuples',\n",
       " 'ix',\n",
       " 'join',\n",
       " 'keys',\n",
       " 'kurt',\n",
       " 'kurtosis',\n",
       " 'last',\n",
       " 'last_valid_index',\n",
       " 'le',\n",
       " 'loc',\n",
       " 'lookup',\n",
       " 'lt',\n",
       " 'mad',\n",
       " 'mask',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'melt',\n",
       " 'memory_usage',\n",
       " 'merge',\n",
       " 'min',\n",
       " 'mod',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'multiply',\n",
       " 'ndim',\n",
       " 'ne',\n",
       " 'nlargest',\n",
       " 'notna',\n",
       " 'notnull',\n",
       " 'nsmallest',\n",
       " 'nunique',\n",
       " 'pct_change',\n",
       " 'pipe',\n",
       " 'pivot',\n",
       " 'pivot_table',\n",
       " 'plot',\n",
       " 'pop',\n",
       " 'pow',\n",
       " 'prod',\n",
       " 'product',\n",
       " 'quantile',\n",
       " 'query',\n",
       " 'radd',\n",
       " 'rank',\n",
       " 'rdiv',\n",
       " 'reindex',\n",
       " 'reindex_axis',\n",
       " 'reindex_like',\n",
       " 'rename',\n",
       " 'rename_axis',\n",
       " 'reorder_levels',\n",
       " 'replace',\n",
       " 'resample',\n",
       " 'reset_index',\n",
       " 'rfloordiv',\n",
       " 'rmod',\n",
       " 'rmul',\n",
       " 'rolling',\n",
       " 'round',\n",
       " 'rpow',\n",
       " 'rsub',\n",
       " 'rtruediv',\n",
       " 'sample',\n",
       " 'select',\n",
       " 'select_dtypes',\n",
       " 'sem',\n",
       " 'set_axis',\n",
       " 'set_index',\n",
       " 'shape',\n",
       " 'shift',\n",
       " 'size',\n",
       " 'skew',\n",
       " 'slice_shift',\n",
       " 'sort_index',\n",
       " 'sort_values',\n",
       " 'squeeze',\n",
       " 'stack',\n",
       " 'std',\n",
       " 'style',\n",
       " 'sub',\n",
       " 'subtract',\n",
       " 'sum',\n",
       " 'swapaxes',\n",
       " 'swaplevel',\n",
       " 'tail',\n",
       " 'take',\n",
       " 'timetuple',\n",
       " 'to_clipboard',\n",
       " 'to_csv',\n",
       " 'to_dense',\n",
       " 'to_dict',\n",
       " 'to_excel',\n",
       " 'to_feather',\n",
       " 'to_gbq',\n",
       " 'to_hdf',\n",
       " 'to_html',\n",
       " 'to_json',\n",
       " 'to_latex',\n",
       " 'to_msgpack',\n",
       " 'to_numpy',\n",
       " 'to_panel',\n",
       " 'to_parquet',\n",
       " 'to_period',\n",
       " 'to_pickle',\n",
       " 'to_records',\n",
       " 'to_sparse',\n",
       " 'to_sql',\n",
       " 'to_stata',\n",
       " 'to_string',\n",
       " 'to_timestamp',\n",
       " 'to_xarray',\n",
       " 'transform',\n",
       " 'transpose',\n",
       " 'truediv',\n",
       " 'truncate',\n",
       " 'tshift',\n",
       " 'tz_convert',\n",
       " 'tz_localize',\n",
       " 'unstack',\n",
       " 'update',\n",
       " 'values',\n",
       " 'var',\n",
       " 'where',\n",
       " 'xs']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_timeseries = df.shape[1]\n",
    "dir(df)\n",
    "data_kw = df.resample('1D').sum()\n",
    "print(data_kw)\n",
    "# # timeseries = []\n",
    "# for i in range(num_timeseries):\n",
    "#     timeseries.append(np.trim_zeros(data_kw.iloc[:,i], trim='f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Extend Your Solution\n",
    "Now you're getting forecasts, how will you extend your solution? How good are your forecasts? What about getting forecasts for the other stations? Is your model cognizant of the weather?\n",
    "\n",
    "Spend your remaining time growing your modeling solution to leverage additional datasets. Then, think through how you'd set this up to run in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
